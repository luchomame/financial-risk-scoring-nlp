{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference to extracting all labels \n",
    "https://datascience.stackexchange.com/questions/112438/how-to-get-all-3-labels-sentiment-from-finbert-instead-of-the-most-likely-label\n",
    "\n",
    "essentially, us the AutoModelForSequenceClassification to get all raw logits and then apply softmax ourselves \n",
    "\n",
    "normally the pipeline does the softmax and ONLY returns the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Total CPU cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bokeh\n",
    "\n",
    "# !pip install pyarrow==10.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n",
      "Dask version: 2025.2.0\n",
      "PyArrow version: 19.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import pyarrow\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import duckdb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import bokeh\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Dask version:\", dask.__version__)\n",
    "print(\"PyArrow version:\", pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()  # Put model in evaluation mode\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {\"label\": \"NEUTRAL\", \"score\": 1.0, \"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}\n",
    "    \n",
    "    # Tokenize input text\n",
    "    # inputs = tokenizer(text[:512], return_tensors=\"pt\", truncation=True)\n",
    "    # Getting truncation warning. I'ma use tokenizer truncation instead\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits  # Get raw model outputs (logits)\n",
    "    probs = F.softmax(logits, dim=1)  # Apply softmax across dimension 1 (classes)\n",
    "\n",
    "    # Convert to a Python list\n",
    "    probs = probs.numpy()[0]  # Extract probabilities as a NumPy array\n",
    "\n",
    "    # Define label mapping\n",
    "    labels = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "    sentiment_dict = dict(zip(labels, probs))\n",
    "\n",
    "    # Get the highest-probability label\n",
    "    max_label = labels[torch.argmax(logits).item()]\n",
    "    max_score = max(probs)\n",
    "\n",
    "    return {\n",
    "        \"label\": max_label,\n",
    "        \"score\": max_score,\n",
    "        \"positive\": sentiment_dict[\"POSITIVE\"],\n",
    "        \"neutral\": sentiment_dict[\"NEUTRAL\"],\n",
    "        \"negative\": sentiment_dict[\"NEGATIVE\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CANT START A CLIENT AND CLUSTER BEFORE LOADING FINBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n",
      "{'tcp://127.0.0.1:55584': 1, 'tcp://127.0.0.1:55585': 1, 'tcp://127.0.0.1:55586': 1, 'tcp://127.0.0.1:55587': 1, 'tcp://127.0.0.1:55588': 1, 'tcp://127.0.0.1:55589': 1, 'tcp://127.0.0.1:55590': 1, 'tcp://127.0.0.1:55591': 1, 'tcp://127.0.0.1:55592': 1, 'tcp://127.0.0.1:55611': 1}\n"
     ]
    }
   ],
   "source": [
    "# Try to avoid PyArrow\n",
    "pd.options.mode.string_storage = \"python\"\n",
    "\n",
    "# cluster = LocalCluster(n_workers=num_cores//2, threads_per_worker=1)\n",
    "cluster = LocalCluster(n_workers=10, threads_per_worker=1) # upping to full CPU cores when not using my laptop\n",
    "\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "client = Client(cluster)\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "print(client.dashboard_link)\n",
    "print(client.ncores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "DB_PATH = Path(os.getenv(\"DB_PATH\"))\n",
    "DB_FILE = os.getenv(\"DB_FILE\")\n",
    "duckdb_path = DB_PATH / DB_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path, read_only=True)\n",
    "# df = con.execute(\"SELECT guid, description, article_title, ticker FROM headlines.articles\").fetchdf()\n",
    "\n",
    "# okay getting a bottle neck at pandas to ddf so ognna write as csv \n",
    "# df.to_csv(\"articles_db.csv\", index=False)\n",
    "\n",
    "# try writing to parquet instead\n",
    "# file_name = 'articles_db.parquet'\n",
    "# con.execute(f\"COPY (SELECT guid, description, article_title, ticker FROM headlines.articles) TO '{file_name}' WITH (HEADER, DELIMITER ',');\")\n",
    "# con.execute(f\"COPY (SELECT guid, description, article_title, ticker FROM headlines.articles) TO '{file_name}' (FORMAT 'parquet');\")\n",
    "# try partitioning based on date \n",
    "\n",
    "# will try to partition based on date later \n",
    "output_dir = \"articles_partitioned/\"\n",
    "con.execute(f'''\n",
    "    COPY (\n",
    "        SELECT \n",
    "            guid, \n",
    "            description, \n",
    "            article_title, \n",
    "            ticker, \n",
    "            article_pubdate,\n",
    "            YEAR(article_pubdate) AS year, \n",
    "            MONTH(article_pubdate) AS month\n",
    "        FROM headlines.articles\n",
    "    ) \n",
    "    TO '{output_dir}' \n",
    "    (FORMAT 'parquet', PARTITION_BY (year, month));\n",
    "''')\n",
    "\n",
    "con.close()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# testing only\n",
    "# filtered_df = df.head(1000)\n",
    "# len(filtered_df)\n",
    "# ddf = dd.read_csv(\"articles_db.csv\", assume_missing=True, dtype={'guid': 'object', 'description': 'object', 'article_title': 'object', 'ticker': 'object'})\n",
    "# read parquet \n",
    "# ddf = dd.read_parquet(file_name, engine='pyarrow')\n",
    "\n",
    "# read from articles_partitioned output_dir \n",
    "ddf = dd.read_parquet(output_dir, engine='pyarrow')\n",
    "# check partitions in ddf \n",
    "print(ddf.npartitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jovan\\anaconda3\\envs\\cs7643-a2\\Lib\\site-packages\\distributed\\client.py:3370: UserWarning: Sending large graph of size 419.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 05:43:53,425 - distributed.scheduler - WARNING - Worker failed to heartbeat for 22657s; attempting restart: <WorkerState 'tcp://127.0.0.1:55712', name: 4, status: running, memory: 0, processing: 1>\n",
      "2025-02-21 05:43:53,433 - distributed.scheduler - WARNING - Worker failed to heartbeat for 22656s; attempting restart: <WorkerState 'tcp://127.0.0.1:55722', name: 8, status: running, memory: 0, processing: 1>\n",
      "2025-02-21 05:44:00,820 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 05:44:00,822 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 05:44:01,820 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-21 05:44:01,853 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-21 06:23:05,861 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 10:23:38,927 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 10:32:47,206 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:55668'.\n",
      "2025-02-21 10:32:51,206 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 10:52:56,773 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:55584'.\n",
      "2025-02-21 10:53:00,771 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 11:09:49,262 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:55711'.\n",
      "2025-02-21 15:05:12,583 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:55667'.\n",
      "2025-02-21 15:05:15,327 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "# Enable Progress Bar\n",
    "with ProgressBar():\n",
    "    # Process title sentiment\n",
    "    ddf['finbert_title'] = ddf.map_partitions(\n",
    "        lambda df: df['article_title'].apply(classify_sentiment), meta=(\"x\", \"object\")\n",
    "    )\n",
    "    ddf['finbert_title_label'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_title'].apply(lambda x: x['label']), meta=(\"x\", \"str\")\n",
    "    )\n",
    "    ddf['finbert_title_score'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_title'].apply(lambda x: x['score']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_title_positive'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_title'].apply(lambda x: x['positive']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_title_neutral'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_title'].apply(lambda x: x['neutral']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_title_negative'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_title'].apply(lambda x: x['negative']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "\n",
    "    # Process description sentiment\n",
    "    ddf['finbert_description'] = ddf.map_partitions(\n",
    "        lambda df: df['description'].apply(classify_sentiment), meta=(\"x\", \"object\")\n",
    "    )\n",
    "    ddf['finbert_description_label'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['label']), meta=(\"x\", \"str\")\n",
    "    )\n",
    "    ddf['finbert_description_score'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['score']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_description_positive'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['positive']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_description_neutral'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['neutral']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_description_negative'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['negative']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf.to_csv(\"articles_with_finbert_scores.csv\")\n",
    "\n",
    "# Convert back to Pandas\n",
    "# df_final = ddf.compute()\n",
    "\n",
    "# Save results\n",
    "# df_final.to_csv(\"articles_with_all_finbert_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Pandas DataFrame to Dask DataFrame\n",
    "# # ddf = dd.from_pandas(filtered_df, npartitions=8)  # Adjust partitions based on CPU cores\n",
    "# # ddf = dd.from_pandas(df, npartitions=8)   # gonna set it to 4 cause 400k rows / 4 = 100k rows per partition \n",
    "# # raise ValueError(\"stop here\")\n",
    "# # Apply FinBERT sentiment analysis in parallel\n",
    "# with ProgressBar():\n",
    "#     ddf['finbert_title'] = ddf['article_title'].map(classify_sentiment, meta=(\"x\", \"object\"))\n",
    "#     ddf['finbert_title_label'] = ddf['finbert_title'].map(lambda x: x['label'], meta=(\"x\", \"str\"))\n",
    "#     ddf['finbert_title_score'] = ddf['finbert_title'].map(lambda x: x['score'], meta=(\"x\", \"float\"))\n",
    "#     ddf['finbert_title_positive'] = ddf['finbert_title'].map(lambda x: x['positive'], meta=(\"x\", \"float\"))\n",
    "#     ddf['finbert_title_neutral'] = ddf['finbert_title'].map(lambda x: x['neutral'], meta=(\"x\", \"float\"))\n",
    "#     ddf['finbert_title_negative'] = ddf['finbert_title'].map(lambda x: x['negative'], meta=(\"x\", \"float\"))\n",
    "\n",
    "#     ddf['finbert_description'] = ddf['description'].map(classify_sentiment, meta=(\"x\", \"object\"))\n",
    "#     ddf['finbert_description_label'] = ddf['finbert_description'].map(lambda x: x['label'], meta=(\"x\", \"str\"))\n",
    "#     ddf['finbert_description_score'] = ddf['finbert_description'].map(lambda x: x['score'], meta=(\"x\", \"float\"))\n",
    "#     ddf['finbert_description_positive'] = ddf['finbert_description'].map(lambda x: x['positive'], meta=(\"x\", \"float\"))\n",
    "#     ddf['finbert_description_neutral'] = ddf['finbert_description'].map(lambda x: x['neutral'], meta=(\"x\", \"float\"))\n",
    "#     ddf['finbert_description_negative'] = ddf['finbert_description'].map(lambda x: x['negative'], meta=(\"x\", \"float\"))\n",
    "\n",
    "# # print(\"before compute\")\n",
    "# # Convert back to Pandas\n",
    "# df_final = ddf.compute()\n",
    "# # print(\"after compute\")\n",
    "\n",
    "# # Save results\n",
    "# df_final.to_csv(\"articles_with_all_finbert_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.close()\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
