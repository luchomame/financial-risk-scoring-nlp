{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61decfb8-9c36-465b-a983-19dc05dbe0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import pandas_market_calendars as mcal\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b500a1d-3af0-444a-9f5b-207999fb4429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hilun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hilun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure stopwords and tokenizer are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d13af1-6a60-47c1-90be-6f2cb7cd7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "db_file_path = r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\financial_news.db\"\n",
    "conn = duckdb.connect(database=db_file_path, read_only=False)\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load ticker data\n",
    "ticker_file = pd.read_csv(r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\Index tickers\\nasdaq_screener_us_tech_mid.csv\")\n",
    "ticker_all = ticker_file[\"Symbol\"]  # Extract column\n",
    "\n",
    "# Define column names\n",
    "columns = [\"symbol\", \"MAE\", \"r-square\", \"classification accuracy\"]\n",
    "result_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Load NYSE trading calendar\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "\n",
    "# Load negative words\n",
    "negative_words_df = pd.read_csv(r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\Negative_words.csv\", header=None, names=[\"word\"])\n",
    "negative_words_set = set(negative_words_df[\"word\"].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b71994-218a-4347-9e77-c2536dc5e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the next trading day\n",
    "def next_trading_day(date):\n",
    "    date = pd.Timestamp(date)\n",
    "    while len(nyse.valid_days(start_date=date.strftime('%Y-%m-%d'), end_date=date.strftime('%Y-%m-%d'))) == 0:\n",
    "        date += pd.Timedelta(days=1)\n",
    "    return date\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalpha() and len(word) > 2 and word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087efa2a-d94b-4ef8-a066-70dbd77ade73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single ticker\n",
    "def process_ticker(ticker, conn, nyse, stop_words, negative_words_set, result_df):\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            a.mapped_trading_date AS publish_date,\n",
    "            a.description, \n",
    "            dpm.price_change_percentage\n",
    "        FROM \"Headlines\".\"Articles_Trading_Day\" a\n",
    "        INNER JOIN \"Headlines\".\"Daily_Price_Movement\" dpm\n",
    "        ON a.mapped_trading_date = dpm.trading_date  \n",
    "        WHERE a.ticker = ?\n",
    "        AND dpm.ticker = ?;\n",
    "    \"\"\"\n",
    "    \n",
    "    news_df = conn.execute(query, [ticker, ticker]).fetchdf()\n",
    "\n",
    "    if news_df.empty:\n",
    "        print(f\"Skipping {ticker}: No data found.\")\n",
    "        return\n",
    "\n",
    "    news_df[\"publish_date\"] = pd.to_datetime(news_df[\"publish_date\"]).dt.date\n",
    "    news_df[\"description\"] = news_df[\"description\"].fillna(\"\")\n",
    "\n",
    "    # Group descriptions by date\n",
    "    news_df = news_df.groupby(\"publish_date\", as_index=False).agg({\n",
    "        \"description\": lambda x: \" \".join(x),\n",
    "        \"price_change_percentage\": \"first\"\n",
    "    })\n",
    "\n",
    "    # Adjust for non-trading days\n",
    "    news_df[\"adjusted_date\"] = news_df[\"publish_date\"].apply(next_trading_day)\n",
    "\n",
    "    # Re-group after adjusting trading days\n",
    "    news_df = news_df.groupby(\"adjusted_date\", as_index=False).agg({\n",
    "        \"description\": lambda x: \" \".join(x),\n",
    "        \"price_change_percentage\": \"first\"\n",
    "    })\n",
    "\n",
    "    news_df[\"tokenized_words\"] = news_df[\"description\"].astype(str).apply(tokenize_text)\n",
    "\n",
    "    # Calculate token scores\n",
    "    unique_words = set(word for words_list in news_df[\"tokenized_words\"] for word in words_list)\n",
    "    word_scores = {word: [] for word in unique_words}\n",
    "\n",
    "    for _, row in news_df.iterrows():\n",
    "        words_list = row[\"tokenized_words\"]\n",
    "        price_change = row[\"price_change_percentage\"]\n",
    "        total_words = len(words_list)\n",
    "\n",
    "        if total_words > 0:\n",
    "            word_counts = {word: words_list.count(word) / total_words for word in words_list}\n",
    "            for word, ratio in word_counts.items():\n",
    "                word_scores[word].append(ratio * price_change)\n",
    "\n",
    "    tk_info = pd.DataFrame({\n",
    "        \"word\": list(word_scores.keys()),\n",
    "        \"score\": [np.mean(scores) if scores else 0 for scores in word_scores.values()]\n",
    "    }).dropna()\n",
    "\n",
    "    token_scores_dict = dict(zip(tk_info[\"word\"], tk_info[\"score\"]))\n",
    "\n",
    "    def calculate_token_score(tokens):\n",
    "        return sum(token_scores_dict.get(token, 0) for token in tokens)\n",
    "\n",
    "    news_df[\"token_score\"] = news_df[\"tokenized_words\"].apply(calculate_token_score)\n",
    "\n",
    "    # Ensure no missing values in price change\n",
    "    news_df = news_df.dropna()\n",
    "\n",
    "\n",
    "    X_combined = news_df[\"token_score\"].values.reshape(-1, 1)  # Use token score as feature\n",
    "    y = news_df[\"price_change_percentage\"].values\n",
    "\n",
    "    # Train-Test Split\n",
    "    split_index = int(len(news_df) * 0.8)\n",
    "    X_train, X_test = X_combined[:split_index], X_combined[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Train XGBoost Model\n",
    "    xgb_model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    ave = news_df[\"price_change_percentage\"].abs().mean()\n",
    "    l = [7, 3.5, 0, -3.5, -7] if ave >= 1.5 else [5, 2.5, 0, -2.5, -5]\n",
    "\n",
    "    # Categorization/Classification function\n",
    "    def categorize_value(x):\n",
    "        if x > l[0]:\n",
    "            return 0\n",
    "        elif l[1] <= x <= l[0]:\n",
    "            return 1\n",
    "        elif l[2] <= x < l[1]:\n",
    "            return 2\n",
    "        elif l[3] <= x < l[2]:\n",
    "            return 3\n",
    "        elif l[4] <= x < l[3]:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "    start_row = split_index - 1\n",
    "\n",
    "    # Apply categorization\n",
    "    news_df.loc[news_df.index > start_row, \"actual category\"] = news_df.loc[news_df.index > start_row, \"price_change_percentage\"].apply(categorize_value)\n",
    "    news_df[\"predicted_price_change\"] = np.concatenate([np.full(split_index, np.nan), y_pred])\n",
    "    news_df.loc[news_df.index > start_row, \"predicted category\"] = news_df.loc[news_df.index > start_row, \"predicted_price_change\"].apply(categorize_value)\n",
    "\n",
    "    # Compute classification accuracy\n",
    "    df_filtered = news_df.iloc[split_index:].reset_index(drop=True)[[\"actual category\", \"predicted category\"]]\n",
    "    accuracy = accuracy_score(df_filtered[\"actual category\"], df_filtered[\"predicted category\"])\n",
    "\n",
    "    # Store results\n",
    "    result_df.loc[len(result_df)] = [ticker, mae, r2, accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735cb328-e30e-424a-8726-41fcc262a6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PTC...\n",
      "Processing ON...\n",
      "Processing MDB...\n",
      "Processing SSNC...\n",
      "Processing VRSN...\n",
      "Processing WDC...\n",
      "Processing GFS...\n",
      "Processing NTAP...\n",
      "Processing SMCI...\n",
      "Processing ZM...\n",
      "Processing MCHP...\n",
      "Processing ANSS...\n",
      "Processing ZS...\n",
      "Processing EA...\n",
      "Processing MPWR...\n",
      "Processing TTWO...\n",
      "Processing TTD...\n",
      "Processing VRSK...\n",
      "Processing CTSH...\n",
      "Processing DDOG...\n",
      "Processing ADSK...\n",
      "Processing WDAY...\n",
      "Processing CDNS...\n",
      "Processing SNPS...\n",
      "Processing MSTR...\n",
      "Processing FTNT...\n",
      "Processing MRVL...\n",
      "   symbol       MAE  r-square  classification accuracy\n",
      "0     PTC  0.605694  0.575707                 0.666667\n",
      "1      ON  1.112728  0.395235                 0.606061\n",
      "2     MDB  1.163653  0.653458                 0.458333\n",
      "3    SSNC  0.398650  0.533353                 0.611111\n",
      "4    VRSN  0.460197 -0.120305                 0.750000\n",
      "5     WDC  2.026242 -0.794968                 0.208333\n",
      "6     GFS  1.200959  0.832961                 0.571429\n",
      "7    NTAP  1.021625 -0.281023                 0.760000\n",
      "8    SMCI  3.118716  0.372591                 0.365854\n",
      "9      ZM  1.266449  0.274157                 0.592593\n",
      "10   MCHP  0.778310  0.618806                 0.450000\n",
      "11   ANSS  0.658223  0.545194                 0.629630\n",
      "12     ZS  1.024089  0.487709                 0.382353\n",
      "13     EA  0.428711  0.444196                 0.676471\n",
      "14   MPWR  1.235658  0.234288                 0.421053\n",
      "15   TTWO  0.799001  0.252884                 0.629630\n",
      "16    TTD  1.192155  0.363592                 0.540541\n",
      "17   VRSK  0.624285  0.385934                 0.565217\n",
      "18   CTSH  0.407678  0.713996                 0.709677\n",
      "19   DDOG  1.259995  0.467485                 0.543478\n",
      "20   ADSK  0.890196 -0.089471                 0.666667\n",
      "21   WDAY  1.407053 -1.014945                 0.605263\n",
      "22   CDNS  0.971895  0.465640                 0.465116\n",
      "23   SNPS  1.562491 -0.199394                 0.439024\n",
      "24   MSTR  4.495164 -0.071054                 0.305556\n",
      "25   FTNT  0.910122  0.472872                 0.574468\n",
      "26   MRVL  2.925897 -0.007187                 0.318182\n"
     ]
    }
   ],
   "source": [
    "# **Main Execution Loop**\n",
    "for ticker in ticker_all:\n",
    "    process_ticker(ticker, conn, nyse, stop_words, negative_words_set, result_df)\n",
    "\n",
    "# **Close Database Connection**\n",
    "conn.close()\n",
    "\n",
    "print(result_df)\n",
    "# **Save Final Results**\n",
    "result_df.to_csv(\"result_data_words_stock_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84157c-4011-4d11-be77-0a76cf6dba46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
