{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaea71f-7ac3-4fe9-af94-a630b03b9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBRegressor\n",
    "import pandas_market_calendars as mcal\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92a5577-937d-4f47-ac5d-3480b1ecd0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hilun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hilun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure stopwords and tokenizer are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d408b29-9e5b-4650-8e89-13cd306fd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "DB_PATH = Path(os.getenv(\"DB_PATH\"))\n",
    "DB_FILE = os.getenv(\"DB_FILE\")\n",
    "VIX_FILE= os.getenv(\"VIX_FILE\")\n",
    "duckdb_path = DB_PATH / DB_FILE\n",
    "vix_path  =DB_PATH / VIX_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643f033a-3010-48a3-a45a-ed20ace345e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will extract all ticker names and save it in a list\n",
    "\n",
    "# Database connection\n",
    "#db_file_path = r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\financial_news.db\"\n",
    "db_file_path=duckdb_path\n",
    "conn = duckdb.connect(database=db_file_path, read_only=False)    \n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT DISTINCT ticker FROM \"Headlines\".\"Daily_Price_Movement\";\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and fetch results into a DataFrame\n",
    "ticker_a = conn.execute(query).fetchdf()\n",
    "\n",
    "#Convert to a unique ticker list\n",
    "unique_tickers = ticker_a[\"ticker\"].tolist()\n",
    "ticker_all=unique_tickers\n",
    "# ticker_file=pd.read_csv('ticker with over1000 AC.csv')\n",
    "# ticker_all = ticker_file[\"symbol\"]  # Extract column\n",
    "\n",
    "#ticker_all=['AAPL'] #uncomment this for troubleshooting with one stock\n",
    "\n",
    "# **Close Database Connection**\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d284eee-ce05-4db8-9b4b-1bd2eb9e815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYSE trading calendar\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "start_time = time.time()  # Start time measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d693af-7e4c-4fc7-88e4-ebf635a46bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the next trading day\n",
    "def next_trading_day(date):\n",
    "    date = pd.Timestamp(date)\n",
    "    while len(nyse.valid_days(start_date=date.strftime('%Y-%m-%d'), end_date=date.strftime('%Y-%m-%d'))) == 0:\n",
    "        date += pd.Timedelta(days=1)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf28c636-c1e5-4945-abf2-648b0622ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text tokenization including NER removal\n",
    "def tokenize_text(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # Extract named entities\n",
    "    named_entities = {ent.text.lower() for ent in doc.ents}\n",
    "\n",
    "    # Tokenize and filter words\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [\n",
    "        word for word in words \n",
    "        if word.isalpha() and len(word) > 2 and \n",
    "        word not in stop_words and \n",
    "        word not in named_entities  # Remove named entities\n",
    "    ]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3282c3de-4a5b-473c-83c5-52fde43b683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate bollinger band\n",
    "def bb(price, lookback):\n",
    "    std=price.rolling(window=lookback, min_periods=lookback).std()\n",
    "    sma=price.rolling(window=lookback, min_periods=lookback).mean()\n",
    "    bottom=sma-(2*std)\n",
    "    top=sma+(2*std)\n",
    "    return sma, top, bottom\n",
    "\n",
    "#function to calculate relative strength index\n",
    "def get_rsi(price, lookback):\n",
    "    daily_ret=price.diff()\n",
    "    up = daily_ret.clip(lower=0)\n",
    "    down = -1 * daily_ret.clip(upper=0)\n",
    "    sma_up = up.rolling(window = lookback).mean()\n",
    "    sma_down = down.rolling(window = lookback).mean()  \n",
    "    rs=sma_up/sma_down\n",
    "    rsi=100-(100/(1+rs))\n",
    "    return rsi        \t   \t\t  \t\t \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "\n",
    "#function to calculate stochastic osillator\n",
    "def get_so(price, lookback):\n",
    "    high = price.rolling(window=lookback).max()\n",
    "    low = price.rolling(window=lookback).min()\n",
    "    K = 100*(price-low)/(high-low)\n",
    "    D = K.rolling(window=3).mean()\n",
    "\n",
    "    return(K,D)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc84f0d-11df-4576-9c8b-9c09976cea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This the main function to process a single ticker\n",
    "def process_ticker(ticker, nyse, stop_words, result_df):\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    # Database connection\n",
    "    # I chose to open and close a db connection inside this function to avoid issues\n",
    "    # I encountered inaccurate results when I opened and close a connect outside of loop\n",
    "\n",
    "    #*****************************************************************************\n",
    "    #change this to your local path\n",
    "    db_file_path = r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\financial_news.db\" \n",
    "    #*****************************************************************************\n",
    "    conn = duckdb.connect(database=db_file_path, read_only=False)    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            a.mapped_trading_date AS publish_date,\n",
    "            a.description, \n",
    "            a.article_pubDate,\n",
    "            dpm.close_price\n",
    "        FROM \"Headlines\".\"Articles_Trading_Day\" a\n",
    "        INNER JOIN \"Headlines\".\"Daily_Price_Movement\" dpm\n",
    "        ON a.mapped_trading_date = dpm.trading_date  \n",
    "        WHERE a.ticker = ?\n",
    "        AND dpm.ticker = ?;\n",
    "    \"\"\"\n",
    "    \n",
    "    news_df = conn.execute(query, [ticker, ticker]).fetchdf()\n",
    "    article_count=len(news_df)\n",
    "    if news_df.empty:\n",
    "        print(f\"Skipping {ticker}: No data found.\")\n",
    "        return\n",
    "\n",
    "    news_df[\"publish_date\"] = pd.to_datetime(news_df[\"publish_date\"]).dt.date\n",
    "    news_df[\"description\"] = news_df[\"description\"].fillna(\"\")\n",
    "\n",
    "    # Group descriptions by date\n",
    "    news_df = news_df.groupby(\"publish_date\", as_index=False).agg({\n",
    "        \"description\": lambda x: \" \".join(x),\n",
    "        \"close_price\": \"first\",\n",
    "    })\n",
    "\n",
    "    # Adjust for non-trading days\n",
    "    news_df[\"adjusted_date\"] = news_df[\"publish_date\"].apply(next_trading_day)\n",
    "\n",
    "    # Re-group after adjusting trading days\n",
    "    news_df = news_df.groupby(\"adjusted_date\", as_index=False).agg({\n",
    "        \"description\": lambda x: \" \".join(x),\n",
    "        \"close_price\": \"last\",\n",
    "    })\n",
    "    \n",
    "    news_df[\"price_change_percentage\"] = ((news_df[\"close_price\"].shift(-1) - news_df[\"close_price\"]) / news_df[\"close_price\"]) * 100\n",
    "    news_df[\"tokenized_words\"] = news_df[\"description\"].astype(str).apply(tokenize_text)\n",
    "\n",
    "    # Calculate token scores\n",
    "    unique_words = set(word for words_list in news_df[\"tokenized_words\"] for word in words_list)\n",
    "    w_count=len(unique_words)\n",
    "    word_scores = {word: [] for word in unique_words}\n",
    "\n",
    "    for _, row in news_df.iterrows():\n",
    "        words_list = row[\"tokenized_words\"]\n",
    "        price_change = row[\"price_change_percentage\"]\n",
    "        total_words = len(words_list)\n",
    "\n",
    "        if total_words > 0:\n",
    "            word_counts = {word: words_list.count(word) / total_words for word in words_list}\n",
    "            for word, ratio in word_counts.items():\n",
    "                word_scores[word].append(ratio * price_change)\n",
    "\n",
    "    tk_info = pd.DataFrame({\n",
    "        \"word\": list(word_scores.keys()),\n",
    "        \"score\": [np.mean(scores) if scores else 0 for scores in word_scores.values()]\n",
    "    }).dropna()\n",
    "    token_scores_dict = dict(zip(tk_info[\"word\"], tk_info[\"score\"]))\n",
    "    \n",
    "    def calculate_token_score(tokens):\n",
    "        return sum(token_scores_dict.get(token, 0) for token in tokens)\n",
    "\n",
    "    news_df[\"token_score\"] = news_df[\"tokenized_words\"].apply(calculate_token_score)\n",
    "\n",
    "    # Ensure no missing values in price change\n",
    "    news_df = news_df.dropna()\n",
    "    \n",
    "    # Define the SQL query\n",
    "    query = \"\"\"\n",
    "        SELECT trading_day_date, price \n",
    "        FROM Headlines.Pricing_News\n",
    "        WHERE ticker = ?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query and load the data into a pandas DataFrame\n",
    "    df_p=conn.execute(query,[ticker]).fetchdf()\n",
    "\n",
    "    # Adjust for non-trading days\n",
    "    df_p = df_p.rename(columns={\"trading_day_date\": \"trading_date\"})\n",
    "    df_p = df_p.rename(columns={\"price\": \"close_price\"})\n",
    "\n",
    "    df_p[\"adjusted_trading_date\"] =df_p[\"trading_date\"].apply(next_trading_day)\n",
    "    df_p = df_p.drop_duplicates(subset=['adjusted_trading_date'], keep='last')\n",
    "\n",
    "    # Ensure 'trading_date' is in datetime format\n",
    "    df_p['trading_date'] = pd.to_datetime(df_p['trading_date'])\n",
    "    news_df['adjusted_date'] = pd.to_datetime(news_df['adjusted_date'])  # Ensure news_df dates are also datetime\n",
    "\n",
    "    if news_df['adjusted_date'].empty:\n",
    "        print(f\"Skipping {ticker}: No data found.\")\n",
    "        return\n",
    "    \n",
    "    # Get start_date and end_date\n",
    "    start_date = news_df['adjusted_date'].iloc[0]  # First news date\n",
    "    end_date = news_df['adjusted_date'].iloc[-1]   # Last news date\n",
    "    \n",
    "    # Compute the date range\n",
    "    date_lower_bound = start_date - pd.Timedelta(days=30)\n",
    "    date_upper_bound = end_date + pd.Timedelta(days=30)\n",
    "    \n",
    "    #print(date_lower_bound, date_upper_bound )\n",
    "    \n",
    "    # Filter df_p to include only trading dates within this range\n",
    "    df_p = df_p[(df_p['trading_date'] >= date_lower_bound) & (df_p['trading_date'] <= date_upper_bound)]\n",
    "    # Convert to a pandas Series\n",
    "    price = df_p['close_price']\n",
    "    \n",
    "    #add BB columns: SMA, Bottom, Top\n",
    "    sma, bot, top=bb(price,14)\n",
    "    rsi=get_rsi(price,4)\n",
    "    #Stochastic Oscillator\n",
    "    K, D=get_so(price,14)\n",
    "    \n",
    "    df_p['bb_sma']=sma\n",
    "    df_p['bb_bottom']=bot\n",
    "    df_p['bb_top']=top\n",
    "    df_p['rsi']=rsi\n",
    "    df_p['K']=K\n",
    "    df_p['D']=D\n",
    "\n",
    "    df_p['sma_future_7'] = df_p['close_price'].rolling(window=7, min_periods=1).mean().shift(-7)\n",
    "    \n",
    "    df_p = df_p.reset_index(drop=True)  # Reset index and remove old index\n",
    "    #handling edge cases for the last 7 days\n",
    "    t_len=len(df_p)\n",
    "    for i in range(6):\n",
    "        future_values = df_p['close_price'].iloc[t_len-1-i:t_len]  # Get up to 6 future values\n",
    "        df_p.at[t_len-i-2, 'sma_future_7'] = future_values.mean()  # Assign safely  # Compute mean of available values\n",
    "\n",
    "    news_df = news_df.merge(\n",
    "    df_p[['adjusted_trading_date', 'bb_sma', 'bb_bottom', 'bb_top', 'rsi', 'K','D', 'sma_future_7']],\n",
    "    left_on='adjusted_date',\n",
    "    right_on='adjusted_trading_date',\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    news_df = news_df.drop(columns=['adjusted_trading_date'])\n",
    "    \n",
    "    #calculate the trigger column\n",
    "    news_df[\"bb_trigger\"] = 0  # Default value\n",
    "    news_df.loc[(news_df[\"close_price\"].shift(1) > news_df[\"bb_top\"].shift(1)) & (news_df[\"close_price\"] < news_df[\"bb_top\"]), \"bb_trigger\"] = -1\n",
    "    news_df.loc[(news_df[\"close_price\"].shift(1) < news_df[\"bb_bottom\"].shift(1)) & (news_df[\"close_price\"] > news_df[\"bb_bottom\"]), \"bb_trigger\"] = 1\n",
    "    # Calculate the RSI trigger column\n",
    "    news_df[\"rsi_trigger\"] = 0  # Default value\n",
    "    news_df.loc[news_df[\"rsi\"] > 70, \"rsi_trigger\"] = -1\n",
    "    news_df.loc[news_df[\"rsi\"] < 30, \"rsi_trigger\"] = 1 \n",
    "    \n",
    "    news_df['bb_sma']=sma\n",
    "    news_df['bb_bottom']=bot\n",
    "    news_df['bb_top']=top\n",
    "    news_df['rsi']=rsi\n",
    "    news_df['K']=K\n",
    "    news_df['D']=D\n",
    "\n",
    "    #************************************************************\n",
    "    #Make sure you add this file to your local path\n",
    "    #add vix column\n",
    "    #vix_df = pd.read_csv(\"vixGaTechSP25.csv\")\n",
    "    vix_df=pd.read_csv(vix_path)\n",
    "    #************************************************************\n",
    "    \n",
    "    # Rename columns (assumes first column is date and second is VIX value)\n",
    "    vix_df = vix_df.rename(columns={vix_df.columns[0]: \"adjusted_date\", vix_df.columns[1]: \"vix\"})\n",
    "    \n",
    "    # Convert both date columns to datetime\n",
    "    news_df[\"adjusted_date\"] = pd.to_datetime(news_df[\"adjusted_date\"])\n",
    "    vix_df[\"adjusted_date\"] = pd.to_datetime(vix_df[\"adjusted_date\"])\n",
    "\n",
    "    # Now merge\n",
    "    news_df = pd.merge(news_df, vix_df, on=\"adjusted_date\", how=\"left\")\n",
    "   \n",
    "    #select single variable\n",
    "    #X_combined = news_df[\"token_score\"].values.reshape(-1, 1)  # Use token score as feature\n",
    "    \n",
    "    # Select multiple feature columns\n",
    "    X_combined = news_df[[\"token_score\", \"vix\"]].values \n",
    "    X_combined = np.array(X_combined).reshape(-1, 2)  # 2 features  \n",
    "\n",
    "    #This is for Technical Analysis addtion\n",
    "    # X_combined = news_df[[\"token_score\", \"vix\",\"rsi\",\"K\",\"D\",'bb_sma','bb_bottom','bb_top']].values \n",
    "    # X_combined = np.array(X_combined).reshape(-1, 8)  # 8 features\n",
    "    y = news_df[\"price_change_percentage\"].values\n",
    "\n",
    "    # Train-Test Split\n",
    "    split_index = int(len(news_df) * 0.8)\n",
    "    X_train, X_test = X_combined[:split_index], X_combined[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    # Train XGBoost Model\n",
    "    xgb_model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    #4 average groups with 6 categories\n",
    "    ave = news_df[\"price_change_percentage\"].abs().mean()\n",
    "    if ave > 3:\n",
    "        l = [7, 3.5, 0, -3.5, -7]\n",
    "    elif ave >= 2:\n",
    "        l = [4, 2, 0, -2, -4]\n",
    "    elif ave >= 1:\n",
    "        l = [3, 1.5, 0, -1.5, -3]\n",
    "    else:\n",
    "        l = [2.5, 1.25, 0, -1.25, -2.5]\n",
    "\n",
    "    # Categorization/Classification function\n",
    "    def categorize_value(x):\n",
    "        if x > l[0]:\n",
    "            return 0\n",
    "        elif l[1] <= x <= l[0]:\n",
    "            return 1\n",
    "        elif l[2] <= x < l[1]:\n",
    "            return 2\n",
    "        elif l[3] <= x < l[2]:\n",
    "            return 3\n",
    "        elif l[4] <= x < l[3]:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "        \n",
    "    start_row = split_index - 1\n",
    "\n",
    "    # Apply categorization\n",
    "    news_df.loc[news_df.index > start_row, \"actual category\"] = news_df.loc[news_df.index > start_row, \"price_change_percentage\"].apply(categorize_value)\n",
    "    news_df[\"predicted_price_change\"] = np.concatenate([np.full(split_index, np.nan), y_pred])\n",
    "    news_df.loc[news_df.index > start_row, \"predicted category\"] = news_df.loc[news_df.index > start_row, \"predicted_price_change\"].apply(categorize_value)\n",
    "\n",
    "    # Compute classification accuracy\n",
    "    df_filtered = news_df.iloc[split_index:].reset_index(drop=True)[[\"actual category\", \"predicted category\"]]\n",
    "    accuracy = accuracy_score(df_filtered[\"actual category\"], df_filtered[\"predicted category\"])\n",
    "    \n",
    "    true_p = ((news_df[\"actual category\"] > 3) & (news_df[\"predicted category\"] > 3)).sum()\n",
    "    false_neg=((news_df[\"actual category\"] > 3) & (news_df[\"predicted category\"] < 4)).sum()\n",
    "    false_p=((news_df[\"actual category\"] < 4) & (news_df[\"predicted category\"] > 3)).sum()\n",
    "    true_neg=((news_df[\"actual category\"] <4) & (news_df[\"predicted category\"] <4)).sum()\n",
    "    precision=true_p/(true_p+false_p)\n",
    "    recall=true_p/(true_p+false_neg)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    # Store results\n",
    "    result_df.loc[len(result_df)] = [ticker, mae, r2, accuracy, article_count,w_count, len(news_df),precision,recall, f1,true_p,false_neg,false_p,true_neg]\n",
    "    \n",
    "    # **Close Database Connection**\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa83cc3-31a5-44e9-9315-43ff8ad3d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing STAG...\n",
      "Processing FIS...\n",
      "Processing CHK...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# **Main Execution Loop**\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m ticker_all:\n\u001b[1;32m----> 7\u001b[0m     process_ticker(ticker, nyse, stop_words, result_df)\n\u001b[0;32m      9\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End time measurement\u001b[39;00m\n\u001b[0;32m     10\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Calculate elapsed time\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 96\u001b[0m, in \u001b[0;36mprocess_ticker\u001b[1;34m(ticker, nyse, stop_words, result_df)\u001b[0m\n\u001b[0;32m     93\u001b[0m df_p \u001b[38;5;241m=\u001b[39m df_p\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrading_day_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrading_date\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     94\u001b[0m df_p \u001b[38;5;241m=\u001b[39m df_p\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_price\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m---> 96\u001b[0m df_p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_trading_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39mdf_p[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrading_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(next_trading_day)\n\u001b[0;32m     97\u001b[0m df_p \u001b[38;5;241m=\u001b[39m df_p\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madjusted_trading_date\u001b[39m\u001b[38;5;124m'\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Ensure 'trading_date' is in datetime format\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:919\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    916\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:81\u001b[0m, in \u001b[0;36mravel_compat.<locals>.method\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(meth)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmethod\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m meth(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     83\u001b[0m     flags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray\u001b[38;5;241m.\u001b[39mflags\n\u001b[0;32m     84\u001b[0m     flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mravel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:740\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.map\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;129m@ravel_compat\u001b[39m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, mapper, na_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Index\n\u001b[1;32m--> 740\u001b[0m     result \u001b[38;5;241m=\u001b[39m map_array(\u001b[38;5;28mself\u001b[39m, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m    741\u001b[0m     result \u001b[38;5;241m=\u001b[39m Index(result)\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ABCMultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mnext_trading_day\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_trading_day\u001b[39m(date):\n\u001b[0;32m      3\u001b[0m     date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(date)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nyse\u001b[38;5;241m.\u001b[39mvalid_days(start_date\u001b[38;5;241m=\u001b[39mdate\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m), end_date\u001b[38;5;241m=\u001b[39mdate\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m))) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      5\u001b[0m         date \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m date\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas_market_calendars\\calendars\\nyse.py:1321\u001b[0m, in \u001b[0;36mNYSEExchangeCalendar.valid_days\u001b[1;34m(self, start_date, end_date, tz)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;66;03m# Don't care about Saturdays. Call super.\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_date \u001b[38;5;241m>\u001b[39m saturday_end:\n\u001b[1;32m-> 1321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mvalid_days(start_date, end_date, tz\u001b[38;5;241m=\u001b[39mtz)\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;66;03m# Full Date Range is pre 1952. Augment the Super call\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_date \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m saturday_end:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas_market_calendars\\market_calendar.py:582\u001b[0m, in \u001b[0;36mMarketCalendar.valid_days\u001b[1;34m(self, start_date, end_date, tz)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalid_days\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_date, end_date, tz\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDatetimeIndex:\n\u001b[0;32m    574\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;124;03m    Get a DatetimeIndex of valid open business days.\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m    :return: DatetimeIndex of valid business days\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mdate_range(\n\u001b[0;32m    583\u001b[0m         start_date, end_date, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholidays(), normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tz\u001b[38;5;241m=\u001b[39mtz\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:1008\u001b[0m, in \u001b[0;36mdate_range\u001b[1;34m(start, end, periods, freq, tz, normalize, name, inclusive, unit, **kwargs)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m com\u001b[38;5;241m.\u001b[39many_none(periods, start, end):\n\u001b[0;32m   1006\u001b[0m     freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1008\u001b[0m dtarr \u001b[38;5;241m=\u001b[39m DatetimeArray\u001b[38;5;241m.\u001b[39m_generate_range(\n\u001b[0;32m   1009\u001b[0m     start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[0;32m   1010\u001b[0m     end\u001b[38;5;241m=\u001b[39mend,\n\u001b[0;32m   1011\u001b[0m     periods\u001b[38;5;241m=\u001b[39mperiods,\n\u001b[0;32m   1012\u001b[0m     freq\u001b[38;5;241m=\u001b[39mfreq,\n\u001b[0;32m   1013\u001b[0m     tz\u001b[38;5;241m=\u001b[39mtz,\n\u001b[0;32m   1014\u001b[0m     normalize\u001b[38;5;241m=\u001b[39mnormalize,\n\u001b[0;32m   1015\u001b[0m     inclusive\u001b[38;5;241m=\u001b[39minclusive,\n\u001b[0;32m   1016\u001b[0m     unit\u001b[38;5;241m=\u001b[39munit,\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1018\u001b[0m )\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeIndex\u001b[38;5;241m.\u001b[39m_simple_new(dtarr, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:443\u001b[0m, in \u001b[0;36mDatetimeArray._generate_range\u001b[1;34m(cls, start, end, periods, freq, tz, normalize, ambiguous, nonexistent, inclusive, unit)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     end \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m.\u001b[39mas_unit(unit, round_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 443\u001b[0m left_inclusive, right_inclusive \u001b[38;5;241m=\u001b[39m validate_inclusive(inclusive)\n\u001b[0;32m    444\u001b[0m start, end \u001b[38;5;241m=\u001b[39m _maybe_normalize_endpoints(start, end, normalize)\n\u001b[0;32m    445\u001b[0m tz \u001b[38;5;241m=\u001b[39m _infer_tz_from_endpoints(start, end, tz)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_validators.py:398\u001b[0m, in \u001b[0;36mvalidate_inclusive\u001b[1;34m(inclusive)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosed has to be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m left_closed, right_closed\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_inclusive\u001b[39m(inclusive: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Check that the `inclusive` argument is among {\"both\", \"neither\", \"left\", \"right\"}.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m    ValueError : if argument is not among valid values\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     left_right_inclusive: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define column names\n",
    "columns = [\"symbol\", \"MAE\", \"r-square\", \"all categories classification accuracy\",\"article count\",\"word count\", \"total days used for analysis\",'Price drop precision','price drop recall','price drop f1 score','true positive','false negative','false positive','true negative']\n",
    "result_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# **Main Execution Loop**\n",
    "for ticker in ticker_all:\n",
    "    process_ticker(ticker, nyse, stop_words, result_df)\n",
    "\n",
    "end_time = time.time()  # End time measurement\n",
    "elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "\n",
    "print(f\"Execution Time: {elapsed_time:.5f} seconds\")  # Print execution time\n",
    "\n",
    "print(result_df)\n",
    "# **Save Final Results**\n",
    "result_df.to_csv(\"result_data_words_stock_score.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
