{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Files](#this-is-to-create-the-db)\n",
    "* [Code](#code-to-create-database)\n",
    "    * [Headlines Schema](#headlines-schema)\n",
    "    * [SP500 Schema](#sp500-schema)\n",
    "    * [Finbert](#finbert-related-stuff)\n",
    "    * [Finbert 10ks](#10k-finbert)\n",
    "    * [VIX](#vix)\n",
    "    * [VIX Preds](#vix-preds)\n",
    "    * [Finbert All Tags](#finbert-all-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *THIS IS TO CREATE THE DB*\n",
    "You can download the already made DB file `financial_news.db` from the sharepoint \n",
    "\n",
    "[practicum folder](https://gtvault-my.sharepoint.com/:f:/g/personal/ltupac3_gatech_edu/Eg2gLDzQ8H1JoWUrUIq1G04BPkOXMyxmhgcoL84Q58-5dg?e=80dziH)\n",
    "\n",
    "[db file](https://gtvault-my.sharepoint.com/:u:/g/personal/ltupac3_gatech_edu/Edi6YX6MKPxMud1e5maTIjsBo04ISTst1j7uoxeSVH2OBA?e=XQD3Ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd \n",
    "import os \n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "DB_PATH = Path(os.getenv(\"DB_PATH\"))\n",
    "DB_FILE = os.getenv(\"DB_FILE\")\n",
    "duckdb_path = DB_PATH / DB_FILE\n",
    "\n",
    "COMPANY_TXT_PATH = Path(os.getenv(\"company_txt_path\"))\n",
    "COMPANY_TXT_FILE = os.getenv(\"company_txt_file\")\n",
    "COMPANY_TXT = COMPANY_TXT_PATH / COMPANY_TXT_FILE\n",
    "\n",
    "VOLUME_NEWS_PATH = Path(os.getenv(\"volume_news_path\"))\n",
    "VOLUME_NEWS_FILE = os.getenv(\"volume_news_file\")\n",
    "VOLUME_NEWS = VOLUME_NEWS_PATH / VOLUME_NEWS_FILE\n",
    "\n",
    "PRICING_NEWS_PATH = Path(os.getenv(\"pricing_news_path\"))\n",
    "PRICING_NEWS_FILE = os.getenv(\"pricing_news_file\")\n",
    "PRICING_NEWS = PRICING_NEWS_PATH / PRICING_NEWS_FILE\n",
    "\n",
    "MULTICAP_HEADLINES = Path(os.getenv(\"multicap_headlines\"))\n",
    "HEADLINES_PATH = Path(os.getenv(\"headline_august24_path\"))\n",
    "\n",
    "SP500_VOLUME_WEEKLY_PATH = Path(os.getenv(\"sp500_volume_weekly_path\"))\n",
    "SP500_PRICE_WEEKLY_PATH = Path(os.getenv(\"sp500_price_weekly_path\"))\n",
    "SP500_PRICE_DAILY_PATH = Path(os.getenv(\"sp500_price_daily_path\"))\n",
    "SP500_COMPANY_PATH = Path(os.getenv(\"sp500_company_path\"))\n",
    "SP500_PRICE_SP500_PATH = Path(os.getenv(\"sp500_price_sp500_path\"))\n",
    "SP500_ITEM1_PATH = Path(os.getenv(\"sp500_item1_path\"))\n",
    "SP500_ITEM1A_PATH = Path(os.getenv(\"sp500_item1a_path\"))\n",
    "SP500_ITEM7_PATH = Path(os.getenv(\"sp500_item7_path\"))\n",
    "\n",
    "FINBERT_CSV_PATH = Path(os.getenv(\"finbert_csv\"))\n",
    "FINBERT_CSV_10k_PATH = Path(os.getenv(\"finbert_10k_csv\"))\n",
    "\n",
    "# sp500_volume_weekly_path = 'SP500\\\\volume.csv'\n",
    "# sp500_price_weekly_path = 'SP500\\\\price.csv'\n",
    "# sp500_price_daily_path = 'SP500\\\\price_daily.csv'\n",
    "# sp500_company_path = 'SP500\\\\company_info_sp500.txt'\n",
    "# sp500_price_sp500_path = 'SP500\\\\price_SP500.csv'\n",
    "# sp500_item1_path = 'SP500\\\\sp500_item1_sec_filings_0.txt'\n",
    "# sp500_item1a_path = 'SP500\\\\sp500_item1a_sec_filings_0.txt'\n",
    "# sp500_item7_path = 'SP500\\\\sp500_item7_sec_filings_0.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create database\n",
    "Instructions:\n",
    "1. For individual files (company_info_news.txt, volume_news.csv, etc.), copy the relative path to the respective variable below\n",
    "2. For headline data, put the relative path to the folder housing the ticker folders.\n",
    "    - Example: MultiCap_News/HEADLINES houses the individual ticker folders. \n",
    "    - The code will recursively pick up the files from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headlines Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "\n",
    "company_txt_path = COMPANY_TXT\n",
    "# volume_news_path = 'MultiCap_News\\\\volume_news.csv'\n",
    "volume_news_path = VOLUME_NEWS\n",
    "# pricing_news_path = 'MultiCap_News\\\\pricing_news.csv'\n",
    "pricing_news_path = PRICING_NEWS\n",
    "multicap_headlines = MULTICAP_HEADLINES\n",
    "headline_august24_path = HEADLINES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DDLs and indexes\n",
    "ddl_statements = [\n",
    "    \"CREATE SCHEMA IF NOT EXISTS Headlines;\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Articles (\n",
    "        guid TEXT,\n",
    "        ticker TEXT,\n",
    "        description TEXT,\n",
    "        article_link TEXT,\n",
    "        article_pubDate TIMESTAMP,\n",
    "        article_title TEXT,\n",
    "        language TEXT,\n",
    "        lastBuildDate TIMESTAMP,\n",
    "        link TEXT,\n",
    "        title TEXT,\n",
    "        PRIMARY KEY (guid, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Company_Info_News (\n",
    "        ticker TEXT PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        subindustry TEXT\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Pricing_News (\n",
    "        trading_day_date DATE,\n",
    "        ticker TEXT,\n",
    "        price FLOAT,\n",
    "        PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Volume_News (\n",
    "        trading_day_date DATE,\n",
    "        ticker TEXT,\n",
    "        volume INT,\n",
    "        PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    ############ Gold Layer ############\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Market_Data_Daily_Processing (\n",
    "    trading_day_date DATE,\n",
    "    ticker TEXT,\n",
    "    price FLOAT,\n",
    "    volume INT,\n",
    "    PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Market_Data_Headlines (\n",
    "    trading_day_date DATE,\n",
    "    ticker TEXT,\n",
    "    price FLOAT,\n",
    "    volume INT,\n",
    "    headline_count INT,\n",
    "    PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Trading_Calendar (\n",
    "    trading_date DATE PRIMARY KEY\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Articles_Trading_Day (\n",
    "    guid TEXT,\n",
    "    ticker TEXT,\n",
    "    mapped_trading_date DATE,\n",
    "    description TEXT,\n",
    "    article_link TEXT,\n",
    "    article_pubDate TIMESTAMP,\n",
    "    article_title TEXT,\n",
    "    language TEXT,\n",
    "    lastBuildDate TIMESTAMP,\n",
    "    link TEXT,\n",
    "    title TEXT,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Market_Article_Summary (\n",
    "    trading_date DATE PRIMARY KEY,\n",
    "    article_count INT\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Daily_Price_Movement (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    close_price FLOAT,\n",
    "    next_trading_day DATE,\n",
    "    close_price_next FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    PRIMARY KEY (trading_date, ticker),\n",
    "    FOREIGN KEY (trading_date) REFERENCES headlines.trading_calendar(trading_date),\n",
    "    FOREIGN KEY (next_trading_day) REFERENCES headlines.trading_calendar(trading_date)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Weekly_Price_Movement (\n",
    "    trading_week_start DATE,\n",
    "    ticker TEXT,\n",
    "    close_price_start FLOAT,\n",
    "    trading_week_end DATE,\n",
    "    close_price_end FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    PRIMARY KEY (trading_week_start, ticker),\n",
    "    FOREIGN KEY (trading_week_start) REFERENCES headlines.trading_calendar(trading_date),\n",
    "    FOREIGN KEY (trading_week_end) REFERENCES headlines.trading_calendar(trading_date)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.extreme_price_movements (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    close_price FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    movement_type TEXT,  -- Drop|Surge\n",
    "    PRIMARY KEY (trading_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.articles_extreme_drops (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    guid TEXT,\n",
    "    mapped_trading_date DATE, \n",
    "    title_sentiment_score FLOAT,\n",
    "    title_sentiment_label TEXT,\n",
    "    description_sentiment_score FLOAT,\n",
    "    description_sentiment_label TEXT,\n",
    "    PRIMARY KEY (trading_date, ticker, guid)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop_statements = [\n",
    "    \"DROP TABLE IF EXISTS headlines.Articles;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Company_Info_News;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Pricing_News;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Volume_News;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Market_Data_Daily_Processing;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Market_Data_Headlines;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Trading_Calendar;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Articles_Trading_Day;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Market_Article_Summary;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Daily_Price_Movement;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.Weekly_Price_Movement;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.extreme_price_movements;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.articles_extreme_drops;\"\n",
    "]\n",
    "\n",
    "index_statements = [\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_articles_pubDate ON headlines.Articles (article_pubDate);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_articles_pubDate ON headlines.Articles_Trading_Day (article_pubDate);\"\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_stock_movement_ticker ON headlines.Daily_Price_Movement (ticker);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_stock_movement_ticker ON headlines.Weekly_Price_Movement (ticker);\"\n",
    "]\n",
    "\n",
    "for drop in drop_statements:\n",
    "    con.execute(drop)\n",
    "\n",
    "for ddl in ddl_statements:\n",
    "    con.execute(ddl)\n",
    "\n",
    "for index in index_statements:\n",
    "    con.execute(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Company_Info_News`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its all in one line\n",
    "with open(company_txt_path, 'r') as file:\n",
    "    lines = file.readline().split('\\\\n')\n",
    "    # con.execute(\"TRUNCATE Company_Info_News\")\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split('|')\n",
    "        # DONT RUN THIS TWICE BY MISTAKE!\n",
    "        con.execute(\"INSERT INTO headlines.Company_Info_News VALUES (?,?,?)\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Volume_News` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(volume_news_path)\n",
    "# df.head()\n",
    "\n",
    "# convert the wide format to long format\n",
    "volume_long_df = df.melt(id_vars=['Date'], var_name='Ticker', value_name='Volume')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "volume_long_df['Date'] = pd.to_datetime(volume_long_df['Date'])\n",
    "volume_long_df['Volume'] = pd.to_numeric(volume_long_df['Volume'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE Volume_News\")\n",
    "con.execute(\"INSERT INTO headlines.Volume_News (trading_day_date, ticker, Volume) SELECT Date, ticker, Volume FROM volume_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Pricing_News`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(pricing_news_path)\n",
    "# convert the wide format to long format\n",
    "pricing_long_df = df.melt(id_vars=['Date'], var_name='Ticker', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "pricing_long_df['Date'] = pd.to_datetime(pricing_long_df['Date'])\n",
    "pricing_long_df['Price'] = pd.to_numeric(pricing_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE Pricing_News\")\n",
    "con.execute(\"INSERT INTO headlines.Pricing_News (trading_day_date, Ticker, price) SELECT Date, ticker, Price FROM pricing_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Market_Data_Daily_Processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"TRUNCATE Market_Data_Daily_Processing\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO headlines.Market_Data_Daily_Processing\n",
    "SELECT \n",
    "    pn.trading_day_date,\n",
    "    pn.ticker,\n",
    "    pn.price,\n",
    "    vn.volume\n",
    "FROM \n",
    "    headlines.Pricing_News pn\n",
    "LEFT JOIN \n",
    "    headlines.Volume_News vn \n",
    "ON \n",
    "    pn.trading_day_date = vn.trading_day_date AND pn.ticker = vn.ticker\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Trading_Calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pricing_dates = pricing_long_df['Date'].drop_duplicates()\n",
    "# volume_dates = volume_long_df['Date'].drop_duplicates()\n",
    "\n",
    "# trading_dates = pd.concat([pricing_dates, volume_dates]).drop_duplicates().sort_values()\n",
    "\n",
    "# # make sure to have correct col name\n",
    "# trading_dates_df = pd.DataFrame(trading_dates, columns=['trading_date'])\n",
    "# con.execute(\"TRUNCATE Trading_Calendar\")\n",
    "con.execute(\n",
    "\"\"\"\n",
    "INSERT INTO headlines.Trading_Calendar\n",
    "SELECT DISTINCT trading_day_date AS trading_date\n",
    "FROM (\n",
    "    SELECT trading_day_date FROM headlines.Pricing_News\n",
    "    UNION\n",
    "    SELECT trading_day_date FROM headlines.Volume_News\n",
    ") AS all_dates\n",
    "ORDER BY trading_date;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_loader(base_dir):\n",
    "    # lets do this in chunks instead\n",
    "    failed_parses = pd.DataFrame()\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        data = [] \n",
    "        # extract ticker from foldername \n",
    "        ticker = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root_element = tree.getroot()\n",
    "                \n",
    "                channel = root_element.find('channel')\n",
    "                if channel is not None:\n",
    "                    # extract metadata info\n",
    "                    language = channel.findtext(\"language\") \n",
    "                    lastBuildDate = channel.findtext(\"lastBuildDate\")\n",
    "                    link = channel.findtext(\"link\")\n",
    "                    title = channel.findtext(\"title\")\n",
    "                    \n",
    "                    # now meat and potatoes\n",
    "                    for item in channel.findall(\"item\"):\n",
    "                        description = item.findtext(\"description\")\n",
    "                        guid = item.findtext(\"guid\")\n",
    "                        article_link = item.findtext(\"link\")\n",
    "                        article_pubDate = item.findtext(\"pubDate\")\n",
    "                        article_title = item.findtext(\"title\")\n",
    "                        \n",
    "                        data.append({\n",
    "                            \"guid\": guid,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"description\": description,\n",
    "                            \"article_link\": article_link,\n",
    "                            \"article_pubDate\": article_pubDate,\n",
    "                            \"article_title\": article_title,\n",
    "                            \"language\": language,\n",
    "                            \"lastBuildDate\": lastBuildDate,\n",
    "                            \"link\": link,\n",
    "                            \"title\": title\n",
    "                        })\n",
    "            except ET.parseError as e:\n",
    "                print(f\"Error parsing file {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "        \n",
    "        # insert the data into the database\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            # print(\"Performing timestamp coercion for\", ticker)\n",
    "            df['parsed_date'] = pd.to_datetime(df['article_pubDate'], errors='coerce')\n",
    "            df['lastBuildDate'] = pd.to_datetime(df['lastBuildDate'], errors='coerce')\n",
    "            # print(\"Done timestamp coercion for\", ticker)\n",
    "            \n",
    "            # separate failed cases to avoid nulls\n",
    "            current_failed = df[df['parsed_date'].isna()]\n",
    "            current_valid = df[df['parsed_date'].notna()]\n",
    "\n",
    "            # these are good\n",
    "            current_valid = current_valid.assign(article_pubDate=current_valid['parsed_date']).drop(columns=['parsed_date'])\n",
    "\n",
    "            # remove dupes on guid and ticker\n",
    "            current_valid.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "\n",
    "            \n",
    "            failed_parses = pd.concat([failed_parses, current_failed], ignore_index=True)\n",
    "            \n",
    "            try:\n",
    "                # adding this too just in case\n",
    "                con.execute(\"INSERT INTO headlines.Articles SELECT * FROM current_valid ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "                print(\"inserted data for\", ticker)\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting data for {ticker}: {e}\")\n",
    "                \n",
    "    failed_parses.to_csv(\"failed_article_dates.csv\", index=False)\n",
    "    return failed_parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING NULLS! gonna fix the coercion logic\n",
    "# con.execute(\"truncate Articles\")\n",
    "\n",
    "# load multicap headlines\n",
    "failed_df = xml_loader(multicap_headlines)\n",
    "# load new headlines\n",
    "failed2_df = xml_loader(headline_august24_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with the faulty data \n",
    "failed_df['article_pubDate'] = pd.to_datetime(failed_df['article_pubDate'], errors='coerce')\n",
    "failed_df = failed_df.drop(columns=['parsed_date'])\n",
    "failed_df.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "try:\n",
    "    # adding this too just in case\n",
    "    con.execute(\"INSERT INTO headlines.Articles SELECT * FROM failed_df ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with the faulty data \n",
    "failed2_df['article_pubDate'] = pd.to_datetime(failed2_df['article_pubDate'], errors='coerce')\n",
    "failed2_df = failed2_df.drop(columns=['parsed_date'])\n",
    "failed2_df.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "try:\n",
    "    # adding this too just in case\n",
    "    con.execute(\"INSERT INTO headlines.Articles SELECT * FROM failed2_df ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Articles_Trading_Day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.Articles_Trading_Day\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO headlines.Articles_Trading_Day\n",
    "SELECT \n",
    "    a.guid,\n",
    "    a.ticker,\n",
    "    coalesce(MIN(tc.trading_date), cast(a.article_pubDate as Date)) AS mapped_trading_date,\n",
    "    a.description,\n",
    "    a.article_link,\n",
    "    a.article_pubDate,\n",
    "    a.article_title,\n",
    "    a.language,\n",
    "    a.lastBuildDate,\n",
    "    a.link,\n",
    "    a.title\n",
    "FROM (\n",
    "    SELECT \n",
    "        guid,\n",
    "        ticker,\n",
    "        description,\n",
    "        article_link,\n",
    "        article_pubDate,\n",
    "        article_title,\n",
    "        language,\n",
    "        lastBuildDate,\n",
    "        link,\n",
    "        title,\n",
    "        -- 4 PM EST adjust\n",
    "        CASE \n",
    "            WHEN CAST(article_pubDate AS TIME) >= '16:00:00' \n",
    "            THEN CAST(article_pubDate AS DATE) + INTERVAL '1 day'\n",
    "            ELSE CAST(article_pubDate AS DATE)\n",
    "        END AS adjusted_pub_date\n",
    "    FROM headlines.Articles\n",
    ") a\n",
    "LEFT JOIN \n",
    "    headlines.Trading_Calendar tc\n",
    "ON \n",
    "    tc.trading_date >= a.adjusted_pub_date\n",
    "GROUP BY \n",
    "    a.guid, a.ticker, a.description, a.article_link, a.article_pubDate, \n",
    "    a.article_title, a.language, a.lastBuildDate, a.link, a.title;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `Market_Data_Headlines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"Truncate Market_Data_Headlines\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO headlines.Market_Data_Headlines\n",
    "SELECT \n",
    "    md.trading_day_date,\n",
    "    md.ticker,\n",
    "    md.price,\n",
    "    md.volume,\n",
    "    COALESCE(COUNT(DISTINCT atd.guid), 0) AS headline_count\n",
    "FROM \n",
    "    headlines.Market_Data_Daily_Processing md\n",
    "LEFT JOIN \n",
    "    headlines.Articles_Trading_Day atd\n",
    "ON \n",
    "    md.ticker = atd.ticker AND md.trading_day_date = atd.mapped_trading_date\n",
    "GROUP BY \n",
    "    md.trading_day_date, md.ticker, md.price, md.volume;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `market_article_summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"drop table headlines.Market_Article_Summary\")\n",
    "# con.execute(\"\"\"\n",
    "#                 CREATE TABLE IF NOT EXISTS headlines.Market_Article_Summary (\n",
    "#     trading_date DATE PRIMARY KEY,\n",
    "#     article_count INT\n",
    "#     );\n",
    "\n",
    "#             \"\"\")\n",
    "con.execute('''\n",
    "INSERT INTO headlines.Market_Article_Summary\n",
    "SELECT \n",
    "    atd.mapped_trading_date AS trading_date,\n",
    "    COUNT(DISTINCT atd.guid) AS total_unique_articles\n",
    "FROM \n",
    "    headlines.Articles_Trading_Day atd\n",
    "GROUP BY \n",
    "    atd.mapped_trading_date;\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Daily_Price_Movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.Daily_Price_Movement\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO headlines.Daily_Price_Movement\n",
    "            SELECT \n",
    "                sp1.trading_day_date AS trading_date,\n",
    "                sp1.ticker,\n",
    "                sp1.price AS close_price,\n",
    "                sp2.trading_day_date AS next_trading_day,\n",
    "                sp2.price AS close_price_next,\n",
    "                ROUND(sp2.price - sp1.price, 2) AS price_change,\n",
    "                ROUND((sp2.price - sp1.price) / sp1.price * 100, 2) AS price_change_percentage\n",
    "            FROM headlines.market_data_daily_processing sp1\n",
    "            LEFT JOIN headlines.market_data_daily_processing sp2 \n",
    "            ON sp2.ticker = sp1.ticker \n",
    "            AND sp2.trading_day_date = (\n",
    "                SELECT MIN(sp3.trading_day_date) \n",
    "                FROM headlines.market_data_daily_processing sp3\n",
    "                WHERE sp3.ticker = sp1.ticker\n",
    "                AND sp3.trading_day_date > sp1.trading_day_date\n",
    "            );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Weekly_Price_Movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.Weekly_Price_Movement\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO headlines.Weekly_Price_Movement\n",
    "            WITH WeeklyPrices AS (\n",
    "            SELECT \n",
    "                ticker,\n",
    "                MIN(tc.trading_date) AS trading_week_start,\n",
    "                MAX(tc.trading_date) AS trading_week_end\n",
    "            FROM headlines.market_data_daily_processing mdp\n",
    "            JOIN headlines.trading_calendar tc \n",
    "            ON mdp.trading_day_date = tc.trading_date\n",
    "            WHERE EXTRACT(DOW FROM tc.trading_date) BETWEEN 1 AND 5  -- Only weekdays\n",
    "            GROUP BY ticker, DATE_TRUNC('week', tc.trading_date)\n",
    "        ),\n",
    "        StartPrices AS (\n",
    "            SELECT \n",
    "                mdp.trading_day_date AS trading_week_start, \n",
    "                mdp.ticker, \n",
    "                mdp.price AS close_price\n",
    "            FROM headlines.market_data_daily_processing mdp\n",
    "            JOIN WeeklyPrices wp \n",
    "            ON mdp.ticker = wp.ticker \n",
    "            AND mdp.trading_day_date = wp.trading_week_start\n",
    "        ),\n",
    "        EndPrices AS (\n",
    "            SELECT \n",
    "                mdp.trading_day_date AS trading_week_end, \n",
    "                mdp.ticker, \n",
    "                mdp.price AS close_price_end\n",
    "            FROM headlines.market_data_daily_processing mdp\n",
    "            JOIN WeeklyPrices wp \n",
    "            ON mdp.ticker = wp.ticker \n",
    "            AND mdp.trading_day_date = wp.trading_week_end\n",
    "        )\n",
    "        SELECT \n",
    "            sp.trading_week_start,\n",
    "            sp.ticker,\n",
    "            sp.close_price as close_price_start,\n",
    "            ep.trading_week_end,\n",
    "            ep.close_price_end,\n",
    "            ROUND(ep.close_price_end - sp.close_price, 2) AS price_change,\n",
    "            ROUND((ep.close_price_end - sp.close_price) / sp.close_price * 100, 2) AS price_change_percentage\n",
    "        FROM StartPrices sp\n",
    "        JOIN EndPrices ep \n",
    "        ON sp.ticker = ep.ticker \n",
    "        AND sp.trading_week_start = ep.trading_week_end - INTERVAL '4 days';\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `extreme_price_movements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.extreme_price_movements\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO headlines.extreme_price_movements\n",
    "            SELECT trading_date, ticker, close_price, price_change, price_change_percentage,\n",
    "                CASE \n",
    "                    WHEN price_change_percentage < -5 THEN 'Drop'\n",
    "                    WHEN price_change_percentage > 5 THEN 'Surge'\n",
    "                END AS movement_type\n",
    "            FROM headlines.daily_price_movement\n",
    "            WHERE ABS(price_change_percentage) > 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `articles_extreme_drops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.articles_extreme_drops\")\n",
    "df = con.execute(\"\"\"\n",
    "            SELECT epm.trading_date, epm.ticker, a.guid, a.mapped_trading_date,\n",
    "                fs.finbert_title_score AS title_sentiment_score,\n",
    "                fs.finbert_title_label AS title_sentiment_label,\n",
    "                fs.finbert_description_score AS descripton_sentiment_score,\n",
    "                fs.finbert_description_label AS descripton_sentiment_label\n",
    "            FROM headlines.extreme_price_movements epm\n",
    "            JOIN headlines.articles_trading_day a\n",
    "            ON epm.ticker = a.ticker\n",
    "            AND a.mapped_trading_date BETWEEN epm.trading_date - INTERVAL '3 days' AND epm.trading_date\n",
    "            LEFT JOIN headlines.finbert_sentiment fs\n",
    "            ON a.guid = fs.guid\n",
    "\"\"\").df() \n",
    "\n",
    "# dedupe based on trading_date, ticker, guid \n",
    "df.drop_duplicates(subset=['trading_date', 'ticker', 'guid'], inplace=True)\n",
    "\n",
    "con.execute(\"INSERT INTO headlines.articles_extreme_drops select * from df ON CONFLICT (trading_date, ticker, guid) DO NOTHING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP500 Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "\n",
    "sp500_volume_weekly_path = SP500_VOLUME_WEEKLY_PATH \n",
    "sp500_price_weekly_path = SP500_PRICE_WEEKLY_PATH \n",
    "sp500_price_daily_path = SP500_PRICE_DAILY_PATH\n",
    "sp500_company_path = SP500_COMPANY_PATH\n",
    "sp500_price_sp500_path = SP500_PRICE_SP500_PATH\n",
    "sp500_item1_path = SP500_ITEM1_PATH \n",
    "sp500_item1a_path =SP500_ITEM1A_PATH\n",
    "sp500_item7_path = SP500_ITEM7_PATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table statements \n",
    "drop_statements = [\n",
    "    \"DROP TABLE IF EXISTS SP500.Volume_Weekly;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Price_Daily;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Company_Info;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Weekly_Market_Data;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Price_Weekly;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Price_Weekly_SP500;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.item7;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.item1a;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.item1;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.SEC_Item_Filings;\"\n",
    "]\n",
    "\n",
    "ddl_statements = [\n",
    "    \"CREATE SCHEMA IF NOT EXISTS SP500;\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Volume_Weekly (\n",
    "    trading_week_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    volume FLOAT,\n",
    "    PRIMARY KEY (trading_week_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Price_Daily (\n",
    "    trading_day_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    price FLOAT,\n",
    "    PRIMARY KEY (trading_day_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Company_Info (\n",
    "    cik TEXT PRIMARY KEY,\n",
    "    ticker TEXT,  \n",
    "    name TEXT, \n",
    "    subindustry TEXT \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Weekly_Market_Data (\n",
    "    trading_week_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    price FLOAT NOT NULL,\n",
    "    volume FLOAT NOT NULL,\n",
    "    PRIMARY KEY (trading_week_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Price_Weekly (\n",
    "    trading_week_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    price FLOAT,\n",
    "    PRIMARY KEY (trading_week_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Price_Weekly_SP500 (\n",
    "    trading_week_date DATE PRIMARY KEY,\n",
    "    SP500CapWeighted FLOAT,\n",
    "    SP500EqualWeighted FLOAT\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.item7 (\n",
    "    company TEXT NOT NULL,           \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    cik TEXT,                         \n",
    "    item7 TEXT,                      \n",
    "    PRIMARY KEY (company, filing_ts)      \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.item1a (\n",
    "    company TEXT NOT NULL,           \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    cik TEXT,                         \n",
    "    item1a TEXT,                     \n",
    "    PRIMARY KEY (company, filing_ts)      \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.item1 (\n",
    "    company TEXT NOT NULL,           \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    cik TEXT,                         \n",
    "    item1 TEXT,                      \n",
    "    PRIMARY KEY (company, filing_ts)      \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.SEC_Item_Filings (\n",
    "    cik TEXT NOT NULL,               \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    item_filing TEXT NOT NULL,       \n",
    "    company TEXT,                    \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    item_description TEXT,           \n",
    "    PRIMARY KEY (cik, filing_ts, item_filing)\n",
    ");\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "for drop in drop_statements:\n",
    "    con.execute(drop)\n",
    "\n",
    "for ddl in ddl_statements:\n",
    "    con.execute(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `volume_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sp500_volume_weekly_path)\n",
    "# convert the wide format to long format\n",
    "# volume_long_df = wide_to_long(df, ['Date'], 'Volume', 'cik')\n",
    "volume_long_df = df.melt(id_vars=['Date'], var_name='cik', value_name='Volume')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "volume_long_df['Date'] = pd.to_datetime(volume_long_df['Date'])\n",
    "volume_long_df['Volume'] = pd.to_numeric(volume_long_df['Volume'], errors='coerce')\n",
    "\n",
    "# default null volume values to 0. CIK 1534701 is all nulls so better to just drop the column but keep it for now\n",
    "# nah jk leaving it as null for now. will coalesce the final table\n",
    "# volume_long_df['Volume'] = volume_long_df['Volume'].fillna(0)\n",
    "\n",
    "con.execute(\"INSERT INTO SP500.Volume_Weekly (trading_week_date, cik, volume) SELECT date, cik, volume FROM volume_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Price_Daily`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sp500_price_daily_path)\n",
    "# convert the wide format to long format\n",
    "# price_long_df = wide_to_long(df, ['Date'], 'Price', 'cik')\n",
    "price_long_df = df.melt(id_vars=['Date'], var_name='cik', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "price_long_df['Date'] = pd.to_datetime(price_long_df['Date'])\n",
    "price_long_df['Price'] = pd.to_numeric(price_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE SP500.Price_Daily\")\n",
    "con.execute(\"INSERT INTO SP500.Price_Daily (trading_day_date, cik, price) SELECT Date, cik, Price FROM price_long_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_long_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Price_Weekly`\n",
    "\n",
    "<b>Looks like theres an issue with this dataset. When the office hours recordings come out I'll watch it and fix it. until then it'll be null </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sp500_price_weekly_path)\n",
    "# convert the wide format to long format\n",
    "# price_long_df = wide_to_long(df, ['Date'], 'Price', 'cik')\n",
    "price_long_df = df.melt(id_vars=['Date'], var_name='cik', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "price_long_df['Date'] = pd.to_datetime(price_long_df['Date'])\n",
    "price_long_df['Price'] = pd.to_numeric(price_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE SP500.Price_Weekly\")\n",
    "# con.execute(\"INSERT INTO SP500.Price_Weekly (trading_week_date, cik, price) SELECT Date, cik, price FROM price_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Company_Info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its all in one line\n",
    "with open(sp500_company_path, 'r') as file:\n",
    "    lines = file.readline().split('\\\\n')\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split('|')\n",
    "        # DONT RUN THIS TWICE BY MISTAKE!\n",
    "        con.execute(\"INSERT INTO SP500.Company_Info VALUES (?,?,?,?)\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Weekly_Market_Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "INSERT INTO SP500.Weekly_Market_Data\n",
    "SELECT \n",
    "    pw.trading_week_date AS trading_week_date,\n",
    "    pw.cik AS cik,\n",
    "    coalesce(pw.price, 0) AS price,\n",
    "    coalesce(vw.volume, 0) AS volume\n",
    "FROM \n",
    "    SP500.Price_Weekly pw\n",
    "LEFT JOIN \n",
    "    SP500.Volume_Weekly vw\n",
    "ON \n",
    "    pw.trading_week_date = vw.trading_week_date AND pw.cik = vw.cik\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `SP500.Price_Weekly_SP500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sp500_price_sp500_path)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['SP500CapWeighted'] = pd.to_numeric(df['SP500CapWeighted'], errors='coerce')\n",
    "df['SP500EqualWeighted'] = pd.to_numeric(df['SP500EqualWeighted'], errors='coerce')\n",
    "\n",
    "con.execute(\"truncate SP500.Price_Weekly_SP500\")\n",
    "con.execute(\"INSERT INTO SP500.Price_Weekly_SP500 (trading_week_date,SP500CapWeighted, SP500EqualWeighted) SELECT Date, SP500CapWeighted, SP500EqualWeighted FROM df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `item7 `\n",
    "\n",
    "Sometimes the last columns comes in multiple lines. Sucks b/c its last column and I can't rely on the pipe, so gotta code for that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols: company|date|link|type|cik|item7 \n",
    "# con.execute(\"TRUNCATE SP500.item7\")\n",
    "def parse_items(file_path, table_name):\n",
    "    # use 2 pointer approach to check if next line is a continuation of the current line\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        prev_line = None  \n",
    "\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            fields = line.split('|')\n",
    "\n",
    "            if prev_line is None:\n",
    "                prev_line = fields\n",
    "                continue\n",
    "\n",
    "            # if current line has required number of elements, insert prev line\n",
    "            if len(fields) == 6:\n",
    "                con.execute(f\"INSERT INTO SP500.{table_name} VALUES (?,?,?,?,?,?)\", prev_line)\n",
    "                prev_line = fields\n",
    "            else:\n",
    "                prev_line[-1] += \" \" + line\n",
    "\n",
    "        # the last record\n",
    "        if prev_line:\n",
    "            con.execute(f\"INSERT INTO SP500.{table_name} VALUES (?,?,?,?,?,?)\", prev_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"TRUNCATE SP500.item7\")\n",
    "parse_items(sp500_item7_path, 'item7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `item1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"TRUNCATE SP500.item1\")\n",
    "parse_items(sp500_item1_path, 'item1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `item1a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"TRUNCATE SP500.item1a\")\n",
    "parse_items(sp500_item1a_path, 'item1a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `SEC_Item_Filings`\n",
    "\n",
    "I think the only thing different between the 3 item tables are the item filing # and the item description\n",
    "\n",
    "With that said, I believe its better to use a longer table for simplicity vs a wider table \n",
    "- wide = instead of 1 item_filing and 1 item_description columns, we make a column for each filing and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "INSERT INTO SP500.SEC_Item_Filings\n",
    "SELECT \n",
    "    cik,\n",
    "    filing_ts,\n",
    "    '7' AS item_filing,\n",
    "    company,\n",
    "    link,\n",
    "    type,\n",
    "    item7 AS item_description\n",
    "FROM \n",
    "    SP500.item7\n",
    "UNION ALL\n",
    "SELECT \n",
    "    cik,\n",
    "    filing_ts,\n",
    "    '1a' AS item_filing,\n",
    "    company,\n",
    "    link,\n",
    "    type,\n",
    "    item1a AS item_description\n",
    "FROM \n",
    "    SP500.item1a\n",
    "UNION ALL\n",
    "SELECT \n",
    "    cik,\n",
    "    filing_ts,\n",
    "    '1' AS item_filing,\n",
    "    company,\n",
    "    link,\n",
    "    type,\n",
    "    item1 AS item_description\n",
    "FROM \n",
    "    SP500.item1;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT related stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "finbert_csv = FINBERT_CSV_PATH\n",
    "# SO THIS ONE WILL HAVE DUPES! NEED TO DEDUPE!\n",
    "# tokens_csv = \"tokens.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.finbert_sentiment (\n",
    "    guid TEXT,\n",
    "    description TEXT,\n",
    "    article_title TEXT,\n",
    "    ticker TEXT,\n",
    "    finbert_title_label TEXT,\n",
    "    finbert_title_score FLOAT,\n",
    "    finbert_description_label TEXT,\n",
    "    finbert_description_score FLOAT,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.tokens_description (\n",
    "    guid TEXT,\n",
    "    token TEXT,\n",
    "    token_lemmatized TEXT,\n",
    "    frequency INT,\n",
    "    PRIMARY KEY (guid, token, token_lemmatized)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.tokens_title (\n",
    "    guid TEXT,\n",
    "    token TEXT,\n",
    "    token_lemmatized TEXT,\n",
    "    frequency INT,\n",
    "    PRIMARY KEY (guid, token, token_lemmatized)\n",
    ");\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS headlines.finbert_sentiment;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.tokens_description;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.tokens_title;\"\n",
    "    ]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `finbert_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(finbert_csv)\n",
    "# df.head()\n",
    "filtered_df = df[['guid', 'description', 'article_title', 'ticker', 'finbert_title_label', 'finbert_title_score', 'finbert_description_label', 'finbert_description_score']]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.finbert_sentiment\")\n",
    "con.execute(\"INSERT INTO headlines.finbert_sentiment select * from filtered_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `tokens_description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SO THIS ONE WILL HAVE DUPES! NEED TO DEDUPE!\n",
    "# df = pd.read_csv(tokens_csv)\n",
    "# print(df.size)\n",
    "\n",
    "# # remove df duplicates based on guid \n",
    "# df.drop_duplicates(subset=['guid'], inplace=True)\n",
    "# print(df.size)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_description_data = []\n",
    "# tokens_title_data = []\n",
    "\n",
    "# for _, row in df.iterrows():\n",
    "#     guid = row['guid']\n",
    "    \n",
    "#     tokens_description = eval(row['tokens_description'])\n",
    "#     tokens_title = eval(row['tokens_title'])\n",
    "    \n",
    "#     for token, lemma  in tokens_description:\n",
    "#         tokens_description_data.append((guid, token, lemma))\n",
    "    \n",
    "#     for token, lemma  in tokens_title:\n",
    "#         tokens_title_data.append((guid, token, lemma))\n",
    "\n",
    "# df_tokens_description = pd.DataFrame(tokens_description_data, columns=[\"guid\", \"token\", \"token_lemmatized\"])\n",
    "# df_tokens_title = pd.DataFrame(tokens_title_data, columns=[\"guid\", \"token\", \"token_lemmatized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_tokens_description.head()\n",
    "# # find guid 367bed80-8d07-3dce-8092-fd53d70578fe with token quarter\n",
    "# # df_tokens_description[(df_tokens_description['guid'] == '367bed80-8d07-3dce-8092-fd53d70578fe' ) & (df_tokens_description['token'] == 'quarter')]\n",
    "# # aggregate any duplicates and count them and add them to column frequency\n",
    "# ### OKAY LETS KEEP token_lemmatized HERE BC LEMMAS RELY ON CONTEXT! \n",
    "# df_tokens_description = df_tokens_description.groupby(['guid', 'token', 'token_lemmatized']).size().reset_index(name='frequency')\n",
    "# # df_tokens_title = df_tokens_title.groupby(['guid', 'token', 'token_lemmatized']).size().reset_index(name='frequency')\n",
    "# df_tokens_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 000705ff-4abf-355e-bed9-aeb6733f92b3 with token spending \n",
    "# df_tokens_description[(df_tokens_description['guid'] == '000705ff-4abf-355e-bed9-aeb6733f92b3' ) & (df_tokens_description['token'] == 'spending')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"truncate headlines.tokens_description\")\n",
    "# con.execute(\"INSERT INTO headlines.tokens_description select * from df_tokens_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `tokens_title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tokens_title = df_tokens_title.groupby(['guid', 'token', 'token_lemmatized']).size().reset_index(name='frequency')\n",
    "# df_tokens_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.execute(\"truncate headlines.tokens_title\")\n",
    "# con.execute(\"INSERT INTO headlines.tokens_title select * from df_tokens_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10k FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "finbert_csv = FINBERT_CSV_10k_PATH\n",
    "# # SO THIS ONE WILL HAVE DUPES! NEED TO DEDUPE!\n",
    "# tokens_csv = \"tokens.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE if not exists sp500.SEC_Item_Filings_FinBERT (\n",
    "    cik TEXT,\n",
    "    filing_ts TIMESTAMP,\n",
    "    item_filing TEXT,\n",
    "    finbert_description_label TEXT,\n",
    "    finbert_description_score FLOAT,\n",
    "    PRIMARY KEY (cik, filing_ts, item_filing),\n",
    "    FOREIGN KEY (cik, filing_ts, item_filing) \n",
    "        REFERENCES sp500.SEC_Item_Filings (cik, filing_ts, item_filing) \n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS sp500.SEC_Item_Filings_FinBERT;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC_Item_Filings_FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_10k_df = pd.read_csv(finbert_csv)\n",
    "finbert_10k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_10k_df = finbert_10k_df[['cik', 'filing_ts', 'item_filing', 'finbert_description_label', 'finbert_description_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate sp500.SEC_Item_Filings_FinBERT\")\n",
    "con.execute(\"INSERT INTO sp500.SEC_Item_Filings_FinBERT select * from finbert_10k_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "vix = \"SP500/vixGaTechSP25.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE sp500.VIX_Index (\n",
    "    vix_date DATE PRIMARY KEY,\n",
    "    vix_value FLOAT\n",
    ");\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS sp500.VIX_Index;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `VIX_Index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_df = pd.read_csv(vix, names=[\"vix_date\", \"vix_value\"], parse_dates=[\"vix_date\"], skiprows=1) # first row is the header but not the best\n",
    "vix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls \n",
    "vix_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"Truncate sp500.VIX_Index\")\n",
    "con.execute(\"INSERT INTO sp500.VIX_Index select * from vix_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "\n",
    "VIX_DAILY_RES_PATH = Path(os.getenv(\"vix_daily_res_path\"))\n",
    "VIX_WEEKLY_RES_PATH = Path(os.getenv(\"vix_weekly_res_path\"))\n",
    "VIX_DAILY_TRAIN_PATH = Path(os.getenv(\"vix_daily_train_path\"))\n",
    "VIX_WEEKLY_TRAIN_PATH = Path(os.getenv(\"vix_weekly_train_path\"))\n",
    "\n",
    "\n",
    "vix_daily_res = VIX_DAILY_RES_PATH\n",
    "vix_weekly_res = VIX_WEEKLY_RES_PATH\n",
    "vix_daily_train = VIX_DAILY_TRAIN_PATH\n",
    "vix_weekly_train = VIX_WEEKLY_TRAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddls = [\n",
    "\"\"\"\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS sp500.rolling_predictions_daily (\n",
    "        date DATE,\n",
    "        actual_vix FLOAT,\n",
    "        predicted_vix FLOAT,\n",
    "        primary key (date)\n",
    "    );\n",
    "\n",
    "\"\"\", \n",
    "\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS sp500.rolling_predictions_weekly (\n",
    "        date DATE,\n",
    "        actual_vix FLOAT,\n",
    "        predicted_vix FLOAT,\n",
    "        primary key (date)\n",
    "    );\n",
    "\"\"\",\n",
    "# make ddl for \"vix_date\",\"vix_value\"\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sp500.vix_daily_training (\n",
    "    vix_date DATE PRIMARY KEY,\n",
    "    vix_value FLOAT\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Create Table if not exists sp500.vix_weekly_training (\n",
    "    vix_date DATE PRIMARY KEY,\n",
    "    vix_value FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "drops = [\n",
    "    \"DROP TABLE IF EXISTS sp500.rolling_predictions_daily;\",\n",
    "    \"DROP TABLE IF EXISTS sp500.rolling_predictions_weekly;\"\n",
    "    \"DROP TABLE IF EXISTS sp500.vix_daily_training;\",\n",
    "    \"DROP TABLE IF EXISTS sp500.vix_weekly_training;\"\n",
    "]\n",
    "\n",
    "for drop in drops:\n",
    "    con.execute(drop)\n",
    "    \n",
    "for ddl in ddls:\n",
    "    con.execute(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `rolling_predictions_daily`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(vix_daily_res)\n",
    "con.execute(\"truncate sp500.rolling_predictions_daily\")\n",
    "con.execute(\"INSERT INTO sp500.rolling_predictions_daily select * from df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `rolling_predictions_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(vix_weekly_res)\n",
    "con.execute(\"truncate sp500.rolling_predictions_weekly\")\n",
    "con.execute(\"INSERT INTO sp500.rolling_predictions_weekly select * from df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sp500.vix_daily_training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x2ce0e7ae070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(vix_daily_train)\n",
    "con.execute(\"truncate sp500.vix_daily_training\")\n",
    "con.execute(\"INSERT INTO sp500.vix_daily_training select * from df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sp500.vix_weekly_training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x2ce0e7ae070>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(vix_weekly_train)\n",
    "con.execute(\"truncate sp500.vix_weekly_training\")\n",
    "con.execute(\"INSERT INTO sp500.vix_weekly_training select * from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT All Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "# THIS USES THE RESULT OF THE FINBERT ALL SCORES FILES\n",
    "FINBERT_ALL_TAGS = Path(os.getenv(\"finbert_all_tags_path\"))\n",
    "finbert_all_tags = \"articles_with_finbert_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.finbert_analysis (\n",
    "    guid UUID ,\n",
    "    ticker VARCHAR(10) NOT NULL,\n",
    "    description TEXT,\n",
    "    article_title TEXT,\n",
    "    finbert_title_label VARCHAR(20) NOT NULL,\n",
    "    finbert_title_score FLOAT NOT NULL,\n",
    "    finbert_title_positive FLOAT NOT NULL,\n",
    "    finbert_title_neutral FLOAT NOT NULL,\n",
    "    finbert_title_negative FLOAT NOT NULL,\n",
    "    finbert_description_label VARCHAR(20) NOT NULL,\n",
    "    finbert_description_score FLOAT NOT NULL,\n",
    "    finbert_description_positive FLOAT NOT NULL,\n",
    "    finbert_description_neutral FLOAT NOT NULL,\n",
    "    finbert_description_negative FLOAT NOT NULL,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    ");\n",
    "\n",
    "\"\"\"]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS headlines.finbert_analysis;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "\n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `headlines.finbert_analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate headlines.finbert_analysis\")\n",
    "df = pd.read_csv(finbert_all_tags)\n",
    "con.execute(\"INSERT INTO headlines.finbert_analysis select guid, ticker, description, article_title, finbert_title_label, finbert_title_score, finbert_title_positive, finbert_title_neutral, finbert_title_negative, finbert_description_label, finbert_description_score, finbert_description_positive, finbert_description_neutral, finbert_description_negative from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data for VIX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "CREATE TABLE Headlines.weekly_training_data(guid VARCHAR,\n",
    "date_t DATE,\n",
    "ticker VARCHAR,\n",
    "subindustry VARCHAR,\n",
    "vix_t FLOAT,\n",
    "vix_t_7_past FLOAT,\n",
    "vix_t_7_future FLOAT,\n",
    "price_t FLOAT,\n",
    "price_t_7_past FLOAT,\n",
    "price_change_t_7 FLOAT,\n",
    "volume_t INTEGER,\n",
    "volume_t_7_past INTEGER,\n",
    "volume_change_t_7 DOUBLE,\n",
    "sentiment_label_t VARCHAR,\n",
    "sentiment_positive_t FLOAT,\n",
    "sentiment_neutral_t FLOAT,\n",
    "sentiment_negative_t FLOAT);\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS Headlines.weekly_training_data;\"\n",
    "]\n",
    "\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Headlines.weekly_training_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate Headlines.weekly_training_data\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO Headlines.weekly_training_data\n",
    "            SELECT * FROM (\n",
    "WITH vix_lagged AS (\n",
    "    SELECT \n",
    "        v1.vix_date AS date_t,\n",
    "        v1.vix_value AS vix_t,\n",
    "        COALESCE(LAG(v1.vix_value, 1) OVER (ORDER BY v1.vix_date), 17.22) AS vix_t_7_past, -- same add last val\n",
    "        COALESCE(LEAD(v1.vix_value, 1) OVER (ORDER BY v1.vix_date), 23.39) AS vix_t_7_future -- adding to handle last vix day that we don't know\n",
    "    FROM sp500.vix_weekly_training v1\n",
    ")\n",
    "--select * from vix_lagged;\n",
    ",\n",
    "-- ANYTHING WITH -1 in price_t, price_t7 or volume cols should be removed! \n",
    "market_lagged AS (\n",
    "    SELECT \n",
    "        m1.trading_day_date AS date_t,\n",
    "        m1.ticker,\n",
    "        COALESCE(m1.price, -1) AS price_t,  -- Set -1 if all price data is NULL\n",
    "        COALESCE(LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date), -1) AS price_t_7_past,\n",
    "        (CASE \n",
    "            WHEN LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date) IS NOT NULL \n",
    "            THEN ((m1.price - LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date)) / \n",
    "                  LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date)) * 100\n",
    "            ELSE NULL \n",
    "        END) AS price_change_t_7,\n",
    "        COALESCE(m1.volume, -1) AS volume_t,  -- Set -1 if all volume data is NULL\n",
    "        COALESCE(LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date), -1) AS volume_t_7_past,\n",
    "        (CASE \n",
    "            WHEN LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date) IS NOT NULL \n",
    "            THEN ((m1.volume - LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date)) / \n",
    "                  NULLIF(LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date), 0)) * 100\n",
    "            ELSE NULL \n",
    "        END) AS volume_change_t_7\n",
    "    FROM headlines.Market_Data_Headlines m1\n",
    "--    where ticker in (\n",
    "--    \tselect * from headlines.sp500_active_stocks\n",
    "--    )\n",
    ")\n",
    "--select * from market_lagged;\n",
    ",\n",
    "article_sentiment AS (\n",
    "    SELECT \n",
    "    \ta.guid,\n",
    "        a.mapped_trading_date AS date_t,\n",
    "        a.ticker,\n",
    "        f.finbert_description_positive AS sentiment_positive_t,\n",
    "        f.finbert_description_neutral AS sentiment_neutral_t,\n",
    "        f.finbert_description_negative AS sentiment_negative_t,\n",
    "        f.finbert_description_label AS sentiment_label_t  -- Just default to NEUTRAL\n",
    "    FROM Headlines.Articles_Trading_Day a\n",
    "    JOIN Headlines.finbert_analysis f ON a.guid = f.guid\n",
    ")\n",
    "--select * from article_sentiment;\n",
    "--select count(*) from (\n",
    "SELECT distinct -- sometimes we have dupes...I think we have dupes upstream but oh whale\n",
    "s.guid,\n",
    "    v.date_t,\n",
    "    m.ticker,\n",
    "    c.subindustry,\n",
    "    v.vix_t,\n",
    "    v.vix_t_7_past,\n",
    "    v.vix_t_7_future,\n",
    "    m.price_t,\n",
    "    m.price_t_7_past,\n",
    "    m.price_change_t_7,\n",
    "    m.volume_t,\n",
    "    m.volume_t_7_past,\n",
    "    m.volume_change_t_7,\n",
    "    coalesce(s.sentiment_label_t, 'NEUTRAL') as sentiment_label_t,\n",
    "    coalesce(s.sentiment_positive_t, 0) as sentiment_positive_t,\n",
    "    coalesce(s.sentiment_neutral_t, 1) as sentiment_neutral_t,\n",
    "    coalesce(s.sentiment_negative_t, 0) as sentiment_negative_t\n",
    "FROM vix_lagged v\n",
    "JOIN market_lagged m ON v.date_t = m.date_t\n",
    "LEFT JOIN article_sentiment s ON v.date_t = s.date_t AND m.ticker = s.ticker\n",
    "LEFT JOIN sp500.company_info c ON m.ticker = c.ticker  -- NEW JOIN\n",
    "order by v.date_t desc) as insert_query;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
