{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Files](#this-is-to-create-the-db)\n",
    "* [Code](#code-to-create-database)\n",
    "    * [Test Schema](#Test-schema)\n",
    "    * [SP500 Schema](#sp500-schema)\n",
    "    * [Finbert](#finbert-related-stuff)\n",
    "    * [Finbert 10ks](#10k-finbert)\n",
    "    * [VIX](#vix)\n",
    "    * [VIX Preds](#vix-preds)\n",
    "    * [Finbert All Tags](#finbert-all-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *THIS IS TO CREATE THE DB*\n",
    "You can download the already made DB file `financial_news.db` from the sharepoint \n",
    "\n",
    "[practicum folder](https://gtvault-my.sharepoint.com/:f:/g/personal/ltupac3_gatech_edu/Eg2gLDzQ8H1JoWUrUIq1G04BPkOXMyxmhgcoL84Q58-5dg?e=80dziH)\n",
    "\n",
    "[db file](https://gtvault-my.sharepoint.com/:u:/g/personal/ltupac3_gatech_edu/Edi6YX6MKPxMud1e5maTIjsBo04ISTst1j7uoxeSVH2OBA?e=XQD3Ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd \n",
    "import os \n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "DB_PATH = Path(os.getenv(\"DB_PATH\"))\n",
    "DB_FILE = os.getenv(\"DB_FILE\")\n",
    "duckdb_path = DB_PATH / DB_FILE\n",
    "\n",
    "VOLUME_NEWS_PATH = Path(os.getenv(\"volume_news_path\"))\n",
    "VOLUME_NEWS_FILE = os.getenv(\"volume_news_file\")\n",
    "VOLUME_FULL_PATH = VOLUME_NEWS_PATH / VOLUME_NEWS_FILE\n",
    "\n",
    "PRICING_NEWS_PATH = Path(os.getenv(\"pricing_news_path\"))\n",
    "PRICING_NEWS_FILE = os.getenv(\"pricing_news_file\")\n",
    "PRICING_FULL_PATH = PRICING_NEWS_PATH / PRICING_NEWS_FILE\n",
    "\n",
    "HEADLINE_JAN25_PATH = Path(os.getenv(\"headline_jan25_path\"))\n",
    "\n",
    "VIX_PATH = Path(os.getenv(\"vix_path\"))\n",
    "VIX_FILE = os.getenv(\"vix_file\")\n",
    "VIX_FULL_PATH = VIX_PATH / VIX_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create database\n",
    "Instructions:\n",
    "1. For individual files (company_info_news.txt, volume_news.csv, etc.), copy the relative path to the respective variable below\n",
    "2. For headline data, put the relative path to the folder housing the ticker folders.\n",
    "    - Example: MultiCap_News/Test houses the individual ticker folders. \n",
    "    - The code will recursively pick up the files from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "\n",
    "# company_txt_path = 'MultiCap_News\\\\company_info_news.txt'\n",
    "# volume_news_path = 'MultiCap_News\\\\volume_news.csv'\n",
    "volume_news_path = VOLUME_FULL_PATH\n",
    "# pricing_news_path = 'MultiCap_News\\\\pricing_news.csv'\n",
    "pricing_news_path = PRICING_FULL_PATH\n",
    "# multicap_Test = 'new_data\\\\mcap_Jan25.csv'\n",
    "headline_jan25_path = HEADLINE_JAN25_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DDLs and indexes\n",
    "ddl_statements = [\n",
    "    \"CREATE SCHEMA IF NOT EXISTS Test;\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Articles (\n",
    "        guid TEXT,\n",
    "        ticker TEXT,\n",
    "        description TEXT,\n",
    "        article_link TEXT,\n",
    "        article_pubDate TIMESTAMP,\n",
    "        article_title TEXT,\n",
    "        language TEXT,\n",
    "        lastBuildDate TIMESTAMP,\n",
    "        link TEXT,\n",
    "        title TEXT,\n",
    "        PRIMARY KEY (guid, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Company_Info_News (\n",
    "        ticker TEXT PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        subindustry TEXT\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Pricing_News (\n",
    "        trading_day_date DATE,\n",
    "        ticker TEXT,\n",
    "        price FLOAT,\n",
    "        PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Volume_News (\n",
    "        trading_day_date DATE,\n",
    "        ticker TEXT,\n",
    "        volume INT,\n",
    "        PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    ############ Gold Layer ############\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Market_Data_Daily_Processing (\n",
    "    trading_day_date DATE,\n",
    "    ticker TEXT,\n",
    "    price FLOAT,\n",
    "    volume INT,\n",
    "    PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Market_Data_Test (\n",
    "    trading_day_date DATE,\n",
    "    ticker TEXT,\n",
    "    price FLOAT,\n",
    "    volume INT,\n",
    "    headline_count INT,\n",
    "    PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Trading_Calendar (\n",
    "    trading_date DATE PRIMARY KEY\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Articles_Trading_Day (\n",
    "    guid TEXT,\n",
    "    ticker TEXT,\n",
    "    mapped_trading_date DATE,\n",
    "    description TEXT,\n",
    "    article_link TEXT,\n",
    "    article_pubDate TIMESTAMP,\n",
    "    article_title TEXT,\n",
    "    language TEXT,\n",
    "    lastBuildDate TIMESTAMP,\n",
    "    link TEXT,\n",
    "    title TEXT,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Market_Article_Summary (\n",
    "    trading_date DATE PRIMARY KEY,\n",
    "    article_count INT\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Daily_Price_Movement (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    close_price FLOAT,\n",
    "    next_trading_day DATE,\n",
    "    close_price_next FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    PRIMARY KEY (trading_date, ticker),\n",
    "    FOREIGN KEY (trading_date) REFERENCES Test.trading_calendar(trading_date),\n",
    "    FOREIGN KEY (next_trading_day) REFERENCES Test.trading_calendar(trading_date)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.Weekly_Price_Movement (\n",
    "    trading_week_start DATE,\n",
    "    ticker TEXT,\n",
    "    close_price_start FLOAT,\n",
    "    trading_week_end DATE,\n",
    "    close_price_end FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    PRIMARY KEY (trading_week_start, ticker),\n",
    "    FOREIGN KEY (trading_week_start) REFERENCES Test.trading_calendar(trading_date),\n",
    "    FOREIGN KEY (trading_week_end) REFERENCES Test.trading_calendar(trading_date)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.extreme_price_movements (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    close_price FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    movement_type TEXT,  -- Drop|Surge\n",
    "    PRIMARY KEY (trading_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Test.articles_extreme_drops (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    guid TEXT,\n",
    "    mapped_trading_date DATE, \n",
    "    title_sentiment_score FLOAT,\n",
    "    title_sentiment_label TEXT,\n",
    "    description_sentiment_score FLOAT,\n",
    "    description_sentiment_label TEXT,\n",
    "    PRIMARY KEY (trading_date, ticker, guid)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop_statements = [\n",
    "    \"DROP TABLE IF EXISTS Test.Articles;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Company_Info_News;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Pricing_News;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Volume_News;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Market_Data_Daily_Processing;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Market_Data_Test;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Trading_Calendar;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Articles_Trading_Day;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Market_Article_Summary;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Daily_Price_Movement;\",\n",
    "    \"DROP TABLE IF EXISTS Test.Weekly_Price_Movement;\",\n",
    "    \"DROP TABLE IF EXISTS Test.extreme_price_movements;\",\n",
    "    \"DROP TABLE IF EXISTS Test.articles_extreme_drops;\"\n",
    "]\n",
    "\n",
    "index_statements = [\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_articles_pubDate ON Test.Articles (article_pubDate);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_articles_pubDate ON Test.Articles_Trading_Day (article_pubDate);\"\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_stock_movement_ticker ON Test.Daily_Price_Movement (ticker);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_stock_movement_ticker ON Test.Weekly_Price_Movement (ticker);\"\n",
    "]\n",
    "\n",
    "for drop in drop_statements:\n",
    "    con.execute(drop)\n",
    "\n",
    "for ddl in ddl_statements:\n",
    "    con.execute(ddl)\n",
    "\n",
    "for index in index_statements:\n",
    "    con.execute(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Company_Info_News`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # its all in one line\n",
    "# with open(company_txt_path, 'r') as file:\n",
    "#     lines = file.readline().split('\\\\n')\n",
    "#     # con.execute(\"TRUNCATE Company_Info_News\")\n",
    "#     for line in lines[1:]:\n",
    "#         line = line.strip().split('|')\n",
    "#         # DONT RUN THIS TWICE BY MISTAKE!\n",
    "#         con.execute(\"INSERT INTO Test.Company_Info_News VALUES (?,?,?)\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Volume_News` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(volume_news_path)\n",
    "# df.head()\n",
    "\n",
    "# convert the wide format to long format\n",
    "volume_long_df = df.melt(id_vars=['Date'], var_name='Ticker', value_name='Volume')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "volume_long_df['Date'] = pd.to_datetime(volume_long_df['Date'])\n",
    "volume_long_df['Volume'] = pd.to_numeric(volume_long_df['Volume'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE Volume_News\")\n",
    "con.execute(\"INSERT INTO Test.Volume_News (trading_day_date, ticker, Volume) SELECT Date, ticker, Volume FROM volume_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Pricing_News`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(pricing_news_path)\n",
    "# convert the wide format to long format\n",
    "pricing_long_df = df.melt(id_vars=['Date'], var_name='Ticker', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "pricing_long_df['Date'] = pd.to_datetime(pricing_long_df['Date'])\n",
    "pricing_long_df['Price'] = pd.to_numeric(pricing_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE Pricing_News\")\n",
    "con.execute(\"INSERT INTO Test.Pricing_News (trading_day_date, Ticker, price) SELECT Date, ticker, Price FROM pricing_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Market_Data_Daily_Processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con.execute(\"TRUNCATE Market_Data_Daily_Processing\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO Test.Market_Data_Daily_Processing\n",
    "SELECT \n",
    "    pn.trading_day_date,\n",
    "    pn.ticker,\n",
    "    pn.price,\n",
    "    vn.volume\n",
    "FROM \n",
    "    Test.Pricing_News pn\n",
    "LEFT JOIN \n",
    "    Test.Volume_News vn \n",
    "ON \n",
    "    pn.trading_day_date = vn.trading_day_date AND pn.ticker = vn.ticker\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Trading_Calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pricing_dates = pricing_long_df['Date'].drop_duplicates()\n",
    "# volume_dates = volume_long_df['Date'].drop_duplicates()\n",
    "\n",
    "# trading_dates = pd.concat([pricing_dates, volume_dates]).drop_duplicates().sort_values()\n",
    "\n",
    "# # make sure to have correct col name\n",
    "# trading_dates_df = pd.DataFrame(trading_dates, columns=['trading_date'])\n",
    "# con.execute(\"TRUNCATE Trading_Calendar\")\n",
    "con.execute(\n",
    "\"\"\"\n",
    "INSERT INTO Test.Trading_Calendar\n",
    "SELECT DISTINCT trading_day_date AS trading_date\n",
    "FROM (\n",
    "    SELECT trading_day_date FROM Test.Pricing_News\n",
    "    UNION\n",
    "    SELECT trading_day_date FROM Test.Volume_News\n",
    ") AS all_dates\n",
    "ORDER BY trading_date;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_loader(base_dir):\n",
    "    # lets do this in chunks instead\n",
    "    failed_parses = pd.DataFrame()\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        data = [] \n",
    "        # extract ticker from foldername \n",
    "        ticker = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root_element = tree.getroot()\n",
    "                \n",
    "                channel = root_element.find('channel')\n",
    "                if channel is not None:\n",
    "                    # extract metadata info\n",
    "                    language = channel.findtext(\"language\") \n",
    "                    lastBuildDate = channel.findtext(\"lastBuildDate\")\n",
    "                    link = channel.findtext(\"link\")\n",
    "                    title = channel.findtext(\"title\")\n",
    "                    \n",
    "                    # now meat and potatoes\n",
    "                    for item in channel.findall(\"item\"):\n",
    "                        description = item.findtext(\"description\")\n",
    "                        guid = item.findtext(\"guid\")\n",
    "                        article_link = item.findtext(\"link\")\n",
    "                        article_pubDate = item.findtext(\"pubDate\")\n",
    "                        article_title = item.findtext(\"title\")\n",
    "                        \n",
    "                        data.append({\n",
    "                            \"guid\": guid,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"description\": description,\n",
    "                            \"article_link\": article_link,\n",
    "                            \"article_pubDate\": article_pubDate,\n",
    "                            \"article_title\": article_title,\n",
    "                            \"language\": language,\n",
    "                            \"lastBuildDate\": lastBuildDate,\n",
    "                            \"link\": link,\n",
    "                            \"title\": title\n",
    "                        })\n",
    "            except ET.parseError as e:\n",
    "                print(f\"Error parsing file {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "        \n",
    "        # insert the data into the database\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            # print(\"Performing timestamp coercion for\", ticker)\n",
    "            df['parsed_date'] = pd.to_datetime(df['article_pubDate'], errors='coerce')\n",
    "            df['lastBuildDate'] = pd.to_datetime(df['lastBuildDate'], errors='coerce')\n",
    "            # print(\"Done timestamp coercion for\", ticker)\n",
    "            \n",
    "            # separate failed cases to avoid nulls\n",
    "            current_failed = df[df['parsed_date'].isna()]\n",
    "            current_valid = df[df['parsed_date'].notna()]\n",
    "\n",
    "            # these are good\n",
    "            current_valid = current_valid.assign(article_pubDate=current_valid['parsed_date']).drop(columns=['parsed_date'])\n",
    "\n",
    "            # remove dupes on guid and ticker\n",
    "            current_valid.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "\n",
    "            \n",
    "            failed_parses = pd.concat([failed_parses, current_failed], ignore_index=True)\n",
    "            \n",
    "            try:\n",
    "                # adding this too just in case\n",
    "                con.execute(\"INSERT INTO Test.Articles SELECT * FROM current_valid ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "                print(\"inserted data for\", ticker)\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting data for {ticker}: {e}\")\n",
    "                \n",
    "    failed_parses.to_csv(\"failed_article_dates.csv\", index=False)\n",
    "    return failed_parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted data for A\n",
      "inserted data for AA\n",
      "inserted data for AAL\n",
      "inserted data for AAON\n",
      "inserted data for AAPL\n",
      "inserted data for ABBV\n",
      "inserted data for ABCB\n",
      "inserted data for ABG\n",
      "inserted data for ABNB\n",
      "inserted data for ABT\n",
      "inserted data for ACA\n",
      "inserted data for ACGL\n",
      "inserted data for ACHC\n",
      "inserted data for ACI\n",
      "inserted data for ACIW\n",
      "inserted data for ACM\n",
      "inserted data for ACN\n",
      "inserted data for ACT\n",
      "inserted data for ADBE\n",
      "inserted data for ADC\n",
      "inserted data for ADI\n",
      "inserted data for ADM\n",
      "inserted data for ADMA\n",
      "inserted data for ADP\n",
      "inserted data for ADSK\n",
      "inserted data for ADT\n",
      "inserted data for AEE\n",
      "inserted data for AEIS\n",
      "inserted data for AEO\n",
      "inserted data for AEP\n",
      "inserted data for AER\n",
      "inserted data for AES\n",
      "inserted data for AFG\n",
      "inserted data for AFL\n",
      "inserted data for AFRM\n",
      "inserted data for AGCO\n",
      "inserted data for AGNC\n",
      "inserted data for AGO\n",
      "inserted data for AIG\n",
      "inserted data for AIT\n",
      "inserted data for AIZ\n",
      "inserted data for AJG\n",
      "inserted data for AKAM\n",
      "inserted data for AL\n",
      "inserted data for ALAB\n",
      "inserted data for ALB\n",
      "inserted data for ALGM\n",
      "inserted data for ALGN\n",
      "inserted data for ALIT\n",
      "inserted data for ALK\n",
      "inserted data for ALKS\n",
      "inserted data for ALL\n",
      "inserted data for ALLE\n",
      "inserted data for ALLY\n",
      "inserted data for ALNY\n",
      "inserted data for ALSN\n",
      "inserted data for ALTR\n",
      "inserted data for ALV\n",
      "inserted data for AM\n",
      "inserted data for AMAT\n",
      "inserted data for AMCR\n",
      "inserted data for AMD\n",
      "inserted data for AME\n",
      "inserted data for AMG\n",
      "inserted data for AMGN\n",
      "inserted data for AMH\n",
      "inserted data for AMKR\n",
      "inserted data for AMLP\n",
      "inserted data for AMP\n",
      "inserted data for AMT\n",
      "inserted data for AMZN\n",
      "inserted data for AN\n",
      "inserted data for ANET\n",
      "inserted data for ANF\n",
      "inserted data for ANSS\n",
      "inserted data for AON\n",
      "inserted data for AOS\n",
      "inserted data for APA\n",
      "inserted data for APD\n",
      "inserted data for APG\n",
      "inserted data for APH\n",
      "inserted data for APLS\n",
      "inserted data for APO\n",
      "inserted data for APP\n",
      "inserted data for APPF\n",
      "inserted data for APTV\n",
      "inserted data for AR\n",
      "inserted data for ARCC\n",
      "inserted data for ARE\n",
      "inserted data for ARMK\n",
      "inserted data for ARW\n",
      "inserted data for AS\n",
      "inserted data for ASGN\n",
      "inserted data for ASH\n",
      "inserted data for ASML\n",
      "inserted data for ASO\n",
      "inserted data for ASTS\n",
      "inserted data for ATI\n",
      "inserted data for ATO\n",
      "inserted data for ATR\n",
      "inserted data for AUR\n",
      "inserted data for AVAV\n",
      "inserted data for AVB\n",
      "inserted data for AVGO\n",
      "inserted data for AVNT\n",
      "inserted data for AVT\n",
      "inserted data for AVTR\n",
      "inserted data for AVY\n",
      "inserted data for AWI\n",
      "inserted data for AWK\n",
      "inserted data for AX\n",
      "inserted data for AXON\n",
      "inserted data for AXP\n",
      "inserted data for AXS\n",
      "inserted data for AXSM\n",
      "inserted data for AXTA\n",
      "inserted data for AYI\n",
      "inserted data for AZEK\n",
      "inserted data for AZO\n",
      "inserted data for AZPN\n",
      "inserted data for BA\n",
      "inserted data for BAC\n",
      "inserted data for BAH\n",
      "inserted data for BALL\n",
      "inserted data for BAP\n",
      "inserted data for BAX\n",
      "inserted data for BBIO\n",
      "inserted data for BBWI\n",
      "inserted data for BBY\n",
      "inserted data for BC\n",
      "inserted data for BCC\n",
      "inserted data for BCIM\n",
      "inserted data for BCO\n",
      "inserted data for BCPC\n",
      "inserted data for BDC\n",
      "inserted data for BDX\n",
      "inserted data for BECN\n",
      "inserted data for BEN\n",
      "inserted data for BERY\n",
      "inserted data for BFAM\n",
      "inserted data for BG\n",
      "inserted data for BGC\n",
      "inserted data for BIIB\n",
      "inserted data for BIL\n",
      "inserted data for BILL\n",
      "inserted data for BIO\n",
      "inserted data for BIPC\n",
      "inserted data for BIRK\n",
      "inserted data for BITO\n",
      "inserted data for BJ\n",
      "inserted data for BK\n",
      "inserted data for BKH\n",
      "inserted data for BKNG\n",
      "inserted data for BKR\n",
      "inserted data for BLD\n",
      "inserted data for BLDR\n",
      "inserted data for BLK\n",
      "inserted data for BLKB\n",
      "inserted data for BMI\n",
      "inserted data for BMRN\n",
      "inserted data for BMY\n",
      "inserted data for BOKF\n",
      "inserted data for BOOT\n",
      "inserted data for BOX\n",
      "inserted data for BPMC\n",
      "inserted data for BPOP\n",
      "inserted data for BR\n",
      "inserted data for BRBR\n",
      "inserted data for BRKR\n",
      "inserted data for BRO\n",
      "inserted data for BRX\n",
      "inserted data for BRZE\n",
      "inserted data for BSX\n",
      "inserted data for BSY\n",
      "inserted data for BURL\n",
      "inserted data for BWA\n",
      "inserted data for BWXT\n",
      "inserted data for BX\n",
      "inserted data for BXP\n",
      "inserted data for BYD\n",
      "inserted data for C\n",
      "inserted data for CACC\n",
      "inserted data for CACI\n",
      "inserted data for CADE\n",
      "inserted data for CAG\n",
      "inserted data for CAH\n",
      "inserted data for CAMT\n",
      "inserted data for CARR\n",
      "inserted data for CART\n",
      "inserted data for CASY\n",
      "inserted data for CAT\n",
      "inserted data for CAVA\n",
      "inserted data for CB\n",
      "inserted data for CBRE\n",
      "inserted data for CBSH\n",
      "inserted data for CBT\n",
      "inserted data for CCCS\n",
      "inserted data for CCEP\n",
      "inserted data for CCI\n",
      "inserted data for CCK\n",
      "inserted data for CCL\n",
      "inserted data for CDNS\n",
      "inserted data for CDW\n",
      "inserted data for CE\n",
      "inserted data for CEG\n",
      "inserted data for CELH\n",
      "inserted data for CF\n",
      "inserted data for CFG\n",
      "inserted data for CFLT\n",
      "inserted data for CFR\n",
      "inserted data for CG\n",
      "inserted data for CGNX\n",
      "inserted data for CHD\n",
      "inserted data for CHDN\n",
      "inserted data for CHE\n",
      "inserted data for CHH\n",
      "inserted data for CHKP\n",
      "inserted data for CHRD\n",
      "inserted data for CHRW\n",
      "inserted data for CHTR\n",
      "inserted data for CHWY\n",
      "inserted data for CHX\n",
      "inserted data for CI\n",
      "inserted data for CIEN\n",
      "inserted data for CINF\n",
      "inserted data for CIVI\n",
      "inserted data for CL\n",
      "inserted data for CLF\n",
      "inserted data for CLH\n",
      "inserted data for CLVT\n",
      "inserted data for CLX\n",
      "inserted data for CMA\n",
      "inserted data for CMC\n",
      "inserted data for CMCSA\n",
      "inserted data for CME\n",
      "inserted data for CMG\n",
      "inserted data for CMI\n",
      "inserted data for CMS\n",
      "inserted data for CNA\n",
      "inserted data for CNC\n",
      "inserted data for CNH\n",
      "inserted data for CNM\n",
      "inserted data for CNP\n",
      "inserted data for CNS\n",
      "inserted data for CNX\n",
      "inserted data for CNXC\n",
      "inserted data for COF\n",
      "inserted data for COHR\n",
      "inserted data for COIN\n",
      "inserted data for COKE\n",
      "inserted data for COLB\n",
      "inserted data for COLD\n",
      "inserted data for COLM\n",
      "inserted data for COO\n",
      "inserted data for COOP\n",
      "inserted data for COP\n",
      "inserted data for COR\n",
      "inserted data for COST\n",
      "inserted data for COTY\n",
      "inserted data for CPAY\n",
      "inserted data for CPB\n",
      "inserted data for CPNG\n",
      "inserted data for CPRI\n",
      "inserted data for CPRT\n",
      "inserted data for CPT\n",
      "inserted data for CR\n",
      "inserted data for CRBG\n",
      "inserted data for CRC\n",
      "inserted data for CRDO\n",
      "inserted data for CRH\n",
      "inserted data for CRL\n",
      "inserted data for CRM\n",
      "inserted data for CRNX\n",
      "inserted data for CROX\n",
      "inserted data for CRS\n",
      "inserted data for CRSP\n",
      "inserted data for CRUS\n",
      "inserted data for CRVL\n",
      "inserted data for CRWD\n",
      "inserted data for CSCO\n",
      "inserted data for CSGP\n",
      "inserted data for CSL\n",
      "inserted data for CSWI\n",
      "inserted data for CSX\n",
      "inserted data for CTAS\n",
      "inserted data for CTRA\n",
      "inserted data for CTRE\n",
      "inserted data for CTSH\n",
      "inserted data for CTVA\n",
      "inserted data for CUBE\n",
      "inserted data for CUZ\n",
      "inserted data for CVLT\n",
      "inserted data for CVNA\n",
      "inserted data for CVS\n",
      "inserted data for CVX\n",
      "inserted data for CW\n",
      "inserted data for CWAN\n",
      "inserted data for CWST\n",
      "inserted data for CXT\n",
      "inserted data for CYBR\n",
      "inserted data for CYTK\n",
      "inserted data for CZR\n",
      "inserted data for D\n",
      "inserted data for DAL\n",
      "inserted data for DAR\n",
      "inserted data for DASH\n",
      "inserted data for DAY\n",
      "inserted data for DBMF\n",
      "inserted data for DBX\n",
      "inserted data for DCI\n",
      "inserted data for DD\n",
      "inserted data for DDOG\n",
      "inserted data for DDS\n",
      "inserted data for DE\n",
      "inserted data for DECK\n",
      "inserted data for DELL\n",
      "inserted data for DFS\n",
      "inserted data for DG\n",
      "inserted data for DGX\n",
      "inserted data for DHI\n",
      "inserted data for DHR\n",
      "inserted data for DINO\n",
      "inserted data for DIS\n",
      "inserted data for DJT\n",
      "inserted data for DKNG\n",
      "inserted data for DKS\n",
      "inserted data for DLB\n",
      "inserted data for DLO\n",
      "inserted data for DLR\n",
      "inserted data for DLTR\n",
      "inserted data for DNB\n",
      "inserted data for DOC\n",
      "inserted data for DOCS\n",
      "inserted data for DOCU\n",
      "inserted data for DOV\n",
      "inserted data for DOW\n",
      "inserted data for DOX\n",
      "inserted data for DPZ\n",
      "inserted data for DRI\n",
      "inserted data for DRS\n",
      "inserted data for DT\n",
      "inserted data for DTCR\n",
      "inserted data for DTE\n",
      "inserted data for DTM\n",
      "inserted data for DUK\n",
      "inserted data for DUOL\n",
      "inserted data for DVA\n",
      "inserted data for DVN\n",
      "inserted data for DXCM\n",
      "inserted data for DY\n",
      "inserted data for DYN\n",
      "inserted data for EA\n",
      "inserted data for EBAY\n",
      "inserted data for ECL\n",
      "inserted data for ED\n",
      "inserted data for EEFT\n",
      "inserted data for EFX\n",
      "inserted data for EG\n",
      "inserted data for EGP\n",
      "inserted data for EHC\n",
      "inserted data for EIX\n",
      "inserted data for EL\n",
      "inserted data for ELAN\n",
      "inserted data for ELF\n",
      "inserted data for ELS\n",
      "inserted data for ELV\n",
      "inserted data for EME\n",
      "inserted data for EMN\n",
      "inserted data for EMR\n",
      "inserted data for ENLC\n",
      "inserted data for ENPH\n",
      "inserted data for ENS\n",
      "inserted data for ENSG\n",
      "inserted data for ENTG\n",
      "inserted data for EOG\n",
      "inserted data for EPAM\n",
      "inserted data for EPRT\n",
      "inserted data for EQH\n",
      "inserted data for EQIX\n",
      "inserted data for EQR\n",
      "inserted data for EQT\n",
      "inserted data for ERIE\n",
      "inserted data for ES\n",
      "inserted data for ESAB\n",
      "inserted data for ESGR\n",
      "inserted data for ESI\n",
      "inserted data for ESLT\n",
      "inserted data for ESNT\n",
      "inserted data for ESS\n",
      "inserted data for ESTC\n",
      "inserted data for ETN\n",
      "inserted data for ETR\n",
      "inserted data for ETSY\n",
      "inserted data for EVR\n",
      "inserted data for EVRG\n",
      "inserted data for EW\n",
      "inserted data for EWBC\n",
      "inserted data for EXAS\n",
      "inserted data for EXC\n",
      "inserted data for EXE\n",
      "inserted data for EXEL\n",
      "inserted data for EXLS\n",
      "inserted data for EXP\n",
      "inserted data for EXPD\n",
      "inserted data for EXPE\n",
      "inserted data for EXPO\n",
      "inserted data for EXR\n",
      "inserted data for F\n",
      "inserted data for FAF\n",
      "inserted data for FANG\n",
      "inserted data for FAST\n",
      "inserted data for FBIN\n",
      "inserted data for FCFS\n",
      "inserted data for FCN\n",
      "inserted data for FCNCA\n",
      "inserted data for FCX\n",
      "inserted data for FDS\n",
      "inserted data for FDX\n",
      "inserted data for FE\n",
      "inserted data for FELE\n",
      "inserted data for FERG\n",
      "inserted data for FFIN\n",
      "inserted data for FFIV\n",
      "inserted data for FHN\n",
      "inserted data for FI\n",
      "inserted data for FICO\n",
      "inserted data for FIS\n",
      "inserted data for FITB\n",
      "inserted data for FIVE\n",
      "inserted data for FIX\n",
      "inserted data for FIZZ\n",
      "inserted data for FLEX\n",
      "inserted data for FLG\n",
      "inserted data for FLO\n",
      "inserted data for FLR\n",
      "inserted data for FLS\n",
      "inserted data for FMC\n",
      "inserted data for FN\n",
      "inserted data for FNB\n",
      "inserted data for FND\n",
      "inserted data for FNF\n",
      "inserted data for FORM\n",
      "inserted data for FOUR\n",
      "inserted data for FOX\n",
      "inserted data for FOXA\n",
      "inserted data for FR\n",
      "inserted data for FRHC\n",
      "inserted data for FRO\n",
      "inserted data for FRPT\n",
      "inserted data for FRT\n",
      "inserted data for FSK\n",
      "inserted data for FSLR\n",
      "inserted data for FSS\n",
      "inserted data for FTAI\n",
      "inserted data for FTI\n",
      "inserted data for FTLS\n",
      "inserted data for FTNT\n",
      "inserted data for FTV\n",
      "inserted data for FUL\n",
      "inserted data for FUN\n",
      "inserted data for FWONA\n",
      "inserted data for FWONK\n",
      "inserted data for FXF\n",
      "inserted data for FXY\n",
      "inserted data for FYBR\n",
      "inserted data for G\n",
      "inserted data for GAP\n",
      "inserted data for GATX\n",
      "inserted data for GBCI\n",
      "inserted data for GBDC\n",
      "inserted data for GD\n",
      "inserted data for GDDY\n",
      "inserted data for GE\n",
      "inserted data for GEN\n",
      "inserted data for GEV\n",
      "inserted data for GFS\n",
      "inserted data for GGG\n",
      "inserted data for GILD\n",
      "inserted data for GIS\n",
      "inserted data for GKOS\n",
      "inserted data for GL\n",
      "inserted data for GLBE\n",
      "inserted data for GLD\n",
      "inserted data for GLOB\n",
      "inserted data for GLPI\n",
      "inserted data for GLW\n",
      "inserted data for GM\n",
      "inserted data for GME\n",
      "inserted data for GMED\n",
      "inserted data for GNRC\n",
      "inserted data for GNTX\n",
      "inserted data for GOLF\n",
      "inserted data for GOOG\n",
      "inserted data for GOOGL\n",
      "inserted data for GPC\n",
      "inserted data for GPI\n",
      "inserted data for GPK\n",
      "inserted data for GPN\n",
      "inserted data for GRAB\n",
      "inserted data for GRMN\n",
      "inserted data for GS\n",
      "inserted data for GTES\n",
      "inserted data for GTLB\n",
      "inserted data for GTLS\n",
      "inserted data for GWRE\n",
      "inserted data for GWW\n",
      "inserted data for GXO\n",
      "inserted data for H\n",
      "inserted data for HAE\n",
      "inserted data for HAL\n",
      "inserted data for HALO\n",
      "inserted data for HAS\n",
      "inserted data for HASI\n",
      "inserted data for HBAN\n",
      "inserted data for HCA\n",
      "inserted data for HD\n",
      "inserted data for HEI\n",
      "inserted data for HES\n",
      "inserted data for HGV\n",
      "inserted data for HHH\n",
      "inserted data for HIG\n",
      "inserted data for HII\n",
      "inserted data for HLI\n",
      "inserted data for HLNE\n",
      "inserted data for HLT\n",
      "inserted data for HOG\n",
      "inserted data for HOLX\n",
      "inserted data for HOMB\n",
      "inserted data for HON\n",
      "inserted data for HOOD\n",
      "inserted data for HPE\n",
      "inserted data for HPQ\n",
      "inserted data for HQY\n",
      "inserted data for HR\n",
      "inserted data for HRB\n",
      "inserted data for HRI\n",
      "inserted data for HRL\n",
      "inserted data for HSIC\n",
      "inserted data for HST\n",
      "inserted data for HSY\n",
      "inserted data for HUBB\n",
      "inserted data for HUBS\n",
      "inserted data for HUM\n",
      "inserted data for HUN\n",
      "inserted data for HWC\n",
      "inserted data for HWM\n",
      "inserted data for HXL\n",
      "inserted data for IAC\n",
      "inserted data for IBKR\n",
      "inserted data for IBM\n",
      "inserted data for IBOC\n",
      "inserted data for IBP\n",
      "inserted data for ICE\n",
      "inserted data for ICLN\n",
      "inserted data for ICLR\n",
      "inserted data for ICUI\n",
      "inserted data for IDA\n",
      "inserted data for IDXX\n",
      "inserted data for IEX\n",
      "inserted data for IFF\n",
      "inserted data for IGT\n",
      "inserted data for ILMN\n",
      "inserted data for IMVT\n",
      "inserted data for INCY\n",
      "inserted data for INFA\n",
      "inserted data for INGR\n",
      "inserted data for INSM\n",
      "inserted data for INSP\n",
      "inserted data for INTC\n",
      "inserted data for INTU\n",
      "inserted data for INVH\n",
      "inserted data for IONS\n",
      "inserted data for IOT\n",
      "inserted data for IP\n",
      "inserted data for IPAR\n",
      "inserted data for IPG\n",
      "inserted data for IQV\n",
      "inserted data for IR\n",
      "inserted data for IRM\n",
      "inserted data for IRT\n",
      "inserted data for ISRG\n",
      "inserted data for IT\n",
      "inserted data for ITCI\n",
      "inserted data for ITGR\n",
      "inserted data for ITRI\n",
      "inserted data for ITT\n",
      "inserted data for ITW\n",
      "inserted data for IVZ\n",
      "inserted data for J\n",
      "inserted data for JAZZ\n",
      "inserted data for JBHT\n",
      "inserted data for JBL\n",
      "inserted data for JCI\n",
      "inserted data for JEF\n",
      "inserted data for JHG\n",
      "inserted data for JKHY\n",
      "inserted data for JLL\n",
      "inserted data for JNJ\n",
      "inserted data for JNPR\n",
      "inserted data for JPM\n",
      "inserted data for JXN\n",
      "inserted data for K\n",
      "inserted data for KAI\n",
      "inserted data for KBH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucho\\AppData\\Local\\Temp\\ipykernel_1788\\3759739947.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['parsed_date'] = pd.to_datetime(df['article_pubDate'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted data for KBR\n",
      "inserted data for KD\n",
      "inserted data for KDP\n",
      "inserted data for KEX\n",
      "inserted data for KEY\n",
      "inserted data for KEYS\n",
      "inserted data for KFY\n",
      "inserted data for KGC\n",
      "inserted data for KHC\n",
      "inserted data for KIM\n",
      "inserted data for KKR\n",
      "inserted data for KLAC\n",
      "inserted data for KMB\n",
      "inserted data for KMI\n",
      "inserted data for KMPR\n",
      "inserted data for KMX\n",
      "inserted data for KNF\n",
      "inserted data for KNSL\n",
      "inserted data for KNX\n",
      "inserted data for KO\n",
      "inserted data for KR\n",
      "inserted data for KRC\n",
      "inserted data for KRG\n",
      "inserted data for KRYS\n",
      "inserted data for KTB\n",
      "inserted data for KVUE\n",
      "inserted data for KVYO\n",
      "inserted data for L\n",
      "inserted data for LAD\n",
      "inserted data for LAMR\n",
      "inserted data for LANC\n",
      "inserted data for LAND\n",
      "inserted data for LAZ\n",
      "inserted data for LBRDA\n",
      "inserted data for LBRDK\n",
      "inserted data for LBTYA\n",
      "inserted data for LBTYK\n",
      "inserted data for LCID\n",
      "inserted data for LDOS\n",
      "inserted data for LEA\n",
      "inserted data for LECO\n",
      "inserted data for LEN\n",
      "inserted data for LEVI\n",
      "inserted data for LFUS\n",
      "inserted data for LH\n",
      "inserted data for LHX\n",
      "inserted data for LII\n",
      "inserted data for LIN\n",
      "inserted data for LITE\n",
      "inserted data for LKQ\n",
      "inserted data for LLY\n",
      "inserted data for LMT\n",
      "inserted data for LNC\n",
      "inserted data for LNG\n",
      "inserted data for LNT\n",
      "inserted data for LNTH\n",
      "inserted data for LNW\n",
      "inserted data for LOAR\n",
      "inserted data for LOGI\n",
      "inserted data for LOPE\n",
      "inserted data for LOW\n",
      "inserted data for LPLA\n",
      "inserted data for LPX\n",
      "inserted data for LRCX\n",
      "inserted data for LSCC\n",
      "inserted data for LSTR\n",
      "inserted data for LTH\n",
      "inserted data for LULU\n",
      "inserted data for LUMN\n",
      "inserted data for LUV\n",
      "inserted data for LVS\n",
      "inserted data for LW\n",
      "inserted data for LYB\n",
      "inserted data for LYFT\n",
      "inserted data for LYV\n",
      "inserted data for M\n",
      "inserted data for MA\n",
      "inserted data for MAA\n",
      "inserted data for MAIN\n",
      "inserted data for MANH\n",
      "inserted data for MAR\n",
      "inserted data for MARA\n",
      "inserted data for MAS\n",
      "inserted data for MASI\n",
      "inserted data for MAT\n",
      "inserted data for MATX\n",
      "inserted data for MBLY\n",
      "inserted data for MC\n",
      "inserted data for MCD\n",
      "inserted data for MCHP\n",
      "inserted data for MCK\n",
      "inserted data for MCO\n",
      "inserted data for MDB\n",
      "inserted data for MDGL\n",
      "inserted data for MDLZ\n",
      "inserted data for MDT\n",
      "inserted data for MDU\n",
      "inserted data for MEDP\n",
      "inserted data for MELI\n",
      "inserted data for MET\n",
      "inserted data for META\n",
      "inserted data for MGM\n",
      "inserted data for MGY\n",
      "inserted data for MHK\n",
      "inserted data for MHO\n",
      "inserted data for MIDD\n",
      "inserted data for MKC\n",
      "inserted data for MKL\n",
      "inserted data for MKSI\n",
      "inserted data for MKTX\n",
      "inserted data for MLI\n",
      "inserted data for MLM\n",
      "inserted data for MMC\n",
      "inserted data for MMM\n",
      "inserted data for MMS\n",
      "inserted data for MMSI\n",
      "inserted data for MMYT\n",
      "inserted data for MNA\n",
      "inserted data for MNDY\n",
      "inserted data for MNST\n",
      "inserted data for MO\n",
      "inserted data for MOD\n",
      "inserted data for MOH\n",
      "inserted data for MORN\n",
      "inserted data for MOS\n",
      "inserted data for MPC\n",
      "inserted data for MPWR\n",
      "inserted data for MRK\n",
      "inserted data for MRNA\n",
      "inserted data for MRVL\n",
      "inserted data for MS\n",
      "inserted data for MSA\n",
      "inserted data for MSCI\n",
      "inserted data for MSFT\n",
      "inserted data for MSGS\n",
      "inserted data for MSI\n",
      "inserted data for MSM\n",
      "inserted data for MSTR\n",
      "inserted data for MTB\n",
      "inserted data for MTCH\n",
      "inserted data for MTD\n",
      "inserted data for MTDR\n",
      "inserted data for MTG\n",
      "inserted data for MTH\n",
      "inserted data for MTN\n",
      "inserted data for MTSI\n",
      "inserted data for MTZ\n",
      "inserted data for MU\n",
      "inserted data for MUR\n",
      "inserted data for MUSA\n",
      "inserted data for NBIX\n",
      "inserted data for NCLH\n",
      "inserted data for NDAQ\n",
      "inserted data for NDSN\n",
      "inserted data for NE\n",
      "inserted data for NEE\n",
      "inserted data for NEM\n",
      "inserted data for NET\n",
      "inserted data for NEU\n",
      "inserted data for NFG\n",
      "inserted data for NFLX\n",
      "inserted data for NI\n",
      "inserted data for NJR\n",
      "inserted data for NKE\n",
      "inserted data for NLY\n",
      "inserted data for NNI\n",
      "inserted data for NNN\n",
      "inserted data for NOC\n",
      "inserted data for NOG\n",
      "inserted data for NOV\n",
      "inserted data for NOVT\n",
      "inserted data for NOW\n",
      "inserted data for NRG\n",
      "inserted data for NSC\n",
      "inserted data for NSIT\n",
      "inserted data for NTAP\n",
      "inserted data for NTNX\n",
      "inserted data for NTRA\n",
      "inserted data for NTRS\n",
      "inserted data for NU\n",
      "inserted data for NUE\n",
      "inserted data for NUVL\n",
      "inserted data for NVDA\n",
      "inserted data for NVMI\n",
      "inserted data for NVR\n",
      "inserted data for NVT\n",
      "inserted data for NWS\n",
      "inserted data for NWSA\n",
      "inserted data for NXPI\n",
      "inserted data for NXST\n",
      "inserted data for NYT\n",
      "inserted data for O\n",
      "inserted data for OBDC\n",
      "inserted data for OC\n",
      "inserted data for ODFL\n",
      "inserted data for OGE\n",
      "inserted data for OGN\n",
      "inserted data for OGS\n",
      "inserted data for OHI\n",
      "inserted data for OILK\n",
      "inserted data for OKE\n",
      "inserted data for OKTA\n",
      "inserted data for OLED\n",
      "inserted data for OLLI\n",
      "inserted data for OLN\n",
      "inserted data for OMC\n",
      "inserted data for OMF\n",
      "inserted data for ON\n",
      "inserted data for ONB\n",
      "inserted data for ONON\n",
      "inserted data for ONTO\n",
      "inserted data for OPCH\n",
      "inserted data for ORA\n",
      "inserted data for ORCL\n",
      "inserted data for ORI\n",
      "inserted data for ORLY\n",
      "inserted data for OSCR\n",
      "inserted data for OSK\n",
      "inserted data for OTEX\n",
      "inserted data for OTIS\n",
      "inserted data for OVV\n",
      "inserted data for OWL\n",
      "inserted data for OXY\n",
      "inserted data for OZK\n",
      "inserted data for PAG\n",
      "inserted data for PAGP\n",
      "inserted data for PANW\n",
      "inserted data for PARA\n",
      "inserted data for PATH\n",
      "inserted data for PAVE\n",
      "inserted data for PAYC\n",
      "inserted data for PAYX\n",
      "inserted data for PB\n",
      "inserted data for PBDC\n",
      "inserted data for PBF\n",
      "inserted data for PCAR\n",
      "inserted data for PCG\n",
      "inserted data for PCOR\n",
      "inserted data for PCTY\n",
      "inserted data for PCVX\n",
      "inserted data for PDBC\n",
      "inserted data for PECO\n",
      "inserted data for PEG\n",
      "inserted data for PEGA\n",
      "inserted data for PEN\n",
      "inserted data for PEP\n",
      "inserted data for PFE\n",
      "inserted data for PFG\n",
      "inserted data for PFGC\n",
      "inserted data for PFIX\n",
      "inserted data for PFSI\n",
      "inserted data for PG\n",
      "inserted data for PGR\n",
      "inserted data for PH\n",
      "inserted data for PHM\n",
      "inserted data for PI\n",
      "inserted data for PII\n",
      "inserted data for PINS\n",
      "inserted data for PIPR\n",
      "inserted data for PKG\n",
      "inserted data for PLD\n",
      "inserted data for PLNT\n",
      "inserted data for PLTR\n",
      "inserted data for PM\n",
      "inserted data for PNC\n",
      "inserted data for PNFP\n",
      "inserted data for PNR\n",
      "inserted data for PNW\n",
      "inserted data for PODD\n",
      "inserted data for POOL\n",
      "inserted data for POR\n",
      "inserted data for POST\n",
      "inserted data for POWI\n",
      "inserted data for PPC\n",
      "inserted data for PPG\n",
      "inserted data for PPL\n",
      "inserted data for PR\n",
      "inserted data for PRCT\n",
      "inserted data for PRGO\n",
      "inserted data for PRI\n",
      "inserted data for PRU\n",
      "inserted data for PSA\n",
      "inserted data for PSN\n",
      "inserted data for PSTG\n",
      "inserted data for PSX\n",
      "inserted data for PTC\n",
      "inserted data for PVH\n",
      "inserted data for PWR\n",
      "inserted data for PYPL\n",
      "inserted data for QCOM\n",
      "inserted data for QLYS\n",
      "inserted data for QRVO\n",
      "inserted data for QSR\n",
      "inserted data for QTWO\n",
      "inserted data for QXO\n",
      "inserted data for QYLD\n",
      "inserted data for R\n",
      "inserted data for RACE\n",
      "inserted data for RARE\n",
      "inserted data for RBC\n",
      "inserted data for RBLX\n",
      "inserted data for RCL\n",
      "inserted data for RDDT\n",
      "inserted data for RDN\n",
      "inserted data for RDNT\n",
      "inserted data for REG\n",
      "inserted data for REGN\n",
      "inserted data for REXR\n",
      "inserted data for REYN\n",
      "inserted data for REZ\n",
      "inserted data for RF\n",
      "inserted data for RGA\n",
      "inserted data for RGEN\n",
      "inserted data for RGLD\n",
      "inserted data for RH\n",
      "inserted data for RHI\n",
      "inserted data for RHP\n",
      "inserted data for RIG\n",
      "inserted data for RITM\n",
      "inserted data for RIVN\n",
      "inserted data for RJF\n",
      "inserted data for RL\n",
      "inserted data for RLI\n",
      "inserted data for RMBS\n",
      "inserted data for RMD\n",
      "inserted data for RNA\n",
      "inserted data for RNR\n",
      "inserted data for ROIV\n",
      "inserted data for ROK\n",
      "inserted data for ROKU\n",
      "inserted data for ROL\n",
      "inserted data for ROP\n",
      "inserted data for ROST\n",
      "inserted data for RPM\n",
      "inserted data for RPRX\n",
      "inserted data for RRC\n",
      "inserted data for RRX\n",
      "inserted data for RS\n",
      "inserted data for RSG\n",
      "inserted data for RTX\n",
      "inserted data for RUN\n",
      "inserted data for RUSHA\n",
      "inserted data for RVMD\n",
      "inserted data for RVTY\n",
      "inserted data for RYAN\n",
      "inserted data for RYN\n",
      "inserted data for S\n",
      "inserted data for SAIA\n",
      "inserted data for SAIC\n",
      "inserted data for SANM\n",
      "inserted data for SATS\n",
      "inserted data for SBAC\n",
      "inserted data for SBRA\n",
      "inserted data for SBUX\n",
      "inserted data for SCCO\n",
      "inserted data for SCHW\n",
      "inserted data for SCI\n",
      "inserted data for SEE\n",
      "inserted data for SEIC\n",
      "inserted data for SEM\n",
      "inserted data for SF\n",
      "inserted data for SFBS\n",
      "inserted data for SFM\n",
      "inserted data for SGI\n",
      "inserted data for SGRY\n",
      "inserted data for SH\n",
      "inserted data for SHAK\n",
      "inserted data for SHC\n",
      "inserted data for SHW\n",
      "inserted data for SIGI\n",
      "inserted data for SIRI\n",
      "inserted data for SITE\n",
      "inserted data for SJM\n",
      "inserted data for SKX\n",
      "inserted data for SKY\n",
      "inserted data for SLAB\n",
      "inserted data for SLB\n",
      "inserted data for SLG\n",
      "inserted data for SLGN\n",
      "inserted data for SLM\n",
      "inserted data for SLV\n",
      "inserted data for SM\n",
      "inserted data for SMAR\n",
      "inserted data for SMCI\n",
      "inserted data for SMG\n",
      "inserted data for SN\n",
      "inserted data for SNA\n",
      "inserted data for SNAP\n",
      "inserted data for SNDR\n",
      "inserted data for SNOW\n",
      "inserted data for SNPS\n",
      "inserted data for SNV\n",
      "inserted data for SNX\n",
      "inserted data for SO\n",
      "inserted data for SOFI\n",
      "inserted data for SOLV\n",
      "inserted data for SON\n",
      "inserted data for SPG\n",
      "inserted data for SPGI\n",
      "inserted data for SPOT\n",
      "inserted data for SPR\n",
      "inserted data for SPSC\n",
      "inserted data for SPXC\n",
      "inserted data for SQ\n",
      "inserted data for SR\n",
      "inserted data for SRE\n",
      "inserted data for SRPT\n",
      "inserted data for SSB\n",
      "inserted data for SSD\n",
      "inserted data for SSNC\n",
      "inserted data for ST\n",
      "inserted data for STAG\n",
      "inserted data for STE\n",
      "inserted data for STLA\n",
      "inserted data for STLD\n",
      "inserted data for STNE\n",
      "inserted data for STNG\n",
      "inserted data for STT\n",
      "inserted data for STVN\n",
      "inserted data for STWD\n",
      "inserted data for STX\n",
      "inserted data for STZ\n",
      "inserted data for SUI\n",
      "inserted data for SUM\n",
      "inserted data for SWK\n",
      "inserted data for SWKS\n",
      "inserted data for SWX\n",
      "inserted data for SYF\n",
      "inserted data for SYK\n",
      "inserted data for SYY\n",
      "inserted data for T\n",
      "inserted data for TAP\n",
      "inserted data for TDG\n",
      "inserted data for TDW\n",
      "inserted data for TDY\n",
      "inserted data for TEAM\n",
      "inserted data for TECH\n",
      "inserted data for TEL\n",
      "inserted data for TEM\n",
      "inserted data for TENB\n",
      "inserted data for TER\n",
      "inserted data for TEX\n",
      "inserted data for TFC\n",
      "inserted data for TFX\n",
      "inserted data for TGT\n",
      "inserted data for THC\n",
      "inserted data for THG\n",
      "inserted data for THO\n",
      "inserted data for TJX\n",
      "inserted data for TKO\n",
      "inserted data for TKR\n",
      "inserted data for TMDX\n",
      "inserted data for TMHC\n",
      "inserted data for TMO\n",
      "inserted data for TMUS\n",
      "inserted data for TNET\n",
      "inserted data for TOL\n",
      "inserted data for TOST\n",
      "inserted data for TPG\n",
      "inserted data for TPH\n",
      "inserted data for TPL\n",
      "inserted data for TPR\n",
      "inserted data for TPX\n",
      "inserted data for TREX\n",
      "inserted data for TRGP\n",
      "inserted data for TRI\n",
      "inserted data for TRMB\n",
      "inserted data for TRNO\n",
      "inserted data for TROW\n",
      "inserted data for TRU\n",
      "inserted data for TRV\n",
      "inserted data for TSCO\n",
      "inserted data for TSEM\n",
      "inserted data for TSLA\n",
      "inserted data for TSN\n",
      "inserted data for TT\n",
      "inserted data for TTC\n",
      "inserted data for TTD\n",
      "inserted data for TTEK\n",
      "inserted data for TTWO\n",
      "inserted data for TW\n",
      "inserted data for TWLO\n",
      "inserted data for TXN\n",
      "inserted data for TXRH\n",
      "inserted data for TXT\n",
      "inserted data for TYL\n",
      "inserted data for U\n",
      "inserted data for UAL\n",
      "inserted data for UBER\n",
      "inserted data for UBS\n",
      "inserted data for UBSI\n",
      "inserted data for UDR\n",
      "inserted data for UFPI\n",
      "inserted data for UGI\n",
      "inserted data for UHAL\n",
      "inserted data for UHS\n",
      "inserted data for UI\n",
      "inserted data for ULS\n",
      "inserted data for ULTA\n",
      "inserted data for UMBF\n",
      "inserted data for UNH\n",
      "inserted data for UNM\n",
      "inserted data for UNP\n",
      "inserted data for UPS\n",
      "inserted data for UPST\n",
      "inserted data for URI\n",
      "inserted data for USB\n",
      "inserted data for USFD\n",
      "inserted data for USM\n",
      "inserted data for UTHR\n",
      "inserted data for V\n",
      "inserted data for VAL\n",
      "inserted data for VCTR\n",
      "inserted data for VEEV\n",
      "inserted data for VERX\n",
      "inserted data for VFC\n",
      "inserted data for VFS\n",
      "inserted data for VICI\n",
      "inserted data for VIK\n",
      "inserted data for VKTX\n",
      "inserted data for VLO\n",
      "inserted data for VLTO\n",
      "inserted data for VLY\n",
      "inserted data for VMC\n",
      "inserted data for VMI\n",
      "inserted data for VNO\n",
      "inserted data for VNOM\n",
      "inserted data for VNT\n",
      "inserted data for VOYA\n",
      "inserted data for VRNS\n",
      "inserted data for VRRM\n",
      "inserted data for VRSK\n",
      "inserted data for VRSN\n",
      "inserted data for VRT\n",
      "inserted data for VRTX\n",
      "inserted data for VST\n",
      "inserted data for VTR\n",
      "inserted data for VTRS\n",
      "inserted data for VVV\n",
      "inserted data for VZ\n",
      "inserted data for W\n",
      "inserted data for WAB\n",
      "inserted data for WAL\n",
      "inserted data for WAT\n",
      "inserted data for WAY\n",
      "inserted data for WBA\n",
      "inserted data for WBD\n",
      "inserted data for WBS\n",
      "inserted data for WCC\n",
      "inserted data for WCN\n",
      "inserted data for WDAY\n",
      "inserted data for WDC\n",
      "inserted data for WEC\n",
      "inserted data for WELL\n",
      "inserted data for WEX\n",
      "inserted data for WFC\n",
      "inserted data for WFRD\n",
      "inserted data for WH\n",
      "inserted data for WHD\n",
      "inserted data for WHR\n",
      "inserted data for WING\n",
      "inserted data for WIX\n",
      "inserted data for WK\n",
      "inserted data for WLK\n",
      "inserted data for WM\n",
      "inserted data for WMB\n",
      "inserted data for WMG\n",
      "inserted data for WMS\n",
      "inserted data for WMT\n",
      "inserted data for WPC\n",
      "inserted data for WRB\n",
      "inserted data for WSC\n",
      "inserted data for WSM\n",
      "inserted data for WSO\n",
      "inserted data for WST\n",
      "inserted data for WTFC\n",
      "inserted data for WTM\n",
      "inserted data for WTRG\n",
      "inserted data for WTS\n",
      "inserted data for WTW\n",
      "inserted data for WU\n",
      "inserted data for WWD\n",
      "inserted data for WY\n",
      "inserted data for WYNN\n",
      "inserted data for X\n",
      "inserted data for XEL\n",
      "inserted data for XOM\n",
      "inserted data for XP\n",
      "inserted data for XPO\n",
      "inserted data for XRAY\n",
      "inserted data for XYL\n",
      "inserted data for XYZ\n",
      "inserted data for YUM\n",
      "inserted data for YUMC\n",
      "inserted data for Z\n",
      "inserted data for ZBH\n",
      "inserted data for ZBRA\n",
      "inserted data for ZETA\n",
      "inserted data for ZG\n",
      "inserted data for ZION\n",
      "inserted data for ZM\n",
      "inserted data for ZS\n",
      "inserted data for ZTS\n",
      "inserted data for ZWS\n"
     ]
    }
   ],
   "source": [
    "# GETTING NULLS! gonna fix the coercion logic\n",
    "# con.execute(\"truncate Articles\")\n",
    "\n",
    "failed_df = xml_loader(headline_jan25_path)\n",
    "\n",
    "# load multicap Test\n",
    "# failed_df = xml_loader(multicap_Test)\n",
    "# load new Test\n",
    "# failed2_df = xml_loader(headline_august24_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with the faulty data \n",
    "failed_df['article_pubDate'] = pd.to_datetime(failed_df['article_pubDate'], errors='coerce')\n",
    "failed_df = failed_df.drop(columns=['parsed_date'])\n",
    "failed_df.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "try:\n",
    "    # adding this too just in case\n",
    "    con.execute(\"INSERT INTO Test.Articles SELECT * FROM failed_df ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try again with the faulty data \n",
    "# failed2_df['article_pubDate'] = pd.to_datetime(failed2_df['article_pubDate'], errors='coerce')\n",
    "# failed2_df = failed2_df.drop(columns=['parsed_date'])\n",
    "# failed2_df.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "# try:\n",
    "#     # adding this too just in case\n",
    "#     con.execute(\"INSERT INTO Test.Articles SELECT * FROM failed2_df ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Articles_Trading_Day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate Test.Articles_Trading_Day\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO Test.Articles_Trading_Day\n",
    "SELECT \n",
    "    a.guid,\n",
    "    a.ticker,\n",
    "    coalesce(MIN(tc.trading_date), cast(a.article_pubDate as Date)) AS mapped_trading_date,\n",
    "    a.description,\n",
    "    a.article_link,\n",
    "    a.article_pubDate,\n",
    "    a.article_title,\n",
    "    a.language,\n",
    "    a.lastBuildDate,\n",
    "    a.link,\n",
    "    a.title\n",
    "FROM (\n",
    "    SELECT \n",
    "        guid,\n",
    "        ticker,\n",
    "        description,\n",
    "        article_link,\n",
    "        article_pubDate,\n",
    "        article_title,\n",
    "        language,\n",
    "        lastBuildDate,\n",
    "        link,\n",
    "        title,\n",
    "        -- 4 PM EST adjust\n",
    "        CASE \n",
    "            WHEN CAST(article_pubDate AS TIME) >= '16:00:00' \n",
    "            THEN CAST(article_pubDate AS DATE) + INTERVAL '1 day'\n",
    "            ELSE CAST(article_pubDate AS DATE)\n",
    "        END AS adjusted_pub_date\n",
    "    FROM Test.Articles\n",
    ") a\n",
    "LEFT JOIN \n",
    "    Test.Trading_Calendar tc\n",
    "ON \n",
    "    tc.trading_date >= a.adjusted_pub_date\n",
    "GROUP BY \n",
    "    a.guid, a.ticker, a.description, a.article_link, a.article_pubDate, \n",
    "    a.article_title, a.language, a.lastBuildDate, a.link, a.title;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `Market_Data_Test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con.execute(\"Truncate Market_Data_Test\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO Test.Market_Data_Test\n",
    "SELECT \n",
    "    md.trading_day_date,\n",
    "    md.ticker,\n",
    "    md.price,\n",
    "    md.volume,\n",
    "    COALESCE(COUNT(DISTINCT atd.guid), 0) AS headline_count\n",
    "FROM \n",
    "    Test.Market_Data_Daily_Processing md\n",
    "LEFT JOIN \n",
    "    Test.Articles_Trading_Day atd\n",
    "ON \n",
    "    md.ticker = atd.ticker AND md.trading_day_date = atd.mapped_trading_date\n",
    "GROUP BY \n",
    "    md.trading_day_date, md.ticker, md.price, md.volume;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `market_article_summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con.execute(\"drop table Test.Market_Article_Summary\")\n",
    "# con.execute(\"\"\"\n",
    "#                 CREATE TABLE IF NOT EXISTS Test.Market_Article_Summary (\n",
    "#     trading_date DATE PRIMARY KEY,\n",
    "#     article_count INT\n",
    "#     );\n",
    "\n",
    "#             \"\"\")\n",
    "con.execute('''\n",
    "INSERT INTO Test.Market_Article_Summary\n",
    "SELECT \n",
    "    atd.mapped_trading_date AS trading_date,\n",
    "    COUNT(DISTINCT atd.guid) AS total_unique_articles\n",
    "FROM \n",
    "    Test.Articles_Trading_Day atd\n",
    "GROUP BY \n",
    "    atd.mapped_trading_date;\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Daily_Price_Movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate Test.Daily_Price_Movement\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO Test.Daily_Price_Movement\n",
    "            SELECT \n",
    "                sp1.trading_day_date AS trading_date,\n",
    "                sp1.ticker,\n",
    "                sp1.price AS close_price,\n",
    "                sp2.trading_day_date AS next_trading_day,\n",
    "                sp2.price AS close_price_next,\n",
    "                ROUND(sp2.price - sp1.price, 2) AS price_change,\n",
    "                ROUND((sp2.price - sp1.price) / sp1.price * 100, 2) AS price_change_percentage\n",
    "            FROM Test.market_data_daily_processing sp1\n",
    "            LEFT JOIN Test.market_data_daily_processing sp2 \n",
    "            ON sp2.ticker = sp1.ticker \n",
    "            AND sp2.trading_day_date = (\n",
    "                SELECT MIN(sp3.trading_day_date) \n",
    "                FROM Test.market_data_daily_processing sp3\n",
    "                WHERE sp3.ticker = sp1.ticker\n",
    "                AND sp3.trading_day_date > sp1.trading_day_date\n",
    "            );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Weekly_Price_Movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate Test.Weekly_Price_Movement\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO Test.Weekly_Price_Movement\n",
    "            WITH WeeklyPrices AS (\n",
    "            SELECT \n",
    "                ticker,\n",
    "                MIN(tc.trading_date) AS trading_week_start,\n",
    "                MAX(tc.trading_date) AS trading_week_end\n",
    "            FROM Test.market_data_daily_processing mdp\n",
    "            JOIN Test.trading_calendar tc \n",
    "            ON mdp.trading_day_date = tc.trading_date\n",
    "            WHERE EXTRACT(DOW FROM tc.trading_date) BETWEEN 1 AND 5  -- Only weekdays\n",
    "            GROUP BY ticker, DATE_TRUNC('week', tc.trading_date)\n",
    "        ),\n",
    "        StartPrices AS (\n",
    "            SELECT \n",
    "                mdp.trading_day_date AS trading_week_start, \n",
    "                mdp.ticker, \n",
    "                mdp.price AS close_price\n",
    "            FROM Test.market_data_daily_processing mdp\n",
    "            JOIN WeeklyPrices wp \n",
    "            ON mdp.ticker = wp.ticker \n",
    "            AND mdp.trading_day_date = wp.trading_week_start\n",
    "        ),\n",
    "        EndPrices AS (\n",
    "            SELECT \n",
    "                mdp.trading_day_date AS trading_week_end, \n",
    "                mdp.ticker, \n",
    "                mdp.price AS close_price_end\n",
    "            FROM Test.market_data_daily_processing mdp\n",
    "            JOIN WeeklyPrices wp \n",
    "            ON mdp.ticker = wp.ticker \n",
    "            AND mdp.trading_day_date = wp.trading_week_end\n",
    "        )\n",
    "        SELECT \n",
    "            sp.trading_week_start,\n",
    "            sp.ticker,\n",
    "            sp.close_price as close_price_start,\n",
    "            ep.trading_week_end,\n",
    "            ep.close_price_end,\n",
    "            ROUND(ep.close_price_end - sp.close_price, 2) AS price_change,\n",
    "            ROUND((ep.close_price_end - sp.close_price) / sp.close_price * 100, 2) AS price_change_percentage\n",
    "        FROM StartPrices sp\n",
    "        JOIN EndPrices ep \n",
    "        ON sp.ticker = ep.ticker \n",
    "        AND sp.trading_week_start = ep.trading_week_end - INTERVAL '4 days';\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `extreme_price_movements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c411619b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate Test.extreme_price_movements\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO Test.extreme_price_movements\n",
    "            SELECT trading_date, ticker, close_price, price_change, price_change_percentage,\n",
    "                CASE \n",
    "                    WHEN price_change_percentage < -5 THEN 'Drop'\n",
    "                    WHEN price_change_percentage > 5 THEN 'Surge'\n",
    "                END AS movement_type\n",
    "            FROM Test.daily_price_movement\n",
    "            WHERE ABS(price_change_percentage) > 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `articles_extreme_drops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: fix this logic for extreme drops and articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x25659245d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate Test.articles_extreme_drops\")\n",
    "df = con.execute(\"\"\"\n",
    "            SELECT epm.trading_date, epm.ticker, a.guid, a.mapped_trading_date,\n",
    "                fs.finbert_title_score AS title_sentiment_score,\n",
    "                fs.finbert_title_label AS title_sentiment_label,\n",
    "                fs.finbert_description_score AS descripton_sentiment_score,\n",
    "                fs.finbert_description_label AS descripton_sentiment_label\n",
    "            FROM Test.extreme_price_movements epm\n",
    "            JOIN Test.articles_trading_day a\n",
    "            ON epm.ticker = a.ticker\n",
    "            AND a.mapped_trading_date BETWEEN epm.trading_date - INTERVAL '3 days' AND epm.trading_date\n",
    "            LEFT JOIN Test.finbert_analysis fs\n",
    "            ON a.guid = fs.guid\n",
    "\"\"\").df() \n",
    "\n",
    "# dedupe based on trading_date, ticker, guid \n",
    "df.drop_duplicates(subset=['trading_date', 'ticker', 'guid'], inplace=True)\n",
    "\n",
    "con.execute(\"INSERT INTO Test.articles_extreme_drops select * from df ON CONFLICT (trading_date, ticker, guid) DO NOTHING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise Exception(\"Test Headlines Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "vix = VIX_FULL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE sp500.VIX_Index (\n",
    "    vix_date DATE PRIMARY KEY,\n",
    "    vix_value FLOAT\n",
    ");\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS sp500.VIX_Index;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `VIX_Index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vix_date</th>\n",
       "      <th>vix_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-01-02</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-01-03</td>\n",
       "      <td>17.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-01-06</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986-01-07</td>\n",
       "      <td>17.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-01-08</td>\n",
       "      <td>19.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vix_date  vix_value\n",
       "0 1986-01-02      18.07\n",
       "1 1986-01-03      17.96\n",
       "2 1986-01-06      17.05\n",
       "3 1986-01-07      17.39\n",
       "4 1986-01-08      19.97"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vix_df = pd.read_csv(vix, names=[\"vix_date\", \"vix_value\"], parse_dates=[\"vix_date\"], skiprows=1) # first row is the header but not the best\n",
    "vix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vix_date     0\n",
       "vix_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nulls \n",
    "vix_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21c42ee4b30>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"Truncate sp500.VIX_Index\")\n",
    "con.execute(\"INSERT INTO sp500.VIX_Index select * from vix_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "DONE WITH VIX",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDONE WITH VIX\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: DONE WITH VIX"
     ]
    }
   ],
   "source": [
    "# raise Exception(\"DONE WITH VIX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT All Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)\n",
    "# THIS IS CREATED BY finbert_all_scores_dask_NEW_DATA.ipynb\n",
    "finbert_all_tags = \"../finbert/finbert_sentiments_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Test.finbert_analysis (\n",
    "    guid UUID ,\n",
    "    ticker VARCHAR(10) NOT NULL,\n",
    "    description TEXT,\n",
    "    article_title TEXT,\n",
    "    finbert_title_label VARCHAR(20) NOT NULL,\n",
    "    finbert_title_score FLOAT NOT NULL,\n",
    "    finbert_title_positive FLOAT NOT NULL,\n",
    "    finbert_title_neutral FLOAT NOT NULL,\n",
    "    finbert_title_negative FLOAT NOT NULL,\n",
    "    finbert_description_label VARCHAR(20) NOT NULL,\n",
    "    finbert_description_score FLOAT NOT NULL,\n",
    "    finbert_description_positive FLOAT NOT NULL,\n",
    "    finbert_description_neutral FLOAT NOT NULL,\n",
    "    finbert_description_negative FLOAT NOT NULL,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    ");\n",
    "\n",
    "\"\"\"]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS Test.finbert_analysis;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "\n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Test.finbert_analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x2567146ca30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate Test.finbert_analysis\")\n",
    "df = pd.read_csv(finbert_all_tags)\n",
    "con.execute(\"INSERT INTO Test.finbert_analysis select guid, ticker, description, article_title, finbert_title_label, finbert_title_score, finbert_title_positive, finbert_title_neutral, finbert_title_negative, finbert_description_label, finbert_description_score, finbert_description_positive, finbert_description_neutral, finbert_description_negative from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "DONE WITH FINBERT",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDONE WITH FINBERT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: DONE WITH FINBERT"
     ]
    }
   ],
   "source": [
    "# raise Exception(\"DONE WITH FINBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data for VIX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(duckdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "CREATE TABLE Test.weekly_training_data(guid VARCHAR,\n",
    "date_t DATE,\n",
    "ticker VARCHAR,\n",
    "subindustry VARCHAR,\n",
    "vix_t FLOAT,\n",
    "vix_t_7_past FLOAT,\n",
    "vix_t_7_future FLOAT,\n",
    "price_t FLOAT,\n",
    "price_t_7_past FLOAT,\n",
    "price_change_t_7 FLOAT,\n",
    "volume_t INTEGER,\n",
    "volume_t_7_past INTEGER,\n",
    "volume_change_t_7 DOUBLE,\n",
    "sentiment_label_t VARCHAR,\n",
    "sentiment_positive_t FLOAT,\n",
    "sentiment_neutral_t FLOAT,\n",
    "sentiment_negative_t FLOAT);\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS Test.weekly_training_data;\"\n",
    "]\n",
    "\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Test.weekly_training_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"truncate Test.weekly_training_data\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO Test.weekly_training_data\n",
    "            SELECT * FROM (\n",
    "WITH vix_lagged AS (\n",
    "    SELECT \n",
    "        v1.vix_date AS date_t,\n",
    "        v1.vix_value AS vix_t,\n",
    "        COALESCE(LAG(v1.vix_value, 1) OVER (ORDER BY v1.vix_date), 17.22) AS vix_t_7_past, -- same add last val\n",
    "        COALESCE(LEAD(v1.vix_value, 1) OVER (ORDER BY v1.vix_date), 23.39) AS vix_t_7_future -- adding to handle last vix day that we don't know\n",
    "    FROM sp500.vix_weekly_training v1\n",
    ")\n",
    "--select * from vix_lagged;\n",
    ",\n",
    "-- ANYTHING WITH -1 in price_t, price_t7 or volume cols should be removed! \n",
    "market_lagged AS (\n",
    "    SELECT \n",
    "        m1.trading_day_date AS date_t,\n",
    "        m1.ticker,\n",
    "        COALESCE(m1.price, -1) AS price_t,  -- Set -1 if all price data is NULL\n",
    "        COALESCE(LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date), -1) AS price_t_7_past,\n",
    "        (CASE \n",
    "            WHEN LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date) IS NOT NULL \n",
    "            THEN ((m1.price - LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date)) / \n",
    "                  LAG(m1.price, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date)) * 100\n",
    "            ELSE NULL \n",
    "        END) AS price_change_t_7,\n",
    "        COALESCE(m1.volume, -1) AS volume_t,  -- Set -1 if all volume data is NULL\n",
    "        COALESCE(LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date), -1) AS volume_t_7_past,\n",
    "        (CASE \n",
    "            WHEN LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date) IS NOT NULL \n",
    "            THEN ((m1.volume - LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date)) / \n",
    "                  NULLIF(LAG(m1.volume, 1) OVER (PARTITION BY m1.ticker ORDER BY m1.trading_day_date), 0)) * 100\n",
    "            ELSE NULL \n",
    "        END) AS volume_change_t_7\n",
    "    FROM Test.Market_Data_Test m1\n",
    "--    where ticker in (\n",
    "--    \tselect * from Test.sp500_active_stocks\n",
    "--    )\n",
    ")\n",
    "--select * from market_lagged;\n",
    ",\n",
    "article_sentiment AS (\n",
    "    SELECT \n",
    "    \ta.guid,\n",
    "        a.mapped_trading_date AS date_t,\n",
    "        a.ticker,\n",
    "        f.finbert_description_positive AS sentiment_positive_t,\n",
    "        f.finbert_description_neutral AS sentiment_neutral_t,\n",
    "        f.finbert_description_negative AS sentiment_negative_t,\n",
    "        f.finbert_description_label AS sentiment_label_t  -- Just default to NEUTRAL\n",
    "    FROM Test.Articles_Trading_Day a\n",
    "    JOIN Test.finbert_analysis f ON a.guid = f.guid\n",
    ")\n",
    "--select * from article_sentiment;\n",
    "--select count(*) from (\n",
    "SELECT distinct -- sometimes we have dupes...I think we have dupes upstream but oh whale\n",
    "s.guid,\n",
    "    v.date_t,\n",
    "    m.ticker,\n",
    "    c.subindustry,\n",
    "    v.vix_t,\n",
    "    v.vix_t_7_past,\n",
    "    v.vix_t_7_future,\n",
    "    m.price_t,\n",
    "    m.price_t_7_past,\n",
    "    m.price_change_t_7,\n",
    "    m.volume_t,\n",
    "    m.volume_t_7_past,\n",
    "    m.volume_change_t_7,\n",
    "    coalesce(s.sentiment_label_t, 'NEUTRAL') as sentiment_label_t,\n",
    "    coalesce(s.sentiment_positive_t, 0) as sentiment_positive_t,\n",
    "    coalesce(s.sentiment_neutral_t, 1) as sentiment_neutral_t,\n",
    "    coalesce(s.sentiment_negative_t, 0) as sentiment_negative_t\n",
    "FROM vix_lagged v\n",
    "JOIN market_lagged m ON v.date_t = m.date_t\n",
    "LEFT JOIN article_sentiment s ON v.date_t = s.date_t AND m.ticker = s.ticker\n",
    "LEFT JOIN sp500.company_info c ON m.ticker = c.ticker  -- NEW JOIN\n",
    "order by v.date_t desc) as insert_query;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a3-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
