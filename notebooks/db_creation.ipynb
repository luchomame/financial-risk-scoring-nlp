{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *THIS IS TO CREATE THE DB*\n",
    "You can download the already made DB file `financial_news.db` from the sharepoint \n",
    "\n",
    "[practicum folder](https://gtvault-my.sharepoint.com/:f:/g/personal/ltupac3_gatech_edu/Eg2gLDzQ8H1JoWUrUIq1G04BPkOXMyxmhgcoL84Q58-5dg?e=80dziH)\n",
    "\n",
    "[db file](https://gtvault-my.sharepoint.com/:u:/g/personal/ltupac3_gatech_edu/Edi6YX6MKPxMud1e5maTIjsBo04ISTst1j7uoxeSVH2OBA?e=XQD3Ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd \n",
    "import os \n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create database\n",
    "Instructions:\n",
    "1. For individual files (company_info_news.txt, volume_news.csv, etc.), copy the relative path to the respective variable below\n",
    "2. For headline data, put the relative path to the folder housing the ticker folders.\n",
    "    - Example: MultiCap_News/HEADLINES houses the individual ticker folders. \n",
    "    - The code will recursively pick up the files from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"financial_news.db\")\n",
    "\n",
    "company_txt_path = 'MultiCap_News\\\\company_info_news.txt'\n",
    "# volume_news_path = 'MultiCap_News\\\\volume_news.csv'\n",
    "volume_news_path = 'Headlines_Data\\\\volume.csv'\n",
    "# pricing_news_path = 'MultiCap_News\\\\pricing_news.csv'\n",
    "pricing_news_path = 'Headlines_Data\\\\pricing.csv'\n",
    "multicap_headlines = 'MultiCap_News\\\\HEADLINES'\n",
    "headline_august24_path = 'HEADLINES_August24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DDLs and indexes\n",
    "ddl_statements = [\n",
    "    \"CREATE SCHEMA IF NOT EXISTS Headlines;\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Articles (\n",
    "        guid TEXT,\n",
    "        ticker TEXT,\n",
    "        description TEXT,\n",
    "        article_link TEXT,\n",
    "        article_pubDate TIMESTAMP,\n",
    "        article_title TEXT,\n",
    "        language TEXT,\n",
    "        lastBuildDate TIMESTAMP,\n",
    "        link TEXT,\n",
    "        title TEXT,\n",
    "        PRIMARY KEY (guid, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Company_Info_News (\n",
    "        ticker TEXT PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        subindustry TEXT\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Pricing_News (\n",
    "        trading_day_date DATE,\n",
    "        ticker TEXT,\n",
    "        price FLOAT,\n",
    "        PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Volume_News (\n",
    "        trading_day_date DATE,\n",
    "        ticker TEXT,\n",
    "        volume INT,\n",
    "        PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    ############ Gold Layer ############\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Market_Data_Daily_Processing (\n",
    "    trading_day_date DATE,\n",
    "    ticker TEXT,\n",
    "    price FLOAT,\n",
    "    volume INT,\n",
    "    PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Market_Data_Headlines (\n",
    "    trading_day_date DATE,\n",
    "    ticker TEXT,\n",
    "    price FLOAT,\n",
    "    volume INT,\n",
    "    headline_count INT,\n",
    "    PRIMARY KEY (trading_day_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Trading_Calendar (\n",
    "    trading_date DATE PRIMARY KEY\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Articles_Trading_Day (\n",
    "    guid TEXT,\n",
    "    ticker TEXT,\n",
    "    mapped_trading_date DATE,\n",
    "    description TEXT,\n",
    "    article_link TEXT,\n",
    "    article_pubDate TIMESTAMP,\n",
    "    article_title TEXT,\n",
    "    language TEXT,\n",
    "    lastBuildDate TIMESTAMP,\n",
    "    link TEXT,\n",
    "    title TEXT,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Market_Article_Summary (\n",
    "    trading_date DATE PRIMARY KEY,\n",
    "    article_count INT\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Daily_Price_Movement (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    close_price FLOAT,\n",
    "    next_trading_day DATE,\n",
    "    close_price_next FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    PRIMARY KEY (trading_date, ticker),\n",
    "    FOREIGN KEY (trading_date) REFERENCES headlines.trading_calendar(trading_date),\n",
    "    FOREIGN KEY (next_trading_day) REFERENCES headlines.trading_calendar(trading_date)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.Weekly_Price_Movement (\n",
    "    trading_week_start DATE,\n",
    "    ticker TEXT,\n",
    "    close_price_start FLOAT,\n",
    "    trading_week_end DATE,\n",
    "    close_price_end FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    PRIMARY KEY (trading_week_start, ticker),\n",
    "    FOREIGN KEY (trading_week_start) REFERENCES headlines.trading_calendar(trading_date),\n",
    "    FOREIGN KEY (trading_week_end) REFERENCES headlines.trading_calendar(trading_date)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.extreme_price_movements (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    close_price FLOAT,\n",
    "    price_change FLOAT,\n",
    "    price_change_percentage FLOAT,\n",
    "    movement_type TEXT,  -- Drop|Surge\n",
    "    PRIMARY KEY (trading_date, ticker)\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS headlines.articles_extreme_drops (\n",
    "    trading_date DATE,\n",
    "    ticker TEXT,\n",
    "    guid TEXT,\n",
    "    mapped_trading_date DATE, \n",
    "    title_sentiment_score FLOAT,\n",
    "    title_sentiment_label TEXT,\n",
    "    description_sentiment_score FLOAT,\n",
    "    description_sentiment_label TEXT,\n",
    "    PRIMARY KEY (trading_date, ticker, guid)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop_statements = [\n",
    "    # \"DROP TABLE IF EXISTS headlines.Articles;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Company_Info_News;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Pricing_News;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Volume_News;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Market_Data_Daily_Processing;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Market_Data_Headlines;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Trading_Calendar;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Articles_Trading_Day;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Market_Article_Summary;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Daily_Price_Movement;\",\n",
    "    # \"DROP TABLE IF EXISTS headlines.Weekly_Price_Movement;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.extreme_price_movements;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.articles_extreme_drops;\"\n",
    "]\n",
    "\n",
    "index_statements = [\n",
    "    # \"CREATE INDEX IF NOT EXISTS idx_articles_pubDate ON headlines.Articles (article_pubDate);\",\n",
    "    # \"CREATE INDEX IF NOT EXISTS idx_articles_pubDate ON headlines.Articles_Trading_Day (article_pubDate);\"\n",
    "    # \"CREATE INDEX IF NOT EXISTS idx_stock_movement_ticker ON headlines.Daily_Price_Movement (ticker);\",\n",
    "    # \"CREATE INDEX IF NOT EXISTS idx_stock_movement_ticker ON headlines.Weekly_Price_Movement (ticker);\"\n",
    "]\n",
    "\n",
    "for drop in drop_statements:\n",
    "    con.execute(drop)\n",
    "\n",
    "for ddl in ddl_statements:\n",
    "    con.execute(ddl)\n",
    "\n",
    "for index in index_statements:\n",
    "    con.execute(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Company Info News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its all in one line\n",
    "with open(company_txt_path, 'r') as file:\n",
    "    lines = file.readline().split('\\\\n')\n",
    "    # con.execute(\"TRUNCATE Company_Info_News\")\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split('|')\n",
    "        # DONT RUN THIS TWICE BY MISTAKE!\n",
    "        con.execute(\"INSERT INTO headlines.Company_Info_News VALUES (?,?,?)\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Volume_News` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x27dd32c67b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(volume_news_path)\n",
    "# df.head()\n",
    "\n",
    "# convert the wide format to long format\n",
    "volume_long_df = df.melt(id_vars=['Date'], var_name='Ticker', value_name='Volume')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "volume_long_df['Date'] = pd.to_datetime(volume_long_df['Date'])\n",
    "volume_long_df['Volume'] = pd.to_numeric(volume_long_df['Volume'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE Volume_News\")\n",
    "con.execute(\"INSERT INTO headlines.Volume_News (trading_day_date, ticker, Volume) SELECT Date, ticker, Volume FROM volume_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Pricing_News`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x27dd32c67b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(pricing_news_path)\n",
    "# convert the wide format to long format\n",
    "pricing_long_df = df.melt(id_vars=['Date'], var_name='Ticker', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "pricing_long_df['Date'] = pd.to_datetime(pricing_long_df['Date'])\n",
    "pricing_long_df['Price'] = pd.to_numeric(pricing_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE Pricing_News\")\n",
    "con.execute(\"INSERT INTO headlines.Pricing_News (trading_day_date, Ticker, price) SELECT Date, ticker, Price FROM pricing_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Market_Data_Daily_Processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x27dd32c67b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con.execute(\"TRUNCATE Market_Data_Daily_Processing\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO headlines.Market_Data_Daily_Processing\n",
    "SELECT \n",
    "    pn.trading_day_date,\n",
    "    pn.ticker,\n",
    "    pn.price,\n",
    "    vn.volume\n",
    "FROM \n",
    "    headlines.Pricing_News pn\n",
    "LEFT JOIN \n",
    "    headlines.Volume_News vn \n",
    "ON \n",
    "    pn.trading_day_date = vn.trading_day_date AND pn.ticker = vn.ticker\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Trading_Calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x27dd32c67b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pricing_dates = pricing_long_df['Date'].drop_duplicates()\n",
    "# volume_dates = volume_long_df['Date'].drop_duplicates()\n",
    "\n",
    "# trading_dates = pd.concat([pricing_dates, volume_dates]).drop_duplicates().sort_values()\n",
    "\n",
    "# # make sure to have correct col name\n",
    "# trading_dates_df = pd.DataFrame(trading_dates, columns=['trading_date'])\n",
    "# con.execute(\"TRUNCATE Trading_Calendar\")\n",
    "con.execute(\n",
    "\"\"\"\n",
    "INSERT INTO headlines.Trading_Calendar\n",
    "SELECT DISTINCT trading_day_date AS trading_date\n",
    "FROM (\n",
    "    SELECT trading_day_date FROM headlines.Pricing_News\n",
    "    UNION\n",
    "    SELECT trading_day_date FROM headlines.Volume_News\n",
    ") AS all_dates\n",
    "ORDER BY trading_date;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_loader(base_dir):\n",
    "    # lets do this in chunks instead\n",
    "    failed_parses = pd.DataFrame()\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        data = [] \n",
    "        # extract ticker from foldername \n",
    "        ticker = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root_element = tree.getroot()\n",
    "                \n",
    "                channel = root_element.find('channel')\n",
    "                if channel is not None:\n",
    "                    # extract metadata info\n",
    "                    language = channel.findtext(\"language\") \n",
    "                    lastBuildDate = channel.findtext(\"lastBuildDate\")\n",
    "                    link = channel.findtext(\"link\")\n",
    "                    title = channel.findtext(\"title\")\n",
    "                    \n",
    "                    # now meat and potatoes\n",
    "                    for item in channel.findall(\"item\"):\n",
    "                        description = item.findtext(\"description\")\n",
    "                        guid = item.findtext(\"guid\")\n",
    "                        article_link = item.findtext(\"link\")\n",
    "                        article_pubDate = item.findtext(\"pubDate\")\n",
    "                        article_title = item.findtext(\"title\")\n",
    "                        \n",
    "                        data.append({\n",
    "                            \"guid\": guid,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"description\": description,\n",
    "                            \"article_link\": article_link,\n",
    "                            \"article_pubDate\": article_pubDate,\n",
    "                            \"article_title\": article_title,\n",
    "                            \"language\": language,\n",
    "                            \"lastBuildDate\": lastBuildDate,\n",
    "                            \"link\": link,\n",
    "                            \"title\": title\n",
    "                        })\n",
    "            except ET.parseError as e:\n",
    "                print(f\"Error parsing file {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "        \n",
    "        # insert the data into the database\n",
    "        if data:\n",
    "            df = pd.DataFrame(data)\n",
    "            # print(\"Performing timestamp coercion for\", ticker)\n",
    "            df['parsed_date'] = pd.to_datetime(df['article_pubDate'], errors='coerce')\n",
    "            df['lastBuildDate'] = pd.to_datetime(df['lastBuildDate'], errors='coerce')\n",
    "            # print(\"Done timestamp coercion for\", ticker)\n",
    "            \n",
    "            # separate failed cases to avoid nulls\n",
    "            current_failed = df[df['parsed_date'].isna()]\n",
    "            current_valid = df[df['parsed_date'].notna()]\n",
    "\n",
    "            # these are good\n",
    "            current_valid = current_valid.assign(article_pubDate=current_valid['parsed_date']).drop(columns=['parsed_date'])\n",
    "\n",
    "            # remove dupes on guid and ticker\n",
    "            current_valid.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "\n",
    "            \n",
    "            failed_parses = pd.concat([failed_parses, current_failed], ignore_index=True)\n",
    "            \n",
    "            try:\n",
    "                # adding this too just in case\n",
    "                con.execute(\"INSERT INTO headlines.Articles SELECT * FROM current_valid ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "                print(\"inserted data for\", ticker)\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting data for {ticker}: {e}\")\n",
    "                \n",
    "    failed_parses.to_csv(\"failed_article_dates.csv\", index=False)\n",
    "    return failed_parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted data for A\n",
      "inserted data for AA\n",
      "inserted data for AAL\n",
      "inserted data for AAON\n",
      "inserted data for AAP\n",
      "inserted data for AAPL\n",
      "inserted data for ABBV\n",
      "inserted data for ABG\n",
      "inserted data for ABNB\n",
      "inserted data for ABT\n",
      "inserted data for ACA\n",
      "inserted data for ACAD\n",
      "inserted data for ACGL\n",
      "inserted data for ACHC\n",
      "inserted data for ACI\n",
      "inserted data for ACIW\n",
      "inserted data for ACLS\n",
      "inserted data for ACLX\n",
      "inserted data for ACM\n",
      "inserted data for ACN\n",
      "inserted data for ACT\n",
      "inserted data for ADBE\n",
      "inserted data for ADC\n",
      "inserted data for ADI\n",
      "inserted data for ADM\n",
      "inserted data for ADP\n",
      "inserted data for ADSK\n",
      "inserted data for ADT\n",
      "inserted data for AEE\n",
      "inserted data for AEIS\n",
      "inserted data for AEL\n",
      "inserted data for AEO\n",
      "inserted data for AEP\n",
      "inserted data for AER\n",
      "inserted data for AES\n",
      "inserted data for AFG\n",
      "inserted data for AFL\n",
      "inserted data for AFRM\n",
      "inserted data for AGCO\n",
      "inserted data for AGNC\n",
      "inserted data for AGO\n",
      "inserted data for AGR\n",
      "inserted data for AI\n",
      "inserted data for AIG\n",
      "inserted data for AIRC\n",
      "inserted data for AIT\n",
      "inserted data for AIZ\n",
      "inserted data for AJG\n",
      "inserted data for AKAM\n",
      "inserted data for AL\n",
      "inserted data for ALB\n",
      "inserted data for ALE\n",
      "inserted data for ALGM\n",
      "inserted data for ALGN\n",
      "inserted data for ALIT\n",
      "inserted data for ALK\n",
      "inserted data for ALKS\n",
      "inserted data for ALL\n",
      "inserted data for ALLE\n",
      "inserted data for ALLY\n",
      "inserted data for ALNY\n",
      "inserted data for ALRM\n",
      "inserted data for ALSN\n",
      "inserted data for ALTR\n",
      "inserted data for ALV\n",
      "inserted data for AM\n",
      "inserted data for AMAT\n",
      "inserted data for AMCR\n",
      "inserted data for AMD\n",
      "inserted data for AME\n",
      "inserted data for AMG\n",
      "inserted data for AMGN\n",
      "inserted data for AMH\n",
      "inserted data for AMKR\n",
      "inserted data for AMN\n",
      "inserted data for AMP\n",
      "inserted data for AMR\n",
      "inserted data for AMT\n",
      "inserted data for AMZN\n",
      "inserted data for AN\n",
      "inserted data for ANET\n",
      "inserted data for ANF\n",
      "inserted data for ANSS\n",
      "inserted data for AON\n",
      "inserted data for AOS\n",
      "inserted data for APA\n",
      "inserted data for APD\n",
      "inserted data for APG\n",
      "inserted data for APH\n",
      "inserted data for APLE\n",
      "inserted data for APLS\n",
      "inserted data for APO\n",
      "inserted data for APP\n",
      "inserted data for APPF\n",
      "inserted data for APPN\n",
      "inserted data for APTV\n",
      "inserted data for AR\n",
      "inserted data for ARCC\n",
      "inserted data for ARE\n",
      "inserted data for ARMK\n",
      "inserted data for ARRY\n",
      "inserted data for ARW\n",
      "inserted data for ARWR\n",
      "inserted data for ASAN\n",
      "inserted data for ASGN\n",
      "inserted data for ASH\n",
      "inserted data for ASML\n",
      "inserted data for ASO\n",
      "inserted data for ATI\n",
      "inserted data for ATKR\n",
      "inserted data for ATO\n",
      "inserted data for ATR\n",
      "inserted data for AUR\n",
      "inserted data for AVAV\n",
      "inserted data for AVB\n",
      "inserted data for AVGO\n",
      "inserted data for AVNT\n",
      "inserted data for AVT\n",
      "inserted data for AVTR\n",
      "inserted data for AVY\n",
      "inserted data for AWI\n",
      "inserted data for AWK\n",
      "inserted data for AXNX\n",
      "inserted data for AXON\n",
      "inserted data for AXP\n",
      "inserted data for AXS\n",
      "inserted data for AXSM\n",
      "inserted data for AXTA\n",
      "inserted data for AYI\n",
      "inserted data for AZEK\n",
      "inserted data for AZO\n",
      "inserted data for AZPN\n",
      "inserted data for AZTA\n",
      "inserted data for BA\n",
      "inserted data for BAC\n",
      "inserted data for BAH\n",
      "inserted data for BALL\n",
      "inserted data for BAP\n",
      "inserted data for BAX\n",
      "inserted data for BBIO\n",
      "inserted data for BBWI\n",
      "inserted data for BBY\n",
      "inserted data for BC\n",
      "inserted data for BCC\n",
      "inserted data for BCO\n",
      "inserted data for BCPC\n",
      "inserted data for BDC\n",
      "inserted data for BDX\n",
      "inserted data for BE\n",
      "inserted data for BECN\n",
      "inserted data for BEN\n",
      "inserted data for BERY\n",
      "inserted data for BFAM\n",
      "inserted data for BG\n",
      "inserted data for BGC\n",
      "inserted data for BHVN\n",
      "inserted data for BIIB\n",
      "inserted data for BILL\n",
      "inserted data for BIO\n",
      "inserted data for BIPC\n",
      "inserted data for BIRK\n",
      "inserted data for BJ\n",
      "inserted data for BK\n",
      "inserted data for BKH\n",
      "inserted data for BKNG\n",
      "inserted data for BKR\n",
      "inserted data for BL\n",
      "inserted data for BLD\n",
      "inserted data for BLDR\n",
      "inserted data for BLK\n",
      "inserted data for BLKB\n",
      "inserted data for BMI\n",
      "inserted data for BMRN\n",
      "inserted data for BMY\n",
      "inserted data for BOKF\n",
      "inserted data for BOX\n",
      "inserted data for BPMC\n",
      "inserted data for BPOP\n",
      "inserted data for BR\n",
      "inserted data for BRBR\n",
      "inserted data for BRKR\n",
      "inserted data for BRO\n",
      "inserted data for BRX\n",
      "inserted data for BRZE\n",
      "inserted data for BSX\n",
      "inserted data for BSY\n",
      "inserted data for BURL\n",
      "inserted data for BWA\n",
      "inserted data for BWXT\n",
      "inserted data for BX\n",
      "inserted data for BXMT\n",
      "inserted data for BXP\n",
      "inserted data for BYD\n",
      "inserted data for C\n",
      "inserted data for CABO\n",
      "inserted data for CACC\n",
      "inserted data for CACI\n",
      "inserted data for CADE\n",
      "inserted data for CAG\n",
      "inserted data for CAH\n",
      "inserted data for CALX\n",
      "inserted data for CAMT\n",
      "inserted data for CAR\n",
      "inserted data for CARR\n",
      "inserted data for CART\n",
      "inserted data for CASY\n",
      "inserted data for CAT\n",
      "inserted data for CAVA\n",
      "inserted data for CB\n",
      "inserted data for CBRE\n",
      "inserted data for CBSH\n",
      "inserted data for CBT\n",
      "inserted data for CBZ\n",
      "inserted data for CC\n",
      "inserted data for CCCS\n",
      "inserted data for CCEP\n",
      "inserted data for CCI\n",
      "inserted data for CCK\n",
      "inserted data for CCL\n",
      "inserted data for CCOI\n",
      "inserted data for CDNS\n",
      "inserted data for CDW\n",
      "inserted data for CE\n",
      "inserted data for CEG\n",
      "inserted data for CELH\n",
      "inserted data for CERE\n",
      "inserted data for CF\n",
      "inserted data for CFG\n",
      "inserted data for CFLT\n",
      "inserted data for CFR\n",
      "inserted data for CG\n",
      "inserted data for CGNX\n",
      "inserted data for CHD\n",
      "inserted data for CHDN\n",
      "inserted data for CHE\n",
      "inserted data for CHH\n",
      "inserted data for CHK\n",
      "inserted data for CHKP\n",
      "inserted data for CHRD\n",
      "inserted data for CHRW\n",
      "inserted data for CHTR\n",
      "inserted data for CHWY\n",
      "inserted data for CHX\n",
      "inserted data for CI\n",
      "inserted data for CIEN\n",
      "inserted data for CINF\n",
      "inserted data for CIVI\n",
      "inserted data for CL\n",
      "inserted data for CLF\n",
      "inserted data for CLH\n",
      "inserted data for CLVT\n",
      "inserted data for CLX\n",
      "inserted data for CMA\n",
      "inserted data for CMC\n",
      "inserted data for CMCSA\n",
      "inserted data for CME\n",
      "inserted data for CMG\n",
      "inserted data for CMI\n",
      "inserted data for CMS\n",
      "inserted data for CNA\n",
      "inserted data for CNC\n",
      "inserted data for CNH\n",
      "inserted data for CNHI\n",
      "inserted data for CNM\n",
      "inserted data for CNMD\n",
      "inserted data for CNP\n",
      "inserted data for CNS\n",
      "inserted data for CNXC\n",
      "inserted data for COF\n",
      "inserted data for COHR\n",
      "inserted data for COIN\n",
      "inserted data for COKE\n",
      "inserted data for COLB\n",
      "inserted data for COLD\n",
      "inserted data for COLM\n",
      "inserted data for COO\n",
      "inserted data for COOP\n",
      "inserted data for COP\n",
      "inserted data for COR\n",
      "inserted data for CORT\n",
      "inserted data for COST\n",
      "inserted data for COTY\n",
      "inserted data for CPA\n",
      "inserted data for CPAY\n",
      "inserted data for CPB\n",
      "inserted data for CPNG\n",
      "inserted data for CPRI\n",
      "inserted data for CPRT\n",
      "inserted data for CPT\n",
      "inserted data for CR\n",
      "inserted data for CRBG\n",
      "inserted data for CRC\n",
      "inserted data for CRDO\n",
      "inserted data for CRH\n",
      "inserted data for CRL\n",
      "inserted data for CRM\n",
      "inserted data for CROX\n",
      "inserted data for CRSP\n",
      "inserted data for CRUS\n",
      "inserted data for CRVL\n",
      "inserted data for CRWD\n",
      "inserted data for CSCO\n",
      "inserted data for CSGP\n",
      "inserted data for CSL\n",
      "inserted data for CSWI\n",
      "inserted data for CSX\n",
      "inserted data for CTAS\n",
      "inserted data for CTLT\n",
      "inserted data for CTRA\n",
      "inserted data for CTSH\n",
      "inserted data for CTVA\n",
      "inserted data for CUBE\n",
      "inserted data for CUZ\n",
      "inserted data for CVI\n",
      "inserted data for CVLT\n",
      "inserted data for CVNA\n",
      "inserted data for CVS\n",
      "inserted data for CVX\n",
      "inserted data for CW\n",
      "inserted data for CWAN\n",
      "inserted data for CWST\n",
      "inserted data for CXM\n",
      "inserted data for CXT\n",
      "inserted data for CYBR\n",
      "inserted data for CYTK\n",
      "inserted data for CZR\n",
      "inserted data for D\n",
      "inserted data for DAL\n",
      "inserted data for DAR\n",
      "inserted data for DASH\n",
      "inserted data for DAY\n",
      "inserted data for DBX\n",
      "inserted data for DCI\n",
      "inserted data for DD\n",
      "inserted data for DDOG\n",
      "inserted data for DDS\n",
      "inserted data for DE\n",
      "inserted data for DECK\n",
      "inserted data for DELL\n",
      "inserted data for DFH\n",
      "inserted data for DFS\n",
      "inserted data for DG\n",
      "inserted data for DGX\n",
      "inserted data for DHI\n",
      "inserted data for DHR\n",
      "inserted data for DINO\n",
      "inserted data for DIOD\n",
      "inserted data for DIS\n",
      "inserted data for DKNG\n",
      "inserted data for DKS\n",
      "inserted data for DLB\n",
      "inserted data for DLO\n",
      "inserted data for DLR\n",
      "inserted data for DLTR\n",
      "inserted data for DNA\n",
      "inserted data for DNB\n",
      "inserted data for DNLI\n",
      "inserted data for DOC\n",
      "inserted data for DOCN\n",
      "inserted data for DOCS\n",
      "inserted data for DOCU\n",
      "inserted data for DOV\n",
      "inserted data for DOW\n",
      "inserted data for DOX\n",
      "inserted data for DPZ\n",
      "inserted data for DRI\n",
      "inserted data for DRS\n",
      "inserted data for DT\n",
      "inserted data for DTE\n",
      "inserted data for DTM\n",
      "inserted data for DUK\n",
      "inserted data for DUOL\n",
      "inserted data for DV\n",
      "inserted data for DVA\n",
      "inserted data for DVN\n",
      "inserted data for DXC\n",
      "inserted data for DXCM\n",
      "inserted data for DY\n",
      "inserted data for EA\n",
      "inserted data for EBAY\n",
      "inserted data for ECL\n",
      "inserted data for ED\n",
      "inserted data for EEFT\n",
      "inserted data for EFX\n",
      "inserted data for EG\n",
      "inserted data for EGP\n",
      "inserted data for EHC\n",
      "inserted data for EIX\n",
      "inserted data for EL\n",
      "inserted data for ELAN\n",
      "inserted data for ELF\n",
      "inserted data for ELS\n",
      "inserted data for ELV\n",
      "inserted data for EME\n",
      "inserted data for EMN\n",
      "inserted data for EMR\n",
      "inserted data for ENLC\n",
      "inserted data for ENPH\n",
      "inserted data for ENS\n",
      "inserted data for ENSG\n",
      "inserted data for ENTG\n",
      "inserted data for EOG\n",
      "inserted data for EPAM\n",
      "inserted data for EPR\n",
      "inserted data for EPRT\n",
      "inserted data for EQH\n",
      "inserted data for EQIX\n",
      "inserted data for EQR\n",
      "inserted data for EQT\n",
      "inserted data for ERIE\n",
      "inserted data for ES\n",
      "inserted data for ESAB\n",
      "inserted data for ESGR\n",
      "inserted data for ESI\n",
      "inserted data for ESLT\n",
      "inserted data for ESNT\n",
      "inserted data for ESS\n",
      "inserted data for ESTC\n",
      "inserted data for ETN\n",
      "inserted data for ETR\n",
      "inserted data for ETRN\n",
      "inserted data for ETSY\n",
      "inserted data for EURN\n",
      "inserted data for EVH\n",
      "inserted data for EVR\n",
      "inserted data for EVRG\n",
      "inserted data for EW\n",
      "inserted data for EWBC\n",
      "inserted data for EXAS\n",
      "inserted data for EXC\n",
      "inserted data for EXEL\n",
      "inserted data for EXLS\n",
      "inserted data for EXP\n",
      "inserted data for EXPD\n",
      "inserted data for EXPE\n",
      "inserted data for EXPO\n",
      "inserted data for EXR\n",
      "inserted data for EXTR\n",
      "inserted data for F\n",
      "inserted data for FAF\n",
      "inserted data for FANG\n",
      "inserted data for FAST\n",
      "inserted data for FBIN\n",
      "inserted data for FCFS\n",
      "inserted data for FCN\n",
      "inserted data for FCNCA\n",
      "inserted data for FCX\n",
      "inserted data for FDS\n",
      "inserted data for FDX\n",
      "inserted data for FE\n",
      "inserted data for FELE\n",
      "inserted data for FERG\n",
      "inserted data for FFIN\n",
      "inserted data for FFIV\n",
      "inserted data for FG\n",
      "inserted data for FHI\n",
      "inserted data for FHN\n",
      "inserted data for FI\n",
      "inserted data for FICO\n",
      "inserted data for FIS\n",
      "inserted data for FITB\n",
      "inserted data for FIVE\n",
      "inserted data for FIVN\n",
      "inserted data for FIX\n",
      "inserted data for FIZZ\n",
      "inserted data for FLEX\n",
      "inserted data for FLNC\n",
      "inserted data for FLO\n",
      "inserted data for FLR\n",
      "inserted data for FLS\n",
      "inserted data for FLYW\n",
      "inserted data for FMC\n",
      "inserted data for FN\n",
      "inserted data for FNB\n",
      "inserted data for FND\n",
      "inserted data for FNF\n",
      "inserted data for FOLD\n",
      "inserted data for FOUR\n",
      "inserted data for FOX\n",
      "inserted data for FOXA\n",
      "inserted data for FOXF\n",
      "inserted data for FR\n",
      "inserted data for FRHC\n",
      "inserted data for FRO\n",
      "inserted data for FROG\n",
      "inserted data for FRPT\n",
      "inserted data for FRSH\n",
      "inserted data for FRT\n",
      "inserted data for FSK\n",
      "inserted data for FSLR\n",
      "inserted data for FSLY\n",
      "inserted data for FSS\n",
      "inserted data for FTAI\n",
      "inserted data for FTI\n",
      "inserted data for FTNT\n",
      "inserted data for FTV\n",
      "inserted data for FUL\n",
      "inserted data for FWONA\n",
      "inserted data for FWONK\n",
      "inserted data for FYBR\n",
      "inserted data for G\n",
      "inserted data for GATX\n",
      "inserted data for GBCI\n",
      "inserted data for GD\n",
      "inserted data for GDDY\n",
      "inserted data for GE\n",
      "inserted data for GEN\n",
      "inserted data for GFF\n",
      "inserted data for GFS\n",
      "inserted data for GGG\n",
      "inserted data for GH\n",
      "inserted data for GILD\n",
      "inserted data for GIS\n",
      "inserted data for GKOS\n",
      "inserted data for GL\n",
      "inserted data for GLBE\n",
      "inserted data for GLOB\n",
      "inserted data for GLPI\n",
      "inserted data for GLW\n",
      "inserted data for GM\n",
      "inserted data for GME\n",
      "inserted data for GMED\n",
      "inserted data for GMS\n",
      "inserted data for GNRC\n",
      "inserted data for GNTX\n",
      "inserted data for GO\n",
      "inserted data for GOLF\n",
      "inserted data for GOOG\n",
      "inserted data for GOOGL\n",
      "inserted data for GPC\n",
      "inserted data for GPI\n",
      "inserted data for GPK\n",
      "inserted data for GPN\n",
      "inserted data for GPS\n",
      "inserted data for GRAB\n",
      "inserted data for GRMN\n",
      "inserted data for GS\n",
      "inserted data for GT\n",
      "inserted data for GTES\n",
      "inserted data for GTLB\n",
      "inserted data for GTLS\n",
      "inserted data for GWRE\n",
      "inserted data for GWW\n",
      "inserted data for GXO\n",
      "inserted data for H\n",
      "inserted data for HAE\n",
      "inserted data for HAL\n",
      "inserted data for HALO\n",
      "inserted data for HAS\n",
      "inserted data for HBAN\n",
      "inserted data for HCA\n",
      "inserted data for HD\n",
      "inserted data for HEI\n",
      "inserted data for HES\n",
      "inserted data for HGV\n",
      "inserted data for HHH\n",
      "inserted data for HIG\n",
      "inserted data for HII\n",
      "inserted data for HLI\n",
      "inserted data for HLNE\n",
      "inserted data for HLT\n",
      "inserted data for HOG\n",
      "inserted data for HOLX\n",
      "inserted data for HOMB\n",
      "inserted data for HON\n",
      "inserted data for HOOD\n",
      "inserted data for HP\n",
      "inserted data for HPE\n",
      "inserted data for HPQ\n",
      "inserted data for HQY\n",
      "inserted data for HR\n",
      "inserted data for HRB\n",
      "inserted data for HRI\n",
      "inserted data for HRL\n",
      "inserted data for HSIC\n",
      "inserted data for HST\n",
      "inserted data for HSY\n",
      "inserted data for HUBB\n",
      "inserted data for HUBS\n",
      "inserted data for HUM\n",
      "inserted data for HUN\n",
      "inserted data for HWC\n",
      "inserted data for HWM\n",
      "inserted data for HXL\n",
      "inserted data for IAC\n",
      "inserted data for IBKR\n",
      "inserted data for IBM\n",
      "inserted data for IBP\n",
      "inserted data for ICE\n",
      "inserted data for ICL\n",
      "inserted data for ICLR\n",
      "inserted data for IDA\n",
      "inserted data for IDXX\n",
      "inserted data for IEX\n",
      "inserted data for IFF\n",
      "inserted data for IGT\n",
      "inserted data for ILMN\n",
      "inserted data for IMVT\n",
      "inserted data for INCY\n",
      "inserted data for INFA\n",
      "inserted data for INGR\n",
      "inserted data for INSM\n",
      "inserted data for INSP\n",
      "inserted data for INTC\n",
      "inserted data for INTU\n",
      "inserted data for INVH\n",
      "inserted data for IONQ\n",
      "inserted data for IONS\n",
      "inserted data for IOT\n",
      "inserted data for IOVA\n",
      "inserted data for IP\n",
      "inserted data for IPAR\n",
      "inserted data for IPG\n",
      "inserted data for IPGP\n",
      "inserted data for IQV\n",
      "inserted data for IR\n",
      "inserted data for IRDM\n",
      "inserted data for IRM\n",
      "inserted data for IRT\n",
      "inserted data for IRTC\n",
      "inserted data for ISRG\n",
      "inserted data for IT\n",
      "inserted data for ITCI\n",
      "inserted data for ITGR\n",
      "inserted data for ITRI\n",
      "inserted data for ITT\n",
      "inserted data for ITW\n",
      "inserted data for IVZ\n",
      "inserted data for J\n",
      "inserted data for JAZZ\n",
      "inserted data for JBHT\n",
      "inserted data for JBL\n",
      "inserted data for JCI\n",
      "inserted data for JEF\n",
      "inserted data for JHG\n",
      "inserted data for JJSF\n",
      "inserted data for JKHY\n",
      "inserted data for JLL\n",
      "inserted data for JNJ\n",
      "inserted data for JNPR\n",
      "inserted data for JOBY\n",
      "inserted data for JOE\n",
      "inserted data for JPM\n",
      "inserted data for JWN\n",
      "inserted data for JXN\n",
      "inserted data for K\n",
      "inserted data for KAI\n",
      "inserted data for KBH\n",
      "inserted data for KBR\n",
      "inserted data for KD\n",
      "inserted data for KDP\n",
      "inserted data for KEX\n",
      "inserted data for KEY\n",
      "inserted data for KEYS\n",
      "inserted data for KGC\n",
      "inserted data for KHC\n",
      "inserted data for KIM\n",
      "inserted data for KKR\n",
      "inserted data for KLAC\n",
      "inserted data for KMB\n",
      "inserted data for KMI\n",
      "inserted data for KMPR\n",
      "inserted data for KMX\n",
      "inserted data for KNF\n",
      "inserted data for KNSL\n",
      "inserted data for KNX\n",
      "inserted data for KO\n",
      "inserted data for KR\n",
      "inserted data for KRC\n",
      "inserted data for KRG\n",
      "inserted data for KRTX\n",
      "inserted data for KRYS\n",
      "inserted data for KVUE\n",
      "inserted data for KVYO\n",
      "inserted data for KWR\n",
      "inserted data for L\n",
      "inserted data for LAD\n",
      "inserted data for LAMR\n",
      "inserted data for LANC\n",
      "inserted data for LAND\n",
      "inserted data for LAZ\n",
      "inserted data for LBRDA\n",
      "inserted data for LBRDK\n",
      "inserted data for LBRT\n",
      "inserted data for LBTYA\n",
      "inserted data for LBTYK\n",
      "inserted data for LCID\n",
      "inserted data for LDOS\n",
      "inserted data for LEA\n",
      "inserted data for LECO\n",
      "inserted data for LEG\n",
      "inserted data for LEN\n",
      "inserted data for LEVI\n",
      "inserted data for LFST\n",
      "inserted data for LFUS\n",
      "inserted data for LH\n",
      "inserted data for LHX\n",
      "inserted data for LII\n",
      "inserted data for LIN\n",
      "inserted data for LKQ\n",
      "inserted data for LLY\n",
      "inserted data for LLYVA\n",
      "inserted data for LLYVK\n",
      "inserted data for LMT\n",
      "inserted data for LNC\n",
      "inserted data for LNG\n",
      "inserted data for LNT\n",
      "inserted data for LNTH\n",
      "inserted data for LNW\n",
      "inserted data for LOGI\n",
      "inserted data for LOPE\n",
      "inserted data for LOW\n",
      "inserted data for LPLA\n",
      "inserted data for LPX\n",
      "inserted data for LRCX\n",
      "inserted data for LSCC\n",
      "inserted data for LSTR\n",
      "inserted data for LSXMA\n",
      "inserted data for LSXMK\n",
      "inserted data for LULU\n",
      "inserted data for LUV\n",
      "inserted data for LVS\n",
      "inserted data for LW\n",
      "inserted data for LYB\n",
      "inserted data for LYFT\n",
      "inserted data for LYV\n",
      "inserted data for M\n",
      "inserted data for MA\n",
      "inserted data for MAA\n",
      "inserted data for MAC\n",
      "inserted data for MAIN\n",
      "inserted data for MAN\n",
      "inserted data for MANH\n",
      "inserted data for MAR\n",
      "inserted data for MARA\n",
      "inserted data for MAS\n",
      "inserted data for MASI\n",
      "inserted data for MAT\n",
      "inserted data for MATX\n",
      "inserted data for MBLY\n",
      "inserted data for MC\n",
      "inserted data for MCD\n",
      "inserted data for MCHP\n",
      "inserted data for MCK\n",
      "inserted data for MCO\n",
      "inserted data for MDB\n",
      "inserted data for MDC\n",
      "inserted data for MDGL\n",
      "inserted data for MDLZ\n",
      "inserted data for MDT\n",
      "inserted data for MDU\n",
      "inserted data for MEDP\n",
      "inserted data for MELI\n",
      "inserted data for MET\n",
      "inserted data for META\n",
      "inserted data for MGM\n",
      "inserted data for MGY\n",
      "inserted data for MHK\n",
      "inserted data for MHO\n",
      "inserted data for MIDD\n",
      "inserted data for MKC\n",
      "inserted data for MKL\n",
      "inserted data for MKSI\n",
      "inserted data for MKTX\n",
      "inserted data for MLI\n",
      "inserted data for MLM\n",
      "inserted data for MMC\n",
      "inserted data for MMM\n",
      "inserted data for MMS\n",
      "inserted data for MMSI\n",
      "inserted data for MMYT\n",
      "inserted data for MNDY\n",
      "inserted data for MNST\n",
      "inserted data for MO\n",
      "inserted data for MOD\n",
      "inserted data for MOH\n",
      "inserted data for MORN\n",
      "inserted data for MOS\n",
      "inserted data for MP\n",
      "inserted data for MPC\n",
      "inserted data for MPW\n",
      "inserted data for MPWR\n",
      "inserted data for MQ\n",
      "inserted data for MRK\n",
      "inserted data for MRNA\n",
      "inserted data for MRO\n",
      "inserted data for MRVL\n",
      "inserted data for MS\n",
      "inserted data for MSA\n",
      "inserted data for MSCI\n",
      "inserted data for MSFT\n",
      "inserted data for MSGS\n",
      "inserted data for MSI\n",
      "inserted data for MSM\n",
      "inserted data for MSTR\n",
      "inserted data for MTB\n",
      "inserted data for MTCH\n",
      "inserted data for MTD\n",
      "inserted data for MTDR\n",
      "inserted data for MTG\n",
      "inserted data for MTH\n",
      "inserted data for MTN\n",
      "inserted data for MTSI\n",
      "inserted data for MTZ\n",
      "inserted data for MU\n",
      "inserted data for MUR\n",
      "inserted data for MUSA\n",
      "inserted data for NARI\n",
      "inserted data for NBIX\n",
      "inserted data for NCLH\n",
      "inserted data for NCNO\n",
      "inserted data for NDAQ\n",
      "inserted data for NDSN\n",
      "inserted data for NE\n",
      "inserted data for NEE\n",
      "inserted data for NEM\n",
      "inserted data for NEOG\n",
      "inserted data for NET\n",
      "inserted data for NEU\n",
      "inserted data for NFE\n",
      "inserted data for NFG\n",
      "inserted data for NFLX\n",
      "inserted data for NI\n",
      "inserted data for NJR\n",
      "inserted data for NKE\n",
      "inserted data for NLY\n",
      "inserted data for NNN\n",
      "inserted data for NOC\n",
      "inserted data for NOG\n",
      "inserted data for NOV\n",
      "inserted data for NOVT\n",
      "inserted data for NOW\n",
      "inserted data for NRG\n",
      "inserted data for NSC\n",
      "inserted data for NSIT\n",
      "inserted data for NSP\n",
      "inserted data for NTAP\n",
      "inserted data for NTLA\n",
      "inserted data for NTNX\n",
      "inserted data for NTRA\n",
      "inserted data for NTRS\n",
      "inserted data for NU\n",
      "inserted data for NUE\n",
      "inserted data for NUVL\n",
      "inserted data for NVDA\n",
      "inserted data for NVMI\n",
      "inserted data for NVR\n",
      "inserted data for NVST\n",
      "inserted data for NVT\n",
      "inserted data for NWE\n",
      "inserted data for NWL\n",
      "inserted data for NWS\n",
      "inserted data for NWSA\n",
      "inserted data for NXE\n",
      "inserted data for NXPI\n",
      "inserted data for NXST\n",
      "inserted data for NYCB\n",
      "inserted data for NYT\n",
      "inserted data for O\n",
      "inserted data for OBDC\n",
      "inserted data for OC\n",
      "inserted data for ODFL\n",
      "inserted data for OGE\n",
      "inserted data for OGN\n",
      "inserted data for OGS\n",
      "inserted data for OHI\n",
      "inserted data for OKE\n",
      "inserted data for OKTA\n",
      "inserted data for OLED\n",
      "inserted data for OLLI\n",
      "inserted data for OLN\n",
      "inserted data for OMC\n",
      "inserted data for OMF\n",
      "inserted data for ON\n",
      "inserted data for ONB\n",
      "inserted data for ONON\n",
      "inserted data for ONTO\n",
      "inserted data for OPCH\n",
      "inserted data for ORA\n",
      "inserted data for ORCL\n",
      "inserted data for ORI\n",
      "inserted data for ORLY\n",
      "inserted data for OSCR\n",
      "inserted data for OSK\n",
      "inserted data for OTEX\n",
      "inserted data for OTIS\n",
      "inserted data for OTTR\n",
      "inserted data for OVV\n",
      "inserted data for OWL\n",
      "inserted data for OXY\n",
      "inserted data for OZK\n",
      "inserted data for PAG\n",
      "inserted data for PAGP\n",
      "inserted data for PAGS\n",
      "inserted data for PANW\n",
      "inserted data for PARA\n",
      "inserted data for PATH\n",
      "inserted data for PAYC\n",
      "inserted data for PAYX\n",
      "inserted data for PB\n",
      "inserted data for PBF\n",
      "inserted data for PBH\n",
      "inserted data for PCAR\n",
      "inserted data for PCG\n",
      "inserted data for PCH\n",
      "inserted data for PCOR\n",
      "inserted data for PCTY\n",
      "inserted data for PCVX\n",
      "inserted data for PECO\n",
      "inserted data for PEG\n",
      "inserted data for PEGA\n",
      "inserted data for PEN\n",
      "inserted data for PEP\n",
      "inserted data for PFE\n",
      "inserted data for PFG\n",
      "inserted data for PFGC\n",
      "inserted data for PFSI\n",
      "inserted data for PG\n",
      "inserted data for PGNY\n",
      "inserted data for PGR\n",
      "inserted data for PH\n",
      "inserted data for PHM\n",
      "inserted data for PII\n",
      "inserted data for PINS\n",
      "inserted data for PK\n",
      "inserted data for PKG\n",
      "inserted data for PLD\n",
      "inserted data for PLNT\n",
      "inserted data for PLTR\n",
      "inserted data for PLUG\n",
      "inserted data for PM\n",
      "inserted data for PNC\n",
      "inserted data for PNFP\n",
      "inserted data for PNR\n",
      "inserted data for PNW\n",
      "inserted data for PODD\n",
      "inserted data for POOL\n",
      "inserted data for POR\n",
      "inserted data for POST\n",
      "inserted data for POWI\n",
      "inserted data for PPC\n",
      "inserted data for PPG\n",
      "inserted data for PPL\n",
      "inserted data for PR\n",
      "inserted data for PRGO\n",
      "inserted data for PRI\n",
      "inserted data for PRKS\n",
      "inserted data for PRU\n",
      "inserted data for PSA\n",
      "inserted data for PSN\n",
      "inserted data for PSTG\n",
      "inserted data for PSX\n",
      "inserted data for PTC\n",
      "inserted data for PTEN\n",
      "inserted data for PVH\n",
      "inserted data for PWR\n",
      "inserted data for PWSC\n",
      "inserted data for PXD\n",
      "inserted data for PYCR\n",
      "inserted data for PYPL\n",
      "inserted data for QCOM\n",
      "inserted data for QLYS\n",
      "inserted data for QRVO\n",
      "inserted data for QS\n",
      "inserted data for QSR\n",
      "inserted data for R\n",
      "inserted data for RACE\n",
      "inserted data for RARE\n",
      "inserted data for RBC\n",
      "inserted data for RBLX\n",
      "inserted data for RCL\n",
      "inserted data for RCM\n",
      "inserted data for RDN\n",
      "inserted data for REG\n",
      "inserted data for REGN\n",
      "inserted data for RELY\n",
      "inserted data for REXR\n",
      "inserted data for REYN\n",
      "inserted data for RF\n",
      "inserted data for RGA\n",
      "inserted data for RGEN\n",
      "inserted data for RGLD\n",
      "inserted data for RH\n",
      "inserted data for RHI\n",
      "inserted data for RHP\n",
      "inserted data for RIG\n",
      "inserted data for RIOT\n",
      "inserted data for RITM\n",
      "inserted data for RIVN\n",
      "inserted data for RJF\n",
      "inserted data for RL\n",
      "inserted data for RLI\n",
      "inserted data for RMBS\n",
      "inserted data for RMD\n",
      "inserted data for RNR\n",
      "inserted data for ROIV\n",
      "inserted data for ROK\n",
      "inserted data for ROKU\n",
      "inserted data for ROL\n",
      "inserted data for ROP\n",
      "inserted data for ROST\n",
      "inserted data for RPD\n",
      "inserted data for RPM\n",
      "inserted data for RPRX\n",
      "inserted data for RRC\n",
      "inserted data for RRR\n",
      "inserted data for RRX\n",
      "inserted data for RS\n",
      "inserted data for RSG\n",
      "inserted data for RTX\n",
      "inserted data for RUN\n",
      "inserted data for RUSHA\n",
      "inserted data for RVMD\n",
      "inserted data for RVTY\n",
      "inserted data for RYAN\n",
      "inserted data for RYN\n",
      "inserted data for S\n",
      "inserted data for SAIA\n",
      "inserted data for SAIC\n",
      "inserted data for SAM\n",
      "inserted data for SANM\n",
      "inserted data for SATS\n",
      "inserted data for SBAC\n",
      "inserted data for SBUX\n",
      "inserted data for SCCO\n",
      "inserted data for SCHW\n",
      "inserted data for SCI\n",
      "inserted data for SEDG\n",
      "inserted data for SEE\n",
      "inserted data for SEIC\n",
      "inserted data for SEM\n",
      "inserted data for SF\n",
      "inserted data for SFBS\n",
      "inserted data for SFM\n",
      "inserted data for SGEN\n",
      "inserted data for SGRY\n",
      "inserted data for SHAK\n",
      "inserted data for SHC\n",
      "inserted data for SHLS\n",
      "inserted data for SHW\n",
      "inserted data for SIG\n",
      "inserted data for SIGI\n",
      "inserted data for SIRI\n",
      "inserted data for SITE\n",
      "inserted data for SJM\n",
      "inserted data for SKX\n",
      "inserted data for SKY\n",
      "inserted data for SLAB\n",
      "inserted data for SLB\n",
      "inserted data for SLGN\n",
      "inserted data for SLM\n",
      "inserted data for SM\n",
      "inserted data for SMAR\n",
      "inserted data for SMCI\n",
      "inserted data for SMG\n",
      "inserted data for SMPL\n",
      "inserted data for SN\n",
      "inserted data for SNA\n",
      "inserted data for SNAP\n",
      "inserted data for SNDR\n",
      "inserted data for SNOW\n",
      "inserted data for SNPS\n",
      "inserted data for SNV\n",
      "inserted data for SNX\n",
      "inserted data for SO\n",
      "inserted data for SOFI\n",
      "inserted data for SON\n",
      "inserted data for SPG\n",
      "inserted data for SPGI\n",
      "inserted data for SPLK\n",
      "inserted data for SPOT\n",
      "inserted data for SPSC\n",
      "inserted data for SPT\n",
      "inserted data for SPXC\n",
      "inserted data for SQ\n",
      "inserted data for SQSP\n",
      "inserted data for SR\n",
      "inserted data for SRCL\n",
      "inserted data for SRE\n",
      "inserted data for SRPT\n",
      "inserted data for SSB\n",
      "inserted data for SSD\n",
      "inserted data for SSNC\n",
      "inserted data for ST\n",
      "inserted data for STAG\n",
      "inserted data for STE\n",
      "inserted data for STLA\n",
      "inserted data for STLD\n",
      "inserted data for STNE\n",
      "inserted data for STNG\n",
      "inserted data for STT\n",
      "inserted data for STVN\n",
      "inserted data for STWD\n",
      "inserted data for STX\n",
      "inserted data for STZ\n",
      "inserted data for SUI\n",
      "inserted data for SUM\n",
      "inserted data for SVV\n",
      "inserted data for SWAV\n",
      "inserted data for SWK\n",
      "inserted data for SWKS\n",
      "inserted data for SWN\n",
      "inserted data for SWTX\n",
      "inserted data for SWX\n",
      "inserted data for SYF\n",
      "inserted data for SYK\n",
      "inserted data for SYM\n",
      "inserted data for SYNA\n",
      "inserted data for SYY\n",
      "inserted data for T\n",
      "inserted data for TAP\n",
      "inserted data for TDC\n",
      "inserted data for TDG\n",
      "inserted data for TDW\n",
      "inserted data for TDY\n",
      "inserted data for TEAM\n",
      "inserted data for TECH\n",
      "inserted data for TEL\n",
      "inserted data for TENB\n",
      "inserted data for TER\n",
      "inserted data for TEX\n",
      "inserted data for TFC\n",
      "inserted data for TFSL\n",
      "inserted data for TFX\n",
      "inserted data for TGNA\n",
      "inserted data for TGT\n",
      "inserted data for THC\n",
      "inserted data for THG\n",
      "inserted data for THO\n",
      "inserted data for TJX\n",
      "inserted data for TKO\n",
      "inserted data for TKR\n",
      "inserted data for TMHC\n",
      "inserted data for TMO\n",
      "inserted data for TMUS\n",
      "inserted data for TNET\n",
      "inserted data for TOL\n",
      "inserted data for TOST\n",
      "inserted data for TPG\n",
      "inserted data for TPH\n",
      "inserted data for TPL\n",
      "inserted data for TPR\n",
      "inserted data for TPX\n",
      "inserted data for TREX\n",
      "inserted data for TRGP\n",
      "inserted data for TRI\n",
      "inserted data for TRIP\n",
      "inserted data for TRMB\n",
      "inserted data for TRNO\n",
      "inserted data for TROW\n",
      "inserted data for TRU\n",
      "inserted data for TRV\n",
      "inserted data for TSCO\n",
      "inserted data for TSEM\n",
      "inserted data for TSLA\n",
      "inserted data for TSN\n",
      "inserted data for TT\n",
      "inserted data for TTC\n",
      "inserted data for TTD\n",
      "inserted data for TTEK\n",
      "inserted data for TTWO\n",
      "inserted data for TW\n",
      "inserted data for TWLO\n",
      "inserted data for TXG\n",
      "inserted data for TXN\n",
      "inserted data for TXRH\n",
      "inserted data for TXT\n",
      "inserted data for TYL\n",
      "inserted data for U\n",
      "inserted data for UA\n",
      "inserted data for UAA\n",
      "inserted data for UAL\n",
      "inserted data for UBER\n",
      "inserted data for UBS\n",
      "inserted data for UBSI\n",
      "inserted data for UCBI\n",
      "inserted data for UDR\n",
      "inserted data for UFPI\n",
      "inserted data for UGI\n",
      "inserted data for UHAL\n",
      "inserted data for UHS\n",
      "inserted data for UI\n",
      "inserted data for ULTA\n",
      "inserted data for UMBF\n",
      "inserted data for UNH\n",
      "inserted data for UNM\n",
      "inserted data for UNP\n",
      "inserted data for UPS\n",
      "inserted data for URBN\n",
      "inserted data for URI\n",
      "inserted data for USB\n",
      "inserted data for USFD\n",
      "inserted data for UTHR\n",
      "inserted data for V\n",
      "inserted data for VAL\n",
      "inserted data for VC\n",
      "inserted data for VCTR\n",
      "inserted data for VEEV\n",
      "inserted data for VERX\n",
      "inserted data for VFC\n",
      "inserted data for VFS\n",
      "inserted data for VICI\n",
      "inserted data for VICR\n",
      "inserted data for VKTX\n",
      "inserted data for VLO\n",
      "inserted data for VLTO\n",
      "inserted data for VLY\n",
      "inserted data for VMC\n",
      "inserted data for VMI\n",
      "inserted data for VNO\n",
      "inserted data for VNT\n",
      "inserted data for VOYA\n",
      "inserted data for VRNS\n",
      "inserted data for VRRM\n",
      "inserted data for VRSK\n",
      "inserted data for VRSN\n",
      "inserted data for VRT\n",
      "inserted data for VRTX\n",
      "inserted data for VST\n",
      "inserted data for VTR\n",
      "inserted data for VTRS\n",
      "inserted data for VVV\n",
      "inserted data for VZ\n",
      "inserted data for W\n",
      "inserted data for WAB\n",
      "inserted data for WAL\n",
      "inserted data for WAT\n",
      "inserted data for WBA\n",
      "inserted data for WBD\n",
      "inserted data for WBS\n",
      "inserted data for WCC\n",
      "inserted data for WCN\n",
      "inserted data for WDAY\n",
      "inserted data for WDC\n",
      "inserted data for WDFC\n",
      "inserted data for WEC\n",
      "inserted data for WELL\n",
      "inserted data for WEN\n",
      "inserted data for WEX\n",
      "inserted data for WFC\n",
      "inserted data for WFRD\n",
      "inserted data for WH\n",
      "inserted data for WHR\n",
      "inserted data for WING\n",
      "inserted data for WIRE\n",
      "inserted data for WIX\n",
      "inserted data for WK\n",
      "inserted data for WLK\n",
      "inserted data for WM\n",
      "inserted data for WMB\n",
      "inserted data for WMG\n",
      "inserted data for WMS\n",
      "inserted data for WMT\n",
      "inserted data for WPC\n",
      "inserted data for WRB\n",
      "inserted data for WRK\n",
      "inserted data for WSC\n",
      "inserted data for WSM\n",
      "inserted data for WSO\n",
      "inserted data for WST\n",
      "inserted data for WTFC\n",
      "inserted data for WTM\n",
      "inserted data for WTRG\n",
      "inserted data for WTS\n",
      "inserted data for WTW\n",
      "inserted data for WU\n",
      "inserted data for WWD\n",
      "inserted data for WY\n",
      "inserted data for WYNN\n",
      "inserted data for X\n",
      "inserted data for XEL\n",
      "inserted data for XENE\n",
      "inserted data for XOM\n",
      "inserted data for XP\n",
      "inserted data for XPO\n",
      "inserted data for XRAY\n",
      "inserted data for XYL\n",
      "inserted data for YETI\n",
      "inserted data for YUM\n",
      "inserted data for YUMC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jovan\\AppData\\Local\\Temp\\ipykernel_20740\\1589084342.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['parsed_date'] = pd.to_datetime(df['article_pubDate'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted data for Z\n",
      "inserted data for ZBH\n",
      "inserted data for ZBRA\n",
      "inserted data for ZG\n",
      "inserted data for ZI\n",
      "inserted data for ZION\n",
      "inserted data for ZM\n",
      "inserted data for ZS\n",
      "inserted data for ZTS\n",
      "inserted data for ZWS\n",
      "inserted data for A\n",
      "inserted data for AA\n",
      "inserted data for AAL\n",
      "inserted data for AAON\n",
      "inserted data for AAP\n",
      "inserted data for AAPL\n",
      "inserted data for ABBV\n",
      "inserted data for ABCB\n",
      "inserted data for ABG\n",
      "inserted data for ABNB\n",
      "inserted data for ABT\n",
      "inserted data for ACA\n",
      "inserted data for ACAD\n",
      "inserted data for ACGL\n",
      "inserted data for ACHC\n",
      "inserted data for ACI\n",
      "inserted data for ACIW\n",
      "inserted data for ACLS\n",
      "inserted data for ACLX\n",
      "inserted data for ACM\n",
      "inserted data for ACN\n",
      "inserted data for ACT\n",
      "inserted data for ADBE\n",
      "inserted data for ADC\n",
      "inserted data for ADI\n",
      "inserted data for ADM\n",
      "inserted data for ADMA\n",
      "inserted data for ADP\n",
      "inserted data for ADSK\n",
      "inserted data for ADT\n",
      "inserted data for AEE\n",
      "inserted data for AEIS\n",
      "inserted data for AEO\n",
      "inserted data for AEP\n",
      "inserted data for AER\n",
      "inserted data for AES\n",
      "inserted data for AFG\n",
      "inserted data for AFL\n",
      "inserted data for AFRM\n",
      "inserted data for AGCO\n",
      "inserted data for AGNC\n",
      "inserted data for AGO\n",
      "inserted data for AGR\n",
      "inserted data for AI\n",
      "inserted data for AIG\n",
      "inserted data for AIT\n",
      "inserted data for AIZ\n",
      "inserted data for AJG\n",
      "inserted data for AKAM\n",
      "inserted data for AL\n",
      "inserted data for ALAB\n",
      "inserted data for ALB\n",
      "inserted data for ALGM\n",
      "inserted data for ALGN\n",
      "inserted data for ALIT\n",
      "inserted data for ALK\n",
      "inserted data for ALKS\n",
      "inserted data for ALL\n",
      "inserted data for ALLE\n",
      "inserted data for ALLY\n",
      "inserted data for ALNY\n",
      "inserted data for ALRM\n",
      "inserted data for ALSN\n",
      "inserted data for ALTR\n",
      "inserted data for ALV\n",
      "inserted data for AM\n",
      "inserted data for AMAT\n",
      "inserted data for AMCR\n",
      "inserted data for AMD\n",
      "inserted data for AME\n",
      "inserted data for AMG\n",
      "inserted data for AMGN\n",
      "inserted data for AMH\n",
      "inserted data for AMKR\n",
      "inserted data for AMLP\n",
      "inserted data for AMP\n",
      "inserted data for AMR\n",
      "inserted data for AMT\n",
      "inserted data for AMZN\n",
      "inserted data for AN\n",
      "inserted data for ANET\n",
      "inserted data for ANF\n",
      "inserted data for ANSS\n",
      "inserted data for AON\n",
      "inserted data for AOS\n",
      "inserted data for APA\n",
      "inserted data for APD\n",
      "inserted data for APG\n",
      "inserted data for APH\n",
      "inserted data for APLE\n",
      "inserted data for APLS\n",
      "inserted data for APO\n",
      "inserted data for APP\n",
      "inserted data for APPF\n",
      "inserted data for APTV\n",
      "inserted data for AR\n",
      "inserted data for ARCC\n",
      "inserted data for ARE\n",
      "inserted data for ARMK\n",
      "inserted data for ARW\n",
      "inserted data for ARWR\n",
      "inserted data for AS\n",
      "inserted data for ASAN\n",
      "inserted data for ASGN\n",
      "inserted data for ASH\n",
      "inserted data for ASML\n",
      "inserted data for ASO\n",
      "inserted data for ASTS\n",
      "inserted data for ATI\n",
      "inserted data for ATKR\n",
      "inserted data for ATO\n",
      "inserted data for ATR\n",
      "inserted data for AUR\n",
      "inserted data for AVAV\n",
      "inserted data for AVB\n",
      "inserted data for AVGO\n",
      "inserted data for AVNT\n",
      "inserted data for AVT\n",
      "inserted data for AVTR\n",
      "inserted data for AVY\n",
      "inserted data for AWI\n",
      "inserted data for AWK\n",
      "inserted data for AX\n",
      "inserted data for AXNX\n",
      "inserted data for AXON\n",
      "inserted data for AXP\n",
      "inserted data for AXS\n",
      "inserted data for AXSM\n",
      "inserted data for AXTA\n",
      "inserted data for AYI\n",
      "inserted data for AZEK\n",
      "inserted data for AZO\n",
      "inserted data for AZPN\n",
      "inserted data for AZTA\n",
      "inserted data for BA\n",
      "inserted data for BAC\n",
      "inserted data for BAH\n",
      "inserted data for BALL\n",
      "inserted data for BAP\n",
      "inserted data for BAX\n",
      "inserted data for BBIO\n",
      "inserted data for BBWI\n",
      "inserted data for BBY\n",
      "inserted data for BC\n",
      "inserted data for BCC\n",
      "inserted data for BCIM\n",
      "inserted data for BCO\n",
      "inserted data for BCPC\n",
      "inserted data for BDC\n",
      "inserted data for BDX\n",
      "inserted data for BECN\n",
      "inserted data for BEN\n",
      "inserted data for BERY\n",
      "inserted data for BFAM\n",
      "inserted data for BG\n",
      "inserted data for BGC\n",
      "inserted data for BHVN\n",
      "inserted data for BIIB\n",
      "inserted data for BIL\n",
      "inserted data for BILL\n",
      "inserted data for BIO\n",
      "inserted data for BIPC\n",
      "inserted data for BIRK\n",
      "inserted data for BITO\n",
      "inserted data for BJ\n",
      "inserted data for BK\n",
      "inserted data for BKH\n",
      "inserted data for BKNG\n",
      "inserted data for BKR\n",
      "inserted data for BL\n",
      "inserted data for BLD\n",
      "inserted data for BLDR\n",
      "inserted data for BLK\n",
      "inserted data for BLKB\n",
      "inserted data for BMI\n",
      "inserted data for BMRN\n",
      "inserted data for BMY\n",
      "inserted data for BOKF\n",
      "inserted data for BOOT\n",
      "inserted data for BOX\n",
      "inserted data for BPMC\n",
      "inserted data for BPOP\n",
      "inserted data for BR\n",
      "inserted data for BRBR\n",
      "inserted data for BRKR\n",
      "inserted data for BRO\n",
      "inserted data for BRX\n",
      "inserted data for BRZE\n",
      "inserted data for BSX\n",
      "inserted data for BSY\n",
      "inserted data for BURL\n",
      "inserted data for BWA\n",
      "inserted data for BWXT\n",
      "inserted data for BX\n",
      "inserted data for BXMT\n",
      "inserted data for BXP\n",
      "inserted data for BYD\n",
      "inserted data for C\n",
      "inserted data for CACC\n",
      "inserted data for CACI\n",
      "inserted data for CADE\n",
      "inserted data for CAG\n",
      "inserted data for CAH\n",
      "inserted data for CAMT\n",
      "inserted data for CAR\n",
      "inserted data for CARR\n",
      "inserted data for CART\n",
      "inserted data for CASY\n",
      "inserted data for CAT\n",
      "inserted data for CAVA\n",
      "inserted data for CB\n",
      "inserted data for CBRE\n",
      "inserted data for CBSH\n",
      "inserted data for CBT\n",
      "inserted data for CBZ\n",
      "inserted data for CCCS\n",
      "inserted data for CCEP\n",
      "inserted data for CCI\n",
      "inserted data for CCK\n",
      "inserted data for CCL\n",
      "inserted data for CCOI\n",
      "inserted data for CDNS\n",
      "inserted data for CDW\n",
      "inserted data for CE\n",
      "inserted data for CEG\n",
      "inserted data for CELH\n",
      "inserted data for CERE\n",
      "inserted data for CF\n",
      "inserted data for CFG\n",
      "inserted data for CFLT\n",
      "inserted data for CFR\n",
      "inserted data for CG\n",
      "inserted data for CGNX\n",
      "inserted data for CHD\n",
      "inserted data for CHDN\n",
      "inserted data for CHE\n",
      "inserted data for CHH\n",
      "inserted data for CHK\n",
      "inserted data for CHKP\n",
      "inserted data for CHRD\n",
      "inserted data for CHRW\n",
      "inserted data for CHTR\n",
      "inserted data for CHWY\n",
      "inserted data for CHX\n",
      "inserted data for CI\n",
      "inserted data for CIEN\n",
      "inserted data for CINF\n",
      "inserted data for CIVI\n",
      "inserted data for CL\n",
      "inserted data for CLF\n",
      "inserted data for CLH\n",
      "inserted data for CLVT\n",
      "inserted data for CLX\n",
      "inserted data for CMA\n",
      "inserted data for CMC\n",
      "inserted data for CMCSA\n",
      "inserted data for CME\n",
      "inserted data for CMG\n",
      "inserted data for CMI\n",
      "inserted data for CMS\n",
      "inserted data for CNA\n",
      "inserted data for CNC\n",
      "inserted data for CNH\n",
      "inserted data for CNM\n",
      "inserted data for CNP\n",
      "inserted data for CNS\n",
      "inserted data for CNX\n",
      "inserted data for CNXC\n",
      "inserted data for COF\n",
      "inserted data for COHR\n",
      "inserted data for COIN\n",
      "inserted data for COKE\n",
      "inserted data for COLB\n",
      "inserted data for COLD\n",
      "inserted data for COLM\n",
      "inserted data for COO\n",
      "inserted data for COOP\n",
      "inserted data for COP\n",
      "inserted data for COR\n",
      "inserted data for COST\n",
      "inserted data for COTY\n",
      "inserted data for CPA\n",
      "inserted data for CPAY\n",
      "inserted data for CPB\n",
      "inserted data for CPNG\n",
      "inserted data for CPRI\n",
      "inserted data for CPRT\n",
      "inserted data for CPT\n",
      "inserted data for CR\n",
      "inserted data for CRBG\n",
      "inserted data for CRC\n",
      "inserted data for CRDO\n",
      "inserted data for CRH\n",
      "inserted data for CRL\n",
      "inserted data for CRM\n",
      "inserted data for CRNX\n",
      "inserted data for CROX\n",
      "inserted data for CRS\n",
      "inserted data for CRSP\n",
      "inserted data for CRUS\n",
      "inserted data for CRVL\n",
      "inserted data for CRWD\n",
      "inserted data for CSCO\n",
      "inserted data for CSGP\n",
      "inserted data for CSL\n",
      "inserted data for CSWI\n",
      "inserted data for CSX\n",
      "inserted data for CTAS\n",
      "inserted data for CTLT\n",
      "inserted data for CTRA\n",
      "inserted data for CTRE\n",
      "inserted data for CTSH\n",
      "inserted data for CTVA\n",
      "inserted data for CUBE\n",
      "inserted data for CUZ\n",
      "inserted data for CVLT\n",
      "inserted data for CVNA\n",
      "inserted data for CVS\n",
      "inserted data for CVX\n",
      "inserted data for CW\n",
      "inserted data for CWAN\n",
      "inserted data for CWST\n",
      "inserted data for CXM\n",
      "inserted data for CXT\n",
      "inserted data for CYBR\n",
      "inserted data for CYTK\n",
      "inserted data for CZR\n",
      "inserted data for D\n",
      "inserted data for DAL\n",
      "inserted data for DAR\n",
      "inserted data for DASH\n",
      "inserted data for DAY\n",
      "inserted data for DBMF\n",
      "inserted data for DBX\n",
      "inserted data for DCI\n",
      "inserted data for DD\n",
      "inserted data for DDOG\n",
      "inserted data for DDS\n",
      "inserted data for DE\n",
      "inserted data for DECK\n",
      "inserted data for DELL\n",
      "inserted data for DFH\n",
      "inserted data for DFS\n",
      "inserted data for DG\n",
      "inserted data for DGX\n",
      "inserted data for DHI\n",
      "inserted data for DHR\n",
      "inserted data for DINO\n",
      "inserted data for DIS\n",
      "inserted data for DJT\n",
      "inserted data for DKNG\n",
      "inserted data for DKS\n",
      "inserted data for DLB\n",
      "inserted data for DLO\n",
      "inserted data for DLR\n",
      "inserted data for DLTR\n",
      "inserted data for DNB\n",
      "inserted data for DOC\n",
      "inserted data for DOCN\n",
      "inserted data for DOCS\n",
      "inserted data for DOCU\n",
      "inserted data for DOV\n",
      "inserted data for DOW\n",
      "inserted data for DOX\n",
      "inserted data for DPZ\n",
      "inserted data for DRI\n",
      "inserted data for DRS\n",
      "inserted data for DT\n",
      "inserted data for DTCR\n",
      "inserted data for DTE\n",
      "inserted data for DTM\n",
      "inserted data for DUK\n",
      "inserted data for DUOL\n",
      "inserted data for DV\n",
      "inserted data for DVA\n",
      "inserted data for DVN\n",
      "inserted data for DXC\n",
      "inserted data for DXCM\n",
      "inserted data for DY\n",
      "inserted data for DYN\n",
      "inserted data for EA\n",
      "inserted data for EBAY\n",
      "inserted data for ECL\n",
      "inserted data for ED\n",
      "inserted data for EEFT\n",
      "inserted data for EFX\n",
      "inserted data for EG\n",
      "inserted data for EGP\n",
      "inserted data for EHC\n",
      "inserted data for EIX\n",
      "inserted data for EL\n",
      "inserted data for ELAN\n",
      "inserted data for ELF\n",
      "inserted data for ELS\n",
      "inserted data for ELV\n",
      "inserted data for EME\n",
      "inserted data for EMN\n",
      "inserted data for EMR\n",
      "inserted data for ENLC\n",
      "inserted data for ENPH\n",
      "inserted data for ENS\n",
      "inserted data for ENSG\n",
      "inserted data for ENTG\n",
      "inserted data for EOG\n",
      "inserted data for EPAM\n",
      "inserted data for EPRT\n",
      "inserted data for EQH\n",
      "inserted data for EQIX\n",
      "inserted data for EQR\n",
      "inserted data for EQT\n",
      "inserted data for ERIE\n",
      "inserted data for ES\n",
      "inserted data for ESAB\n",
      "inserted data for ESGR\n",
      "inserted data for ESI\n",
      "inserted data for ESLT\n",
      "inserted data for ESNT\n",
      "inserted data for ESS\n",
      "inserted data for ESTC\n",
      "inserted data for ETN\n",
      "inserted data for ETR\n",
      "inserted data for ETSY\n",
      "inserted data for EVH\n",
      "inserted data for EVR\n",
      "inserted data for EVRG\n",
      "inserted data for EW\n",
      "inserted data for EWBC\n",
      "inserted data for EXAS\n",
      "inserted data for EXC\n",
      "inserted data for EXE\n",
      "inserted data for EXEL\n",
      "inserted data for EXLS\n",
      "inserted data for EXP\n",
      "inserted data for EXPD\n",
      "inserted data for EXPE\n",
      "inserted data for EXPO\n",
      "inserted data for EXR\n",
      "inserted data for F\n",
      "inserted data for FAF\n",
      "inserted data for FANG\n",
      "inserted data for FAST\n",
      "inserted data for FBIN\n",
      "inserted data for FCFS\n",
      "inserted data for FCN\n",
      "inserted data for FCNCA\n",
      "inserted data for FCX\n",
      "inserted data for FDS\n",
      "inserted data for FDX\n",
      "inserted data for FE\n",
      "inserted data for FELE\n",
      "inserted data for FERG\n",
      "inserted data for FFIN\n",
      "inserted data for FFIV\n",
      "inserted data for FG\n",
      "inserted data for FHN\n",
      "inserted data for FI\n",
      "inserted data for FICO\n",
      "inserted data for FIS\n",
      "inserted data for FITB\n",
      "inserted data for FIVE\n",
      "inserted data for FIVN\n",
      "inserted data for FIX\n",
      "inserted data for FIZZ\n",
      "inserted data for FLEX\n",
      "inserted data for FLG\n",
      "inserted data for FLO\n",
      "inserted data for FLR\n",
      "inserted data for FLS\n",
      "inserted data for FLYW\n",
      "inserted data for FMC\n",
      "inserted data for FN\n",
      "inserted data for FNB\n",
      "inserted data for FND\n",
      "inserted data for FNF\n",
      "inserted data for FOLD\n",
      "inserted data for FORM\n",
      "inserted data for FOUR\n",
      "inserted data for FOX\n",
      "inserted data for FOXA\n",
      "inserted data for FR\n",
      "inserted data for FRHC\n",
      "inserted data for FRO\n",
      "inserted data for FROG\n",
      "inserted data for FRPT\n",
      "inserted data for FRSH\n",
      "inserted data for FRT\n",
      "inserted data for FSK\n",
      "inserted data for FSLR\n",
      "inserted data for FSS\n",
      "inserted data for FTAI\n",
      "inserted data for FTI\n",
      "inserted data for FTLS\n",
      "inserted data for FTNT\n",
      "inserted data for FTV\n",
      "inserted data for FUL\n",
      "inserted data for FUN\n",
      "inserted data for FWONA\n",
      "inserted data for FWONK\n",
      "inserted data for FXF\n",
      "inserted data for FXY\n",
      "inserted data for FYBR\n",
      "inserted data for G\n",
      "inserted data for GAP\n",
      "inserted data for GATX\n",
      "inserted data for GBCI\n",
      "inserted data for GBDC\n",
      "inserted data for GD\n",
      "inserted data for GDDY\n",
      "inserted data for GE\n",
      "inserted data for GEN\n",
      "inserted data for GEV\n",
      "inserted data for GFF\n",
      "inserted data for GFS\n",
      "inserted data for GGG\n",
      "inserted data for GILD\n",
      "inserted data for GIS\n",
      "inserted data for GKOS\n",
      "inserted data for GL\n",
      "inserted data for GLBE\n",
      "inserted data for GLD\n",
      "inserted data for GLOB\n",
      "inserted data for GLPI\n",
      "inserted data for GLW\n",
      "inserted data for GM\n",
      "inserted data for GME\n",
      "inserted data for GMED\n",
      "inserted data for GMS\n",
      "inserted data for GNRC\n",
      "inserted data for GNTX\n",
      "inserted data for GOLF\n",
      "inserted data for GOOG\n",
      "inserted data for GOOGL\n",
      "inserted data for GPC\n",
      "inserted data for GPI\n",
      "inserted data for GPK\n",
      "inserted data for GPN\n",
      "inserted data for GPS\n",
      "inserted data for GRAB\n",
      "inserted data for GRMN\n",
      "inserted data for GS\n",
      "inserted data for GT\n",
      "inserted data for GTES\n",
      "inserted data for GTLB\n",
      "inserted data for GTLS\n",
      "inserted data for GWRE\n",
      "inserted data for GWW\n",
      "inserted data for GXO\n",
      "inserted data for H\n",
      "inserted data for HAE\n",
      "inserted data for HAL\n",
      "inserted data for HALO\n",
      "inserted data for HAS\n",
      "inserted data for HASI\n",
      "inserted data for HBAN\n",
      "inserted data for HCA\n",
      "inserted data for HD\n",
      "inserted data for HEI\n",
      "inserted data for HES\n",
      "inserted data for HGV\n",
      "inserted data for HHH\n",
      "inserted data for HIG\n",
      "inserted data for HII\n",
      "inserted data for HLI\n",
      "inserted data for HLNE\n",
      "inserted data for HLT\n",
      "inserted data for HOG\n",
      "inserted data for HOLX\n",
      "inserted data for HOMB\n",
      "inserted data for HON\n",
      "inserted data for HOOD\n",
      "inserted data for HP\n",
      "inserted data for HPE\n",
      "inserted data for HPQ\n",
      "inserted data for HQY\n",
      "inserted data for HR\n",
      "inserted data for HRB\n",
      "inserted data for HRI\n",
      "inserted data for HRL\n",
      "inserted data for HSIC\n",
      "inserted data for HST\n",
      "inserted data for HSY\n",
      "inserted data for HUBB\n",
      "inserted data for HUBS\n",
      "inserted data for HUM\n",
      "inserted data for HUN\n",
      "inserted data for HWC\n",
      "inserted data for HWM\n",
      "inserted data for HXL\n",
      "inserted data for IAC\n",
      "inserted data for IBKR\n",
      "inserted data for IBM\n",
      "inserted data for IBOC\n",
      "inserted data for IBP\n",
      "inserted data for ICE\n",
      "inserted data for ICL\n",
      "inserted data for ICLN\n",
      "inserted data for ICLR\n",
      "inserted data for ICUI\n",
      "inserted data for IDA\n",
      "inserted data for IDXX\n",
      "inserted data for IEX\n",
      "inserted data for IFF\n",
      "inserted data for IGT\n",
      "inserted data for ILMN\n",
      "inserted data for IMVT\n",
      "inserted data for INCY\n",
      "inserted data for INFA\n",
      "inserted data for INGR\n",
      "inserted data for INSM\n",
      "inserted data for INSP\n",
      "inserted data for INTC\n",
      "inserted data for INTU\n",
      "inserted data for INVH\n",
      "inserted data for IONS\n",
      "inserted data for IOT\n",
      "inserted data for IOVA\n",
      "inserted data for IP\n",
      "inserted data for IPAR\n",
      "inserted data for IPG\n",
      "inserted data for IPGP\n",
      "inserted data for IQV\n",
      "inserted data for IR\n",
      "inserted data for IRDM\n",
      "inserted data for IRM\n",
      "inserted data for IRT\n",
      "inserted data for IRTC\n",
      "inserted data for ISRG\n",
      "inserted data for IT\n",
      "inserted data for ITCI\n",
      "inserted data for ITGR\n",
      "inserted data for ITRI\n",
      "inserted data for ITT\n",
      "inserted data for ITW\n",
      "inserted data for IVZ\n",
      "inserted data for J\n",
      "inserted data for JAZZ\n",
      "inserted data for JBHT\n",
      "inserted data for JBL\n",
      "inserted data for JCI\n",
      "inserted data for JEF\n",
      "inserted data for JHG\n",
      "inserted data for JKHY\n",
      "inserted data for JLL\n",
      "inserted data for JNJ\n",
      "inserted data for JNPR\n",
      "inserted data for JOBY\n",
      "inserted data for JPM\n",
      "inserted data for JWN\n",
      "inserted data for JXN\n",
      "inserted data for K\n",
      "inserted data for KAI\n",
      "inserted data for KBH\n",
      "inserted data for KBR\n",
      "inserted data for KD\n",
      "inserted data for KDP\n",
      "inserted data for KEX\n",
      "inserted data for KEY\n",
      "inserted data for KEYS\n",
      "inserted data for KFY\n",
      "inserted data for KGC\n",
      "inserted data for KHC\n",
      "inserted data for KIM\n",
      "inserted data for KKR\n",
      "inserted data for KLAC\n",
      "inserted data for KMB\n",
      "inserted data for KMI\n",
      "inserted data for KMPR\n",
      "inserted data for KMX\n",
      "inserted data for KNF\n",
      "inserted data for KNSL\n",
      "inserted data for KNX\n",
      "inserted data for KO\n",
      "inserted data for KR\n",
      "inserted data for KRC\n",
      "inserted data for KRG\n",
      "inserted data for KRYS\n",
      "inserted data for KTB\n",
      "inserted data for KVUE\n",
      "inserted data for KVYO\n",
      "inserted data for KWR\n",
      "inserted data for L\n",
      "inserted data for LAD\n",
      "inserted data for LAMR\n",
      "inserted data for LANC\n",
      "inserted data for LAND\n",
      "inserted data for LAZ\n",
      "inserted data for LBRDA\n",
      "inserted data for LBRDK\n",
      "inserted data for LBRT\n",
      "inserted data for LBTYA\n",
      "inserted data for LBTYK\n",
      "inserted data for LCID\n",
      "inserted data for LDOS\n",
      "inserted data for LEA\n",
      "inserted data for LECO\n",
      "inserted data for LEN\n",
      "inserted data for LEVI\n",
      "inserted data for LFUS\n",
      "inserted data for LH\n",
      "inserted data for LHX\n",
      "inserted data for LII\n",
      "inserted data for LIN\n",
      "inserted data for LITE\n",
      "inserted data for LKQ\n",
      "inserted data for LLY\n",
      "inserted data for LLYVA\n",
      "inserted data for LLYVK\n",
      "inserted data for LMT\n",
      "inserted data for LNC\n",
      "inserted data for LNG\n",
      "inserted data for LNT\n",
      "inserted data for LNTH\n",
      "inserted data for LNW\n",
      "inserted data for LOAR\n",
      "inserted data for LOGI\n",
      "inserted data for LOPE\n",
      "inserted data for LOW\n",
      "inserted data for LPLA\n",
      "inserted data for LPX\n",
      "inserted data for LRCX\n",
      "inserted data for LSCC\n",
      "inserted data for LSTR\n",
      "inserted data for LSXMA\n",
      "inserted data for LSXMK\n",
      "inserted data for LTH\n",
      "inserted data for LULU\n",
      "inserted data for LUMN\n",
      "inserted data for LUV\n",
      "inserted data for LVS\n",
      "inserted data for LW\n",
      "inserted data for LYB\n",
      "inserted data for LYFT\n",
      "inserted data for LYV\n",
      "inserted data for M\n",
      "inserted data for MA\n",
      "inserted data for MAA\n",
      "inserted data for MAC\n",
      "inserted data for MAIN\n",
      "inserted data for MAN\n",
      "inserted data for MANH\n",
      "inserted data for MAR\n",
      "inserted data for MARA\n",
      "inserted data for MAS\n",
      "inserted data for MASI\n",
      "inserted data for MAT\n",
      "inserted data for MATX\n",
      "inserted data for MBLY\n",
      "inserted data for MC\n",
      "inserted data for MCD\n",
      "inserted data for MCHP\n",
      "inserted data for MCK\n",
      "inserted data for MCO\n",
      "inserted data for MDB\n",
      "inserted data for MDGL\n",
      "inserted data for MDLZ\n",
      "inserted data for MDT\n",
      "inserted data for MDU\n",
      "inserted data for MEDP\n",
      "inserted data for MELI\n",
      "inserted data for MET\n",
      "inserted data for META\n",
      "inserted data for MGM\n",
      "inserted data for MGY\n",
      "inserted data for MHK\n",
      "inserted data for MHO\n",
      "inserted data for MIDD\n",
      "inserted data for MKC\n",
      "inserted data for MKL\n",
      "inserted data for MKSI\n",
      "inserted data for MKTX\n",
      "inserted data for MLI\n",
      "inserted data for MLM\n",
      "inserted data for MMC\n",
      "inserted data for MMM\n",
      "inserted data for MMS\n",
      "inserted data for MMSI\n",
      "inserted data for MMYT\n",
      "inserted data for MNA\n",
      "inserted data for MNDY\n",
      "inserted data for MNST\n",
      "inserted data for MO\n",
      "inserted data for MOD\n",
      "inserted data for MOH\n",
      "inserted data for MORN\n",
      "inserted data for MOS\n",
      "inserted data for MPC\n",
      "inserted data for MPWR\n",
      "inserted data for MQ\n",
      "inserted data for MRK\n",
      "inserted data for MRNA\n",
      "inserted data for MRO\n",
      "inserted data for MRVL\n",
      "inserted data for MS\n",
      "inserted data for MSA\n",
      "inserted data for MSCI\n",
      "inserted data for MSFT\n",
      "inserted data for MSGS\n",
      "inserted data for MSI\n",
      "inserted data for MSM\n",
      "inserted data for MSTR\n",
      "inserted data for MTB\n",
      "inserted data for MTCH\n",
      "inserted data for MTD\n",
      "inserted data for MTDR\n",
      "inserted data for MTG\n",
      "inserted data for MTH\n",
      "inserted data for MTN\n",
      "inserted data for MTSI\n",
      "inserted data for MTZ\n",
      "inserted data for MU\n",
      "inserted data for MUR\n",
      "inserted data for MUSA\n",
      "inserted data for NBIX\n",
      "inserted data for NCLH\n",
      "inserted data for NCNO\n",
      "inserted data for NDAQ\n",
      "inserted data for NDSN\n",
      "inserted data for NE\n",
      "inserted data for NEE\n",
      "inserted data for NEM\n",
      "inserted data for NEOG\n",
      "inserted data for NET\n",
      "inserted data for NEU\n",
      "inserted data for NFE\n",
      "inserted data for NFG\n",
      "inserted data for NFLX\n",
      "inserted data for NI\n",
      "inserted data for NJR\n",
      "inserted data for NKE\n",
      "inserted data for NLY\n",
      "inserted data for NNI\n",
      "inserted data for NNN\n",
      "inserted data for NOC\n",
      "inserted data for NOG\n",
      "inserted data for NOV\n",
      "inserted data for NOVT\n",
      "inserted data for NOW\n",
      "inserted data for NRG\n",
      "inserted data for NSC\n",
      "inserted data for NSIT\n",
      "inserted data for NSP\n",
      "inserted data for NTAP\n",
      "inserted data for NTNX\n",
      "inserted data for NTRA\n",
      "inserted data for NTRS\n",
      "inserted data for NU\n",
      "inserted data for NUE\n",
      "inserted data for NUVL\n",
      "inserted data for NVDA\n",
      "inserted data for NVMI\n",
      "inserted data for NVR\n",
      "inserted data for NVST\n",
      "inserted data for NVT\n",
      "inserted data for NWS\n",
      "inserted data for NWSA\n",
      "inserted data for NXE\n",
      "inserted data for NXPI\n",
      "inserted data for NXST\n",
      "inserted data for NYCB\n",
      "inserted data for NYT\n",
      "inserted data for O\n",
      "inserted data for OBDC\n",
      "inserted data for OC\n",
      "inserted data for ODFL\n",
      "inserted data for OGE\n",
      "inserted data for OGN\n",
      "inserted data for OGS\n",
      "inserted data for OHI\n",
      "inserted data for OILK\n",
      "inserted data for OKE\n",
      "inserted data for OKTA\n",
      "inserted data for OLED\n",
      "inserted data for OLLI\n",
      "inserted data for OLN\n",
      "inserted data for OMC\n",
      "inserted data for OMF\n",
      "inserted data for ON\n",
      "inserted data for ONB\n",
      "inserted data for ONON\n",
      "inserted data for ONTO\n",
      "inserted data for OPCH\n",
      "inserted data for ORA\n",
      "inserted data for ORCL\n",
      "inserted data for ORI\n",
      "inserted data for ORLY\n",
      "inserted data for OSCR\n",
      "inserted data for OSK\n",
      "inserted data for OTEX\n",
      "inserted data for OTIS\n",
      "inserted data for OTTR\n",
      "inserted data for OVV\n",
      "inserted data for OWL\n",
      "inserted data for OXY\n",
      "inserted data for OZK\n",
      "inserted data for PAG\n",
      "inserted data for PAGP\n",
      "inserted data for PAGS\n",
      "inserted data for PANW\n",
      "inserted data for PARA\n",
      "inserted data for PATH\n",
      "inserted data for PAVE\n",
      "inserted data for PAYC\n",
      "inserted data for PAYX\n",
      "inserted data for PB\n",
      "inserted data for PBDC\n",
      "inserted data for PBF\n",
      "inserted data for PBH\n",
      "inserted data for PCAR\n",
      "inserted data for PCG\n",
      "inserted data for PCH\n",
      "inserted data for PCOR\n",
      "inserted data for PCTY\n",
      "inserted data for PCVX\n",
      "inserted data for PDBC\n",
      "inserted data for PECO\n",
      "inserted data for PEG\n",
      "inserted data for PEGA\n",
      "inserted data for PEN\n",
      "inserted data for PEP\n",
      "inserted data for PFE\n",
      "inserted data for PFG\n",
      "inserted data for PFGC\n",
      "inserted data for PFIX\n",
      "inserted data for PFSI\n",
      "inserted data for PG\n",
      "inserted data for PGNY\n",
      "inserted data for PGR\n",
      "inserted data for PH\n",
      "inserted data for PHM\n",
      "inserted data for PI\n",
      "inserted data for PII\n",
      "inserted data for PINS\n",
      "inserted data for PIPR\n",
      "inserted data for PK\n",
      "inserted data for PKG\n",
      "inserted data for PLD\n",
      "inserted data for PLNT\n",
      "inserted data for PLTR\n",
      "inserted data for PM\n",
      "inserted data for PNC\n",
      "inserted data for PNFP\n",
      "inserted data for PNR\n",
      "inserted data for PNW\n",
      "inserted data for PODD\n",
      "inserted data for POOL\n",
      "inserted data for POR\n",
      "inserted data for POST\n",
      "inserted data for POWI\n",
      "inserted data for PPC\n",
      "inserted data for PPG\n",
      "inserted data for PPL\n",
      "inserted data for PR\n",
      "inserted data for PRCT\n",
      "inserted data for PRGO\n",
      "inserted data for PRI\n",
      "inserted data for PRU\n",
      "inserted data for PSA\n",
      "inserted data for PSN\n",
      "inserted data for PSTG\n",
      "inserted data for PSX\n",
      "inserted data for PTC\n",
      "inserted data for PTEN\n",
      "inserted data for PVH\n",
      "inserted data for PWR\n",
      "inserted data for PWSC\n",
      "inserted data for PYCR\n",
      "inserted data for PYPL\n",
      "inserted data for QCOM\n",
      "inserted data for QLYS\n",
      "inserted data for QRVO\n",
      "inserted data for QSR\n",
      "inserted data for QTWO\n",
      "inserted data for QXO\n",
      "inserted data for QYLD\n",
      "inserted data for R\n",
      "inserted data for RACE\n",
      "inserted data for RARE\n",
      "inserted data for RBC\n",
      "inserted data for RBLX\n",
      "inserted data for RCL\n",
      "inserted data for RCM\n",
      "inserted data for RDDT\n",
      "inserted data for RDN\n",
      "inserted data for RDNT\n",
      "inserted data for REG\n",
      "inserted data for REGN\n",
      "inserted data for RELY\n",
      "inserted data for REXR\n",
      "inserted data for REYN\n",
      "inserted data for REZ\n",
      "inserted data for RF\n",
      "inserted data for RGA\n",
      "inserted data for RGEN\n",
      "inserted data for RGLD\n",
      "inserted data for RH\n",
      "inserted data for RHI\n",
      "inserted data for RHP\n",
      "inserted data for RIG\n",
      "inserted data for RIOT\n",
      "inserted data for RITM\n",
      "inserted data for RIVN\n",
      "inserted data for RJF\n",
      "inserted data for RL\n",
      "inserted data for RLI\n",
      "inserted data for RMBS\n",
      "inserted data for RMD\n",
      "inserted data for RNA\n",
      "inserted data for RNR\n",
      "inserted data for ROIV\n",
      "inserted data for ROK\n",
      "inserted data for ROKU\n",
      "inserted data for ROL\n",
      "inserted data for ROP\n",
      "inserted data for ROST\n",
      "inserted data for RPD\n",
      "inserted data for RPM\n",
      "inserted data for RPRX\n",
      "inserted data for RRC\n",
      "inserted data for RRR\n",
      "inserted data for RRX\n",
      "inserted data for RS\n",
      "inserted data for RSG\n",
      "inserted data for RTX\n",
      "inserted data for RUN\n",
      "inserted data for RUSHA\n",
      "inserted data for RVMD\n",
      "inserted data for RVTY\n",
      "inserted data for RYAN\n",
      "inserted data for RYN\n",
      "inserted data for S\n",
      "inserted data for SAIA\n",
      "inserted data for SAIC\n",
      "inserted data for SAM\n",
      "inserted data for SANM\n",
      "inserted data for SATS\n",
      "inserted data for SBAC\n",
      "inserted data for SBRA\n",
      "inserted data for SBUX\n",
      "inserted data for SCCO\n",
      "inserted data for SCHW\n",
      "inserted data for SCI\n",
      "inserted data for SEDG\n",
      "inserted data for SEE\n",
      "inserted data for SEIC\n",
      "inserted data for SEM\n",
      "inserted data for SF\n",
      "inserted data for SFBS\n",
      "inserted data for SFM\n",
      "inserted data for SGRY\n",
      "inserted data for SH\n",
      "inserted data for SHAK\n",
      "inserted data for SHC\n",
      "inserted data for SHW\n",
      "inserted data for SIG\n",
      "inserted data for SIGI\n",
      "inserted data for SIRI\n",
      "inserted data for SITE\n",
      "inserted data for SJM\n",
      "inserted data for SKX\n",
      "inserted data for SKY\n",
      "inserted data for SLAB\n",
      "inserted data for SLB\n",
      "inserted data for SLG\n",
      "inserted data for SLGN\n",
      "inserted data for SLM\n",
      "inserted data for SLV\n",
      "inserted data for SM\n",
      "inserted data for SMAR\n",
      "inserted data for SMCI\n",
      "inserted data for SMG\n",
      "inserted data for SMPL\n",
      "inserted data for SN\n",
      "inserted data for SNA\n",
      "inserted data for SNAP\n",
      "inserted data for SNDR\n",
      "inserted data for SNOW\n",
      "inserted data for SNPS\n",
      "inserted data for SNV\n",
      "inserted data for SNX\n",
      "inserted data for SO\n",
      "inserted data for SOFI\n",
      "inserted data for SOLV\n",
      "inserted data for SON\n",
      "inserted data for SPG\n",
      "inserted data for SPGI\n",
      "inserted data for SPOT\n",
      "inserted data for SPR\n",
      "inserted data for SPSC\n",
      "inserted data for SPT\n",
      "inserted data for SPXC\n",
      "inserted data for SQ\n",
      "inserted data for SQSP\n",
      "inserted data for SR\n",
      "inserted data for SRCL\n",
      "inserted data for SRE\n",
      "inserted data for SRPT\n",
      "inserted data for SSB\n",
      "inserted data for SSD\n",
      "inserted data for SSNC\n",
      "inserted data for ST\n",
      "inserted data for STAG\n",
      "inserted data for STE\n",
      "inserted data for STLA\n",
      "inserted data for STLD\n",
      "inserted data for STNE\n",
      "inserted data for STNG\n",
      "inserted data for STT\n",
      "inserted data for STVN\n",
      "inserted data for STWD\n",
      "inserted data for STX\n",
      "inserted data for STZ\n",
      "inserted data for SUI\n",
      "inserted data for SUM\n",
      "inserted data for SWK\n",
      "inserted data for SWKS\n",
      "inserted data for SWN\n",
      "inserted data for SWTX\n",
      "inserted data for SWX\n",
      "inserted data for SYF\n",
      "inserted data for SYK\n",
      "inserted data for SYM\n",
      "inserted data for SYNA\n",
      "inserted data for SYY\n",
      "inserted data for T\n",
      "inserted data for TAP\n",
      "inserted data for TDC\n",
      "inserted data for TDG\n",
      "inserted data for TDW\n",
      "inserted data for TDY\n",
      "inserted data for TEAM\n",
      "inserted data for TECH\n",
      "inserted data for TEL\n",
      "inserted data for TEM\n",
      "inserted data for TENB\n",
      "inserted data for TER\n",
      "inserted data for TEX\n",
      "inserted data for TFC\n",
      "inserted data for TFSL\n",
      "inserted data for TFX\n",
      "inserted data for TGT\n",
      "inserted data for THC\n",
      "inserted data for THG\n",
      "inserted data for THO\n",
      "inserted data for TJX\n",
      "inserted data for TKO\n",
      "inserted data for TKR\n",
      "inserted data for TMDX\n",
      "inserted data for TMHC\n",
      "inserted data for TMO\n",
      "inserted data for TMUS\n",
      "inserted data for TNET\n",
      "inserted data for TOL\n",
      "inserted data for TOST\n",
      "inserted data for TPG\n",
      "inserted data for TPH\n",
      "inserted data for TPL\n",
      "inserted data for TPR\n",
      "inserted data for TPX\n",
      "inserted data for TREX\n",
      "inserted data for TRGP\n",
      "inserted data for TRI\n",
      "inserted data for TRIP\n",
      "inserted data for TRMB\n",
      "inserted data for TRNO\n",
      "inserted data for TROW\n",
      "inserted data for TRU\n",
      "inserted data for TRV\n",
      "inserted data for TSCO\n",
      "inserted data for TSEM\n",
      "inserted data for TSLA\n",
      "inserted data for TSN\n",
      "inserted data for TT\n",
      "inserted data for TTC\n",
      "inserted data for TTD\n",
      "inserted data for TTEK\n",
      "inserted data for TTWO\n",
      "inserted data for TW\n",
      "inserted data for TWLO\n",
      "inserted data for TXG\n",
      "inserted data for TXN\n",
      "inserted data for TXRH\n",
      "inserted data for TXT\n",
      "inserted data for TYL\n",
      "inserted data for U\n",
      "inserted data for UA\n",
      "inserted data for UAA\n",
      "inserted data for UAL\n",
      "inserted data for UBER\n",
      "inserted data for UBS\n",
      "inserted data for UBSI\n",
      "inserted data for UDR\n",
      "inserted data for UFPI\n",
      "inserted data for UGI\n",
      "inserted data for UHAL\n",
      "inserted data for UHS\n",
      "inserted data for UI\n",
      "inserted data for ULS\n",
      "inserted data for ULTA\n",
      "inserted data for UMBF\n",
      "inserted data for UNH\n",
      "inserted data for UNM\n",
      "inserted data for UNP\n",
      "inserted data for UPS\n",
      "inserted data for UPST\n",
      "inserted data for URBN\n",
      "inserted data for URI\n",
      "inserted data for USB\n",
      "inserted data for USFD\n",
      "inserted data for USM\n",
      "inserted data for UTHR\n",
      "inserted data for V\n",
      "inserted data for VAL\n",
      "inserted data for VCTR\n",
      "inserted data for VEEV\n",
      "inserted data for VERX\n",
      "inserted data for VFC\n",
      "inserted data for VFS\n",
      "inserted data for VICI\n",
      "inserted data for VIK\n",
      "inserted data for VKTX\n",
      "inserted data for VLO\n",
      "inserted data for VLTO\n",
      "inserted data for VLY\n",
      "inserted data for VMC\n",
      "inserted data for VMI\n",
      "inserted data for VNO\n",
      "inserted data for VNOM\n",
      "inserted data for VNT\n",
      "inserted data for VOYA\n",
      "inserted data for VRNS\n",
      "inserted data for VRRM\n",
      "inserted data for VRSK\n",
      "inserted data for VRSN\n",
      "inserted data for VRT\n",
      "inserted data for VRTX\n",
      "inserted data for VST\n",
      "inserted data for VTR\n",
      "inserted data for VTRS\n",
      "inserted data for VVV\n",
      "inserted data for VZ\n",
      "inserted data for W\n",
      "inserted data for WAB\n",
      "inserted data for WAL\n",
      "inserted data for WAT\n",
      "inserted data for WAY\n",
      "inserted data for WBA\n",
      "inserted data for WBD\n",
      "inserted data for WBS\n",
      "inserted data for WCC\n",
      "inserted data for WCN\n",
      "inserted data for WDAY\n",
      "inserted data for WDC\n",
      "inserted data for WDFC\n",
      "inserted data for WEC\n",
      "inserted data for WELL\n",
      "inserted data for WEN\n",
      "inserted data for WEX\n",
      "inserted data for WFC\n",
      "inserted data for WFRD\n",
      "inserted data for WH\n",
      "inserted data for WHD\n",
      "inserted data for WHR\n",
      "inserted data for WING\n",
      "inserted data for WIX\n",
      "inserted data for WK\n",
      "inserted data for WLK\n",
      "inserted data for WM\n",
      "inserted data for WMB\n",
      "inserted data for WMG\n",
      "inserted data for WMS\n",
      "inserted data for WMT\n",
      "inserted data for WPC\n",
      "inserted data for WRB\n",
      "inserted data for WSC\n",
      "inserted data for WSM\n",
      "inserted data for WSO\n",
      "inserted data for WST\n",
      "inserted data for WTFC\n",
      "inserted data for WTM\n",
      "inserted data for WTRG\n",
      "inserted data for WTS\n",
      "inserted data for WTW\n",
      "inserted data for WU\n",
      "inserted data for WWD\n",
      "inserted data for WY\n",
      "inserted data for WYNN\n",
      "inserted data for X\n",
      "inserted data for XEL\n",
      "inserted data for XENE\n",
      "inserted data for XOM\n",
      "inserted data for XP\n",
      "inserted data for XPO\n",
      "inserted data for XRAY\n",
      "inserted data for XYL\n",
      "inserted data for YETI\n",
      "inserted data for YUM\n",
      "inserted data for YUMC\n",
      "inserted data for Z\n",
      "inserted data for ZBH\n",
      "inserted data for ZBRA\n",
      "inserted data for ZETA\n",
      "inserted data for ZG\n",
      "inserted data for ZI\n",
      "inserted data for ZION\n",
      "inserted data for ZM\n",
      "inserted data for ZS\n",
      "inserted data for ZTS\n",
      "inserted data for ZWS\n"
     ]
    }
   ],
   "source": [
    "# GETTING NULLS! gonna fix the coercion logic\n",
    "# con.execute(\"truncate Articles\")\n",
    "\n",
    "# load multicap headlines\n",
    "failed_df = xml_loader(multicap_headlines)\n",
    "# load new headlines\n",
    "failed2_df = xml_loader(headline_august24_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>ticker</th>\n",
       "      <th>description</th>\n",
       "      <th>article_link</th>\n",
       "      <th>article_pubDate</th>\n",
       "      <th>article_title</th>\n",
       "      <th>language</th>\n",
       "      <th>lastBuildDate</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>parsed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2d7d8468-a424-37c5-bd5a-b8bdb3574707</td>\n",
       "      <td>AAL</td>\n",
       "      <td>NORTHAMPTON, MA / ACCESSWIRE / April 30, 2024 ...</td>\n",
       "      <td>https://finance.yahoo.com/news/ve-got-back-mee...</td>\n",
       "      <td>Tue, 30 Apr 2024 14:45:00 +0000</td>\n",
       "      <td>Theyâve Got Your Back: Meet Americanâs Sys...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>2024-05-04 13:00:58+00:00</td>\n",
       "      <td>http://finance.yahoo.com/q/h?s=AAL</td>\n",
       "      <td>Yahoo! Finance: AAL News</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628e541a-a3ce-3f16-8e7d-10c27b5e76cd</td>\n",
       "      <td>AAL</td>\n",
       "      <td>Want AAdvantage Platinum status? It isn't abou...</td>\n",
       "      <td>https://www.fool.com/the-ascent/credit-cards/a...</td>\n",
       "      <td>Sun, 28 Apr 2024 14:30:11 +0000</td>\n",
       "      <td>How Much Do You Need to Fly to Earn American A...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>2024-05-04 13:00:58+00:00</td>\n",
       "      <td>http://finance.yahoo.com/q/h?s=AAL</td>\n",
       "      <td>Yahoo! Finance: AAL News</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570c87a0-a8c8-331e-837f-b8477e62866b</td>\n",
       "      <td>AAL</td>\n",
       "      <td>It's not how much you fly, it's how much you s...</td>\n",
       "      <td>https://www.fool.com/the-ascent/credit-cards/a...</td>\n",
       "      <td>Sat, 27 Apr 2024 14:30:12 +0000</td>\n",
       "      <td>How Much Do You Need to Fly to Earn American A...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>2024-05-04 13:00:58+00:00</td>\n",
       "      <td>http://finance.yahoo.com/q/h?s=AAL</td>\n",
       "      <td>Yahoo! Finance: AAL News</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b1b6b71-b2b5-3268-8cf8-f6b716e8d915</td>\n",
       "      <td>AAL</td>\n",
       "      <td>American Airlines Group Inc. ( NASDAQ:AAL ) la...</td>\n",
       "      <td>https://finance.yahoo.com/news/american-airlin...</td>\n",
       "      <td>Sat, 27 Apr 2024 12:25:19 +0000</td>\n",
       "      <td>American Airlines Group Inc. (NASDAQ:AAL) Just...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>2024-05-04 13:00:58+00:00</td>\n",
       "      <td>http://finance.yahoo.com/q/h?s=AAL</td>\n",
       "      <td>Yahoo! Finance: AAL News</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76a935dc-e538-31d2-84c0-4b0d36787ad2</td>\n",
       "      <td>AAL</td>\n",
       "      <td>American Airlines Group Inc. (NASDAQ:AAL) Q1 2...</td>\n",
       "      <td>https://finance.yahoo.com/news/american-airlin...</td>\n",
       "      <td>Sat, 27 Apr 2024 12:24:23 +0000</td>\n",
       "      <td>American Airlines Group Inc. (NASDAQ:AAL) Q1 2...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>2024-05-04 13:00:58+00:00</td>\n",
       "      <td>http://finance.yahoo.com/q/h?s=AAL</td>\n",
       "      <td>Yahoo! Finance: AAL News</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid ticker  \\\n",
       "0  2d7d8468-a424-37c5-bd5a-b8bdb3574707    AAL   \n",
       "1  628e541a-a3ce-3f16-8e7d-10c27b5e76cd    AAL   \n",
       "2  570c87a0-a8c8-331e-837f-b8477e62866b    AAL   \n",
       "3  2b1b6b71-b2b5-3268-8cf8-f6b716e8d915    AAL   \n",
       "4  76a935dc-e538-31d2-84c0-4b0d36787ad2    AAL   \n",
       "\n",
       "                                         description  \\\n",
       "0  NORTHAMPTON, MA / ACCESSWIRE / April 30, 2024 ...   \n",
       "1  Want AAdvantage Platinum status? It isn't abou...   \n",
       "2  It's not how much you fly, it's how much you s...   \n",
       "3  American Airlines Group Inc. ( NASDAQ:AAL ) la...   \n",
       "4  American Airlines Group Inc. (NASDAQ:AAL) Q1 2...   \n",
       "\n",
       "                                        article_link  \\\n",
       "0  https://finance.yahoo.com/news/ve-got-back-mee...   \n",
       "1  https://www.fool.com/the-ascent/credit-cards/a...   \n",
       "2  https://www.fool.com/the-ascent/credit-cards/a...   \n",
       "3  https://finance.yahoo.com/news/american-airlin...   \n",
       "4  https://finance.yahoo.com/news/american-airlin...   \n",
       "\n",
       "                   article_pubDate  \\\n",
       "0  Tue, 30 Apr 2024 14:45:00 +0000   \n",
       "1  Sun, 28 Apr 2024 14:30:11 +0000   \n",
       "2  Sat, 27 Apr 2024 14:30:12 +0000   \n",
       "3  Sat, 27 Apr 2024 12:25:19 +0000   \n",
       "4  Sat, 27 Apr 2024 12:24:23 +0000   \n",
       "\n",
       "                                       article_title language  \\\n",
       "0  Theyâve Got Your Back: Meet Americanâs Sys...    en-US   \n",
       "1  How Much Do You Need to Fly to Earn American A...    en-US   \n",
       "2  How Much Do You Need to Fly to Earn American A...    en-US   \n",
       "3  American Airlines Group Inc. (NASDAQ:AAL) Just...    en-US   \n",
       "4  American Airlines Group Inc. (NASDAQ:AAL) Q1 2...    en-US   \n",
       "\n",
       "              lastBuildDate                                link  \\\n",
       "0 2024-05-04 13:00:58+00:00  http://finance.yahoo.com/q/h?s=AAL   \n",
       "1 2024-05-04 13:00:58+00:00  http://finance.yahoo.com/q/h?s=AAL   \n",
       "2 2024-05-04 13:00:58+00:00  http://finance.yahoo.com/q/h?s=AAL   \n",
       "3 2024-05-04 13:00:58+00:00  http://finance.yahoo.com/q/h?s=AAL   \n",
       "4 2024-05-04 13:00:58+00:00  http://finance.yahoo.com/q/h?s=AAL   \n",
       "\n",
       "                      title parsed_date  \n",
       "0  Yahoo! Finance: AAL News         NaT  \n",
       "1  Yahoo! Finance: AAL News         NaT  \n",
       "2  Yahoo! Finance: AAL News         NaT  \n",
       "3  Yahoo! Finance: AAL News         NaT  \n",
       "4  Yahoo! Finance: AAL News         NaT  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with the faulty data \n",
    "failed_df['article_pubDate'] = pd.to_datetime(failed_df['article_pubDate'], errors='coerce')\n",
    "failed_df = failed_df.drop(columns=['parsed_date'])\n",
    "failed_df.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "try:\n",
    "    # adding this too just in case\n",
    "    con.execute(\"INSERT INTO headlines.Articles SELECT * FROM failed_df ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again with the faulty data \n",
    "failed2_df['article_pubDate'] = pd.to_datetime(failed2_df['article_pubDate'], errors='coerce')\n",
    "failed2_df = failed2_df.drop(columns=['parsed_date'])\n",
    "failed2_df.drop_duplicates(subset=['guid', 'ticker'], inplace=True)\n",
    "try:\n",
    "    # adding this too just in case\n",
    "    con.execute(\"INSERT INTO headlines.Articles SELECT * FROM failed2_df ON CONFLICT (guid, ticker) DO NOTHING\")\n",
    "except Exception as e:\n",
    "    print(f\"Error inserting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Articles_Trading_Day`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1fa0bd7a070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.Articles_Trading_Day\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO headlines.Articles_Trading_Day\n",
    "SELECT \n",
    "    a.guid,\n",
    "    a.ticker,\n",
    "    coalesce(MIN(tc.trading_date), cast(a.article_pubDate as Date)) AS mapped_trading_date,\n",
    "    a.description,\n",
    "    a.article_link,\n",
    "    a.article_pubDate,\n",
    "    a.article_title,\n",
    "    a.language,\n",
    "    a.lastBuildDate,\n",
    "    a.link,\n",
    "    a.title\n",
    "FROM (\n",
    "    SELECT \n",
    "        guid,\n",
    "        ticker,\n",
    "        description,\n",
    "        article_link,\n",
    "        article_pubDate,\n",
    "        article_title,\n",
    "        language,\n",
    "        lastBuildDate,\n",
    "        link,\n",
    "        title,\n",
    "        -- 4 PM EST adjust\n",
    "        CASE \n",
    "            WHEN CAST(article_pubDate AS TIME) >= '16:00:00' \n",
    "            THEN CAST(article_pubDate AS DATE) + INTERVAL '1 day'\n",
    "            ELSE CAST(article_pubDate AS DATE)\n",
    "        END AS adjusted_pub_date\n",
    "    FROM headlines.Articles\n",
    ") a\n",
    "LEFT JOIN \n",
    "    headlines.Trading_Calendar tc\n",
    "ON \n",
    "    tc.trading_date >= a.adjusted_pub_date\n",
    "GROUP BY \n",
    "    a.guid, a.ticker, a.description, a.article_link, a.article_pubDate, \n",
    "    a.article_title, a.language, a.lastBuildDate, a.link, a.title;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create `Market_Data_Headlines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x27dd32c67b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con.execute(\"Truncate Market_Data_Headlines\")\n",
    "con.execute(\"\"\"\n",
    "INSERT INTO headlines.Market_Data_Headlines\n",
    "SELECT \n",
    "    md.trading_day_date,\n",
    "    md.ticker,\n",
    "    md.price,\n",
    "    md.volume,\n",
    "    COALESCE(COUNT(DISTINCT atd.guid), 0) AS headline_count\n",
    "FROM \n",
    "    headlines.Market_Data_Daily_Processing md\n",
    "LEFT JOIN \n",
    "    headlines.Articles_Trading_Day atd\n",
    "ON \n",
    "    md.ticker = atd.ticker AND md.trading_day_date = atd.mapped_trading_date\n",
    "GROUP BY \n",
    "    md.trading_day_date, md.ticker, md.price, md.volume;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `market_article_summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x27dd32c67b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# con.execute(\"drop table headlines.Market_Article_Summary\")\n",
    "# con.execute(\"\"\"\n",
    "#                 CREATE TABLE IF NOT EXISTS headlines.Market_Article_Summary (\n",
    "#     trading_date DATE PRIMARY KEY,\n",
    "#     article_count INT\n",
    "#     );\n",
    "\n",
    "#             \"\"\")\n",
    "con.execute('''\n",
    "INSERT INTO headlines.Market_Article_Summary\n",
    "SELECT \n",
    "    atd.mapped_trading_date AS trading_date,\n",
    "    COUNT(DISTINCT atd.guid) AS total_unique_articles\n",
    "FROM \n",
    "    headlines.Articles_Trading_Day atd\n",
    "GROUP BY \n",
    "    atd.mapped_trading_date;\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Daily_Price_Movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1fa2d5835b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.Daily_Price_Movement\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO headlines.Daily_Price_Movement\n",
    "            SELECT \n",
    "                sp1.trading_day_date AS trading_date,\n",
    "                sp1.ticker,\n",
    "                sp1.price AS close_price,\n",
    "                sp2.trading_day_date AS next_trading_day,\n",
    "                sp2.price AS close_price_next,\n",
    "                ROUND(sp2.price - sp1.price, 2) AS price_change,\n",
    "                ROUND((sp2.price - sp1.price) / sp1.price * 100, 2) AS price_change_percentage\n",
    "            FROM headlines.market_data_daily_processing sp1\n",
    "            LEFT JOIN headlines.market_data_daily_processing sp2 \n",
    "            ON sp2.ticker = sp1.ticker \n",
    "            AND sp2.trading_day_date = (\n",
    "                SELECT MIN(sp3.trading_day_date) \n",
    "                FROM headlines.market_data_daily_processing sp3\n",
    "                WHERE sp3.ticker = sp1.ticker\n",
    "                AND sp3.trading_day_date > sp1.trading_day_date\n",
    "            );\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Weekly_Price_Movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1fa2d5835b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.Weekly_Price_Movement\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO headlines.Weekly_Price_Movement\n",
    "            WITH WeeklyPrices AS (\n",
    "            SELECT \n",
    "                ticker,\n",
    "                MIN(tc.trading_date) AS trading_week_start,\n",
    "                MAX(tc.trading_date) AS trading_week_end\n",
    "            FROM headlines.market_data_daily_processing mdp\n",
    "            JOIN headlines.trading_calendar tc \n",
    "            ON mdp.trading_day_date = tc.trading_date\n",
    "            WHERE EXTRACT(DOW FROM tc.trading_date) BETWEEN 1 AND 5  -- Only weekdays\n",
    "            GROUP BY ticker, DATE_TRUNC('week', tc.trading_date)\n",
    "        ),\n",
    "        StartPrices AS (\n",
    "            SELECT \n",
    "                mdp.trading_day_date AS trading_week_start, \n",
    "                mdp.ticker, \n",
    "                mdp.price AS close_price\n",
    "            FROM headlines.market_data_daily_processing mdp\n",
    "            JOIN WeeklyPrices wp \n",
    "            ON mdp.ticker = wp.ticker \n",
    "            AND mdp.trading_day_date = wp.trading_week_start\n",
    "        ),\n",
    "        EndPrices AS (\n",
    "            SELECT \n",
    "                mdp.trading_day_date AS trading_week_end, \n",
    "                mdp.ticker, \n",
    "                mdp.price AS close_price_end\n",
    "            FROM headlines.market_data_daily_processing mdp\n",
    "            JOIN WeeklyPrices wp \n",
    "            ON mdp.ticker = wp.ticker \n",
    "            AND mdp.trading_day_date = wp.trading_week_end\n",
    "        )\n",
    "        SELECT \n",
    "            sp.trading_week_start,\n",
    "            sp.ticker,\n",
    "            sp.close_price as close_price_start,\n",
    "            ep.trading_week_end,\n",
    "            ep.close_price_end,\n",
    "            ROUND(ep.close_price_end - sp.close_price, 2) AS price_change,\n",
    "            ROUND((ep.close_price_end - sp.close_price) / sp.close_price * 100, 2) AS price_change_percentage\n",
    "        FROM StartPrices sp\n",
    "        JOIN EndPrices ep \n",
    "        ON sp.ticker = ep.ticker \n",
    "        AND sp.trading_week_start = ep.trading_week_end - INTERVAL '4 days';\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `extreme_price_movements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1bfe96580b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.extreme_price_movements\")\n",
    "con.execute(\"\"\"\n",
    "            INSERT INTO headlines.extreme_price_movements\n",
    "            SELECT trading_date, ticker, close_price, price_change, price_change_percentage,\n",
    "                CASE \n",
    "                    WHEN price_change_percentage < -5 THEN 'Drop'\n",
    "                    WHEN price_change_percentage > 5 THEN 'Surge'\n",
    "                END AS movement_type\n",
    "            FROM headlines.daily_price_movement\n",
    "            WHERE ABS(price_change_percentage) > 5;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `articles_extreme_drops`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1bfe96580b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.articles_extreme_drops\")\n",
    "df = con.execute(\"\"\"\n",
    "            SELECT epm.trading_date, epm.ticker, a.guid, a.mapped_trading_date,\n",
    "                fs.finbert_title_score AS title_sentiment_score,\n",
    "                fs.finbert_title_label AS title_sentiment_label,\n",
    "                fs.finbert_description_score AS descripton_sentiment_score,\n",
    "                fs.finbert_description_label AS descripton_sentiment_label\n",
    "            FROM headlines.extreme_price_movements epm\n",
    "            JOIN headlines.articles_trading_day a\n",
    "            ON epm.ticker = a.ticker\n",
    "            AND a.mapped_trading_date BETWEEN epm.trading_date - INTERVAL '3 days' AND epm.trading_date\n",
    "            LEFT JOIN headlines.finbert_sentiment fs\n",
    "            ON a.guid = fs.guid\n",
    "\"\"\").df() \n",
    "\n",
    "# dedupe based on trading_date, ticker, guid \n",
    "df.drop_duplicates(subset=['trading_date', 'ticker', 'guid'], inplace=True)\n",
    "\n",
    "con.execute(\"INSERT INTO headlines.articles_extreme_drops select * from df ON CONFLICT (trading_date, ticker, guid) DO NOTHING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOT S&P mapped, I'll do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"financial_news.db\")\n",
    "sp500_volume_weekly_path = 'SP500\\\\volume.csv'\n",
    "sp500_price_weekly_path = 'SP500\\\\price.csv'\n",
    "sp500_price_daily_path = 'SP500\\\\price_daily.csv'\n",
    "sp500_company_path = 'SP500\\\\company_info_sp500.txt'\n",
    "sp500_price_sp500_path = 'SP500\\\\price_SP500.csv'\n",
    "sp500_item1_path = 'SP500\\\\sp500_item1_sec_filings_0.txt'\n",
    "sp500_item1a_path = 'SP500\\\\sp500_item1a_sec_filings_0.txt'\n",
    "sp500_item7_path = 'SP500\\\\sp500_item7_sec_filings_0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table statements \n",
    "drop_statements = [\n",
    "    \"DROP TABLE IF EXISTS SP500.Volume_Weekly;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Price_Daily;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Company_Info;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Weekly_Market_Data;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Price_Weekly;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.Price_Weekly_SP500;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.item7;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.item1a;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.item1;\",\n",
    "    \"DROP TABLE IF EXISTS SP500.SEC_Item_Filings;\"\n",
    "]\n",
    "\n",
    "ddl_statements = [\n",
    "    \"CREATE SCHEMA IF NOT EXISTS SP500;\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Volume_Weekly (\n",
    "    trading_week_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    volume FLOAT,\n",
    "    PRIMARY KEY (trading_week_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Price_Daily (\n",
    "    trading_day_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    price FLOAT,\n",
    "    PRIMARY KEY (trading_day_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Company_Info (\n",
    "    cik TEXT PRIMARY KEY,\n",
    "    ticker TEXT,  \n",
    "    name TEXT, \n",
    "    subindustry TEXT \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Weekly_Market_Data (\n",
    "    trading_week_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    price FLOAT NOT NULL,\n",
    "    volume FLOAT NOT NULL,\n",
    "    PRIMARY KEY (trading_week_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Price_Weekly (\n",
    "    trading_week_date DATE NOT NULL,\n",
    "    cik TEXT NOT NULL,\n",
    "    price FLOAT,\n",
    "    PRIMARY KEY (trading_week_date, cik)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.Price_Weekly_SP500 (\n",
    "    trading_week_date DATE PRIMARY KEY,\n",
    "    SP500CapWeighted FLOAT,\n",
    "    SP500EqualWeighted FLOAT\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.item7 (\n",
    "    company TEXT NOT NULL,           \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    cik TEXT,                         \n",
    "    item7 TEXT,                      \n",
    "    PRIMARY KEY (company, filing_ts)      \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.item1a (\n",
    "    company TEXT NOT NULL,           \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    cik TEXT,                         \n",
    "    item1a TEXT,                     \n",
    "    PRIMARY KEY (company, filing_ts)      \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.item1 (\n",
    "    company TEXT NOT NULL,           \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    cik TEXT,                         \n",
    "    item1 TEXT,                      \n",
    "    PRIMARY KEY (company, filing_ts)      \n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS SP500.SEC_Item_Filings (\n",
    "    cik TEXT NOT NULL,               \n",
    "    filing_ts TIMESTAMP NOT NULL,              \n",
    "    item_filing TEXT NOT NULL,       \n",
    "    company TEXT,                    \n",
    "    link TEXT,                       \n",
    "    type TEXT,                       \n",
    "    item_description TEXT,           \n",
    "    PRIMARY KEY (cik, filing_ts, item_filing)\n",
    ");\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "for drop in drop_statements:\n",
    "    con.execute(drop)\n",
    "\n",
    "for ddl in ddl_statements:\n",
    "    con.execute(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `volume_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21f97f4d870>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(sp500_volume_weekly_path)\n",
    "# convert the wide format to long format\n",
    "# volume_long_df = wide_to_long(df, ['Date'], 'Volume', 'cik')\n",
    "volume_long_df = df.melt(id_vars=['Date'], var_name='cik', value_name='Volume')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "volume_long_df['Date'] = pd.to_datetime(volume_long_df['Date'])\n",
    "volume_long_df['Volume'] = pd.to_numeric(volume_long_df['Volume'], errors='coerce')\n",
    "\n",
    "# default null volume values to 0. CIK 1534701 is all nulls so better to just drop the column but keep it for now\n",
    "# nah jk leaving it as null for now. will coalesce the final table\n",
    "# volume_long_df['Volume'] = volume_long_df['Volume'].fillna(0)\n",
    "\n",
    "con.execute(\"INSERT INTO SP500.Volume_Weekly (trading_week_date, cik, volume) SELECT date, cik, volume FROM volume_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Price_Daily`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21f97f4d870>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(sp500_price_daily_path)\n",
    "# convert the wide format to long format\n",
    "# price_long_df = wide_to_long(df, ['Date'], 'Price', 'cik')\n",
    "price_long_df = df.melt(id_vars=['Date'], var_name='cik', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "price_long_df['Date'] = pd.to_datetime(price_long_df['Date'])\n",
    "price_long_df['Price'] = pd.to_numeric(price_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE SP500.Price_Daily\")\n",
    "con.execute(\"INSERT INTO SP500.Price_Daily (trading_day_date, cik, price) SELECT Date, cik, Price FROM price_long_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>cik</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>1534701</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-14</td>\n",
       "      <td>1534701</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-21</td>\n",
       "      <td>1534701</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-28</td>\n",
       "      <td>1534701</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-04</td>\n",
       "      <td>1534701</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122945</th>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>814585</td>\n",
       "      <td>10.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122946</th>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>814585</td>\n",
       "      <td>8.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122947</th>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>814585</td>\n",
       "      <td>8.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122948</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>814585</td>\n",
       "      <td>9.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122949</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>814585</td>\n",
       "      <td>10.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1122950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date      cik  Price\n",
       "0       2000-01-07  1534701    NaN\n",
       "1       2000-01-14  1534701    NaN\n",
       "2       2000-01-21  1534701    NaN\n",
       "3       2000-01-28  1534701    NaN\n",
       "4       2000-02-04  1534701    NaN\n",
       "...            ...      ...    ...\n",
       "1122945 2023-03-10   814585  10.92\n",
       "1122946 2023-03-17   814585   8.68\n",
       "1122947 2023-03-24   814585   8.20\n",
       "1122948 2023-03-31   814585   9.26\n",
       "1122949 2023-04-06   814585  10.23\n",
       "\n",
       "[1122950 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_long_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Price_Weekly`\n",
    "\n",
    "<b>Looks like theres an issue with this dataset. When the office hours recordings come out I'll watch it and fix it. until then it'll be null </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>1534701</th>\n",
       "      <th>1341439</th>\n",
       "      <th>792985</th>\n",
       "      <th>1489393</th>\n",
       "      <th>86312</th>\n",
       "      <th>96289</th>\n",
       "      <th>1393612</th>\n",
       "      <th>40704</th>\n",
       "      <th>726513</th>\n",
       "      <th>...</th>\n",
       "      <th>75829</th>\n",
       "      <th>906345</th>\n",
       "      <th>84839</th>\n",
       "      <th>1122304</th>\n",
       "      <th>1526520</th>\n",
       "      <th>1013871</th>\n",
       "      <th>920760</th>\n",
       "      <th>814585</th>\n",
       "      <th>SP500CapWeighted</th>\n",
       "      <th>SP500EqualWeighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.531250</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>54.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.21875</td>\n",
       "      <td>52.500</td>\n",
       "      <td>...</td>\n",
       "      <td>20.5000</td>\n",
       "      <td>26.8125</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.126582</td>\n",
       "      <td>34.458333</td>\n",
       "      <td>1455.219971</td>\n",
       "      <td>1414.1586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.921875</td>\n",
       "      <td>14.8125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>53.9375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.90625</td>\n",
       "      <td>51.125</td>\n",
       "      <td>...</td>\n",
       "      <td>20.1875</td>\n",
       "      <td>26.1250</td>\n",
       "      <td>0.881573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.070687</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>1399.420044</td>\n",
       "      <td>1375.8559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>15.6250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.3125</td>\n",
       "      <td>56.3125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.81250</td>\n",
       "      <td>50.375</td>\n",
       "      <td>...</td>\n",
       "      <td>19.5000</td>\n",
       "      <td>26.1875</td>\n",
       "      <td>0.877915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.098634</td>\n",
       "      <td>33.041667</td>\n",
       "      <td>1402.109985</td>\n",
       "      <td>1382.7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>16.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.9375</td>\n",
       "      <td>51.3750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.78125</td>\n",
       "      <td>50.500</td>\n",
       "      <td>...</td>\n",
       "      <td>20.8125</td>\n",
       "      <td>26.6875</td>\n",
       "      <td>0.885231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.098634</td>\n",
       "      <td>33.916667</td>\n",
       "      <td>1403.449951</td>\n",
       "      <td>1398.3869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.843750</td>\n",
       "      <td>17.8125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.2500</td>\n",
       "      <td>50.9375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.81250</td>\n",
       "      <td>50.500</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0625</td>\n",
       "      <td>27.1875</td>\n",
       "      <td>0.892547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.238371</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>1441.469971</td>\n",
       "      <td>1429.7778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 927 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  1534701    1341439   792985  1489393    86312    96289  \\\n",
       "0  2000-01-03      NaN  29.531250  14.5000      NaN  33.0000  54.0000   \n",
       "1  2000-01-04      NaN  26.921875  14.8125      NaN  32.5625  53.9375   \n",
       "2  2000-01-05      NaN  25.500000  15.6250      NaN  32.3125  56.3125   \n",
       "3  2000-01-06      NaN  24.000000  16.7500      NaN  32.9375  51.3750   \n",
       "4  2000-01-07      NaN  25.843750  17.8125      NaN  34.2500  50.9375   \n",
       "\n",
       "   1393612     40704  726513  ...    75829   906345     84839  1122304  \\\n",
       "0      NaN  17.21875  52.500  ...  20.5000  26.8125  0.888889      NaN   \n",
       "1      NaN  16.90625  51.125  ...  20.1875  26.1250  0.881573      NaN   \n",
       "2      NaN  16.81250  50.375  ...  19.5000  26.1875  0.877915      NaN   \n",
       "3      NaN  16.78125  50.500  ...  20.8125  26.6875  0.885231      NaN   \n",
       "4      NaN  16.81250  50.500  ...  21.0625  27.1875  0.892547      NaN   \n",
       "\n",
       "   1526520  1013871    920760     814585  SP500CapWeighted  SP500EqualWeighted  \n",
       "0      NaN      NaN  7.126582  34.458333       1455.219971           1414.1586  \n",
       "1      NaN      NaN  7.070687  32.750000       1399.420044           1375.8559  \n",
       "2      NaN      NaN  7.098634  33.041667       1402.109985           1382.7999  \n",
       "3      NaN      NaN  7.098634  33.916667       1403.449951           1398.3869  \n",
       "4      NaN      NaN  7.238371  34.250000       1441.469971           1429.7778  \n",
       "\n",
       "[5 rows x 927 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(sp500_price_weekly_path)\n",
    "# convert the wide format to long format\n",
    "# price_long_df = wide_to_long(df, ['Date'], 'Price', 'cik')\n",
    "price_long_df = df.melt(id_vars=['Date'], var_name='cik', value_name='Price')\n",
    "\n",
    "# make sure they have the correct data types\n",
    "price_long_df['Date'] = pd.to_datetime(price_long_df['Date'])\n",
    "price_long_df['Price'] = pd.to_numeric(price_long_df['Price'], errors='coerce')\n",
    "\n",
    "# con.execute(\"TRUNCATE SP500.Price_Weekly\")\n",
    "# con.execute(\"INSERT INTO SP500.Price_Weekly (trading_week_date, cik, price) SELECT Date, cik, price FROM price_long_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Company_Info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its all in one line\n",
    "with open(sp500_company_path, 'r') as file:\n",
    "    lines = file.readline().split('\\\\n')\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split('|')\n",
    "        # DONT RUN THIS TWICE BY MISTAKE!\n",
    "        con.execute(\"INSERT INTO SP500.Company_Info VALUES (?,?,?,?)\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `Weekly_Market_Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21f97f4d870>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "INSERT INTO SP500.Weekly_Market_Data\n",
    "SELECT \n",
    "    pw.trading_week_date AS trading_week_date,\n",
    "    pw.cik AS cik,\n",
    "    coalesce(pw.price, 0) AS price,\n",
    "    coalesce(vw.volume, 0) AS volume\n",
    "FROM \n",
    "    SP500.Price_Weekly pw\n",
    "LEFT JOIN \n",
    "    SP500.Volume_Weekly vw\n",
    "ON \n",
    "    pw.trading_week_date = vw.trading_week_date AND pw.cik = vw.cik\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `SP500.Price_Weekly_SP500`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21f97f4d870>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(sp500_price_sp500_path)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['SP500CapWeighted'] = pd.to_numeric(df['SP500CapWeighted'], errors='coerce')\n",
    "df['SP500EqualWeighted'] = pd.to_numeric(df['SP500EqualWeighted'], errors='coerce')\n",
    "\n",
    "con.execute(\"truncate SP500.Price_Weekly_SP500\")\n",
    "con.execute(\"INSERT INTO SP500.Price_Weekly_SP500 (trading_week_date,SP500CapWeighted, SP500EqualWeighted) SELECT Date, SP500CapWeighted, SP500EqualWeighted FROM df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `item7 `\n",
    "\n",
    "Sometimes the last columns comes in multiple lines. Sucks b/c its last column and I can't rely on the pipe, so gotta code for that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols: company|date|link|type|cik|item7 \n",
    "# con.execute(\"TRUNCATE SP500.item7\")\n",
    "def parse_items(file_path, table_name):\n",
    "    # use 2 pointer approach to check if next line is a continuation of the current line\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        prev_line = None  \n",
    "\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            fields = line.split('|')\n",
    "\n",
    "            if prev_line is None:\n",
    "                prev_line = fields\n",
    "                continue\n",
    "\n",
    "            # if current line has required number of elements, insert prev line\n",
    "            if len(fields) == 6:\n",
    "                con.execute(f\"INSERT INTO SP500.{table_name} VALUES (?,?,?,?,?,?)\", prev_line)\n",
    "                prev_line = fields\n",
    "            else:\n",
    "                prev_line[-1] += \" \" + line\n",
    "\n",
    "        # the last record\n",
    "        if prev_line:\n",
    "            con.execute(f\"INSERT INTO SP500.{table_name} VALUES (?,?,?,?,?,?)\", prev_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"TRUNCATE SP500.item7\")\n",
    "parse_items(sp500_item7_path, 'item7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `item1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"TRUNCATE SP500.item1\")\n",
    "parse_items(sp500_item1_path, 'item1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `item1a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"TRUNCATE SP500.item1a\")\n",
    "parse_items(sp500_item1a_path, 'item1a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `SEC_Item_Filings`\n",
    "\n",
    "I think the only thing different between the 3 item tables are the item filing # and the item description\n",
    "\n",
    "With that said, I believe its better to use a longer table for simplicity vs a wider table \n",
    "- wide = instead of 1 item_filing and 1 item_description columns, we make a column for each filing and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x21f97f4d870>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"\"\"\n",
    "INSERT INTO SP500.SEC_Item_Filings\n",
    "SELECT \n",
    "    cik,\n",
    "    filing_ts,\n",
    "    '7' AS item_filing,\n",
    "    company,\n",
    "    link,\n",
    "    type,\n",
    "    item7 AS item_description\n",
    "FROM \n",
    "    SP500.item7\n",
    "UNION ALL\n",
    "SELECT \n",
    "    cik,\n",
    "    filing_ts,\n",
    "    '1a' AS item_filing,\n",
    "    company,\n",
    "    link,\n",
    "    type,\n",
    "    item1a AS item_description\n",
    "FROM \n",
    "    SP500.item1a\n",
    "UNION ALL\n",
    "SELECT \n",
    "    cik,\n",
    "    filing_ts,\n",
    "    '1' AS item_filing,\n",
    "    company,\n",
    "    link,\n",
    "    type,\n",
    "    item1 AS item_description\n",
    "FROM \n",
    "    SP500.item1;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinBERT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"financial_news.db\")\n",
    "finbert_csv = \"articles_with_all_finbert_scores.csv\"\n",
    "# SO THIS ONE WILL HAVE DUPES! NEED TO DEDUPE!\n",
    "tokens_csv = \"tokens.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.finbert_sentiment (\n",
    "    guid TEXT,\n",
    "    description TEXT,\n",
    "    article_title TEXT,\n",
    "    ticker TEXT,\n",
    "    finbert_title_label TEXT,\n",
    "    finbert_title_score FLOAT,\n",
    "    finbert_description_label TEXT,\n",
    "    finbert_description_score FLOAT,\n",
    "    PRIMARY KEY (guid, ticker)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.tokens_description (\n",
    "    guid TEXT,\n",
    "    token TEXT,\n",
    "    token_lemmatized TEXT,\n",
    "    frequency INT,\n",
    "    PRIMARY KEY (guid, token, token_lemmatized)\n",
    ");\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS headlines.tokens_title (\n",
    "    guid TEXT,\n",
    "    token TEXT,\n",
    "    token_lemmatized TEXT,\n",
    "    frequency INT,\n",
    "    PRIMARY KEY (guid, token, token_lemmatized)\n",
    ");\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS headlines.finbert_sentiment;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.tokens_description;\",\n",
    "    \"DROP TABLE IF EXISTS headlines.tokens_title;\"\n",
    "    ]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `finbert_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>description</th>\n",
       "      <th>article_title</th>\n",
       "      <th>ticker</th>\n",
       "      <th>finbert_title_label</th>\n",
       "      <th>finbert_title_score</th>\n",
       "      <th>finbert_description_label</th>\n",
       "      <th>finbert_description_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76ceb11d-33eb-3af8-82f1-74e4068911f5</td>\n",
       "      <td>Agilent (A) adds a water immersion and confoca...</td>\n",
       "      <td>Agilent (A) Enhances BioTek Cytation C10 With ...</td>\n",
       "      <td>A</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56dc485e-c740-3fcc-ab3a-4e0d707a8f4d</td>\n",
       "      <td>SANTA CLARA, Calif., December 07, 2023--Agilen...</td>\n",
       "      <td>Agilent Resolve Raman Receives Multiple Recogn...</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.861646</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.999870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367bed80-8d07-3dce-8092-fd53d70578fe</td>\n",
       "      <td>Artisan Partners, an investment management com...</td>\n",
       "      <td>Hereâs Why Artisan Partners Mid Cap Fund Har...</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.846491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7bf92827-a505-3d56-98a3-4c9d60794e64</td>\n",
       "      <td>Generally speaking the aim of active stock pic...</td>\n",
       "      <td>Agilent Technologies' (NYSE:A) 14% CAGR outpac...</td>\n",
       "      <td>A</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.999604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8e5bdc52-73a9-30b1-ae97-493cd82da360</td>\n",
       "      <td>SANTA CLARA, Calif., December 04, 2023--Agilen...</td>\n",
       "      <td>Agilent BioTek Cytation C10 Confocal Imaging R...</td>\n",
       "      <td>A</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.999978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  \\\n",
       "0  76ceb11d-33eb-3af8-82f1-74e4068911f5   \n",
       "1  56dc485e-c740-3fcc-ab3a-4e0d707a8f4d   \n",
       "2  367bed80-8d07-3dce-8092-fd53d70578fe   \n",
       "3  7bf92827-a505-3d56-98a3-4c9d60794e64   \n",
       "4  8e5bdc52-73a9-30b1-ae97-493cd82da360   \n",
       "\n",
       "                                         description  \\\n",
       "0  Agilent (A) adds a water immersion and confoca...   \n",
       "1  SANTA CLARA, Calif., December 07, 2023--Agilen...   \n",
       "2  Artisan Partners, an investment management com...   \n",
       "3  Generally speaking the aim of active stock pic...   \n",
       "4  SANTA CLARA, Calif., December 04, 2023--Agilen...   \n",
       "\n",
       "                                       article_title ticker  \\\n",
       "0  Agilent (A) Enhances BioTek Cytation C10 With ...      A   \n",
       "1  Agilent Resolve Raman Receives Multiple Recogn...      A   \n",
       "2  Hereâs Why Artisan Partners Mid Cap Fund Har...      A   \n",
       "3  Agilent Technologies' (NYSE:A) 14% CAGR outpac...      A   \n",
       "4  Agilent BioTek Cytation C10 Confocal Imaging R...      A   \n",
       "\n",
       "  finbert_title_label  finbert_title_score finbert_description_label  \\\n",
       "0            Positive             1.000000                  Positive   \n",
       "1             Neutral             0.861646                  Positive   \n",
       "2             Neutral             0.999948                   Neutral   \n",
       "3            Positive             1.000000                  Positive   \n",
       "4            Positive             1.000000                  Positive   \n",
       "\n",
       "   finbert_description_score  \n",
       "0                   0.999999  \n",
       "1                   0.999870  \n",
       "2                   0.846491  \n",
       "3                   0.999604  \n",
       "4                   0.999978  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(finbert_csv)\n",
    "# df.head()\n",
    "filtered_df = df[['guid', 'description', 'article_title', 'ticker', 'finbert_title_label', 'finbert_title_score', 'finbert_description_label', 'finbert_description_score']]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1bf7fd925b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.finbert_sentiment\")\n",
    "con.execute(\"INSERT INTO headlines.finbert_sentiment select * from filtered_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `tokens_description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091460\n",
      "842712\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>tokens_description</th>\n",
       "      <th>tokens_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76ceb11d-33eb-3af8-82f1-74e4068911f5</td>\n",
       "      <td>[('agilent', 'agilent'), ('adds', 'add'), ('wa...</td>\n",
       "      <td>[('agilent', 'agilent'), ('enhances', 'enhance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56dc485e-c740-3fcc-ab3a-4e0d707a8f4d</td>\n",
       "      <td>[('santa', 'santa'), ('clara', 'clara'), ('cal...</td>\n",
       "      <td>[('agilent', 'agilent'), ('resolve', 'resolve'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367bed80-8d07-3dce-8092-fd53d70578fe</td>\n",
       "      <td>[('artisan', 'artisan'), ('partners', 'partner...</td>\n",
       "      <td>[('artisan', 'artisan'), ('partners', 'partner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7bf92827-a505-3d56-98a3-4c9d60794e64</td>\n",
       "      <td>[('generally', 'generally'), ('speaking', 'spe...</td>\n",
       "      <td>[('agilent', 'agilent'), ('technologies', 'tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8e5bdc52-73a9-30b1-ae97-493cd82da360</td>\n",
       "      <td>[('santa', 'santa'), ('clara', 'clara'), ('cal...</td>\n",
       "      <td>[('agilent', 'agilent'), ('biotek', 'biotek'),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid  \\\n",
       "0  76ceb11d-33eb-3af8-82f1-74e4068911f5   \n",
       "1  56dc485e-c740-3fcc-ab3a-4e0d707a8f4d   \n",
       "2  367bed80-8d07-3dce-8092-fd53d70578fe   \n",
       "3  7bf92827-a505-3d56-98a3-4c9d60794e64   \n",
       "4  8e5bdc52-73a9-30b1-ae97-493cd82da360   \n",
       "\n",
       "                                  tokens_description  \\\n",
       "0  [('agilent', 'agilent'), ('adds', 'add'), ('wa...   \n",
       "1  [('santa', 'santa'), ('clara', 'clara'), ('cal...   \n",
       "2  [('artisan', 'artisan'), ('partners', 'partner...   \n",
       "3  [('generally', 'generally'), ('speaking', 'spe...   \n",
       "4  [('santa', 'santa'), ('clara', 'clara'), ('cal...   \n",
       "\n",
       "                                        tokens_title  \n",
       "0  [('agilent', 'agilent'), ('enhances', 'enhance...  \n",
       "1  [('agilent', 'agilent'), ('resolve', 'resolve'...  \n",
       "2  [('artisan', 'artisan'), ('partners', 'partner...  \n",
       "3  [('agilent', 'agilent'), ('technologies', 'tec...  \n",
       "4  [('agilent', 'agilent'), ('biotek', 'biotek'),...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SO THIS ONE WILL HAVE DUPES! NEED TO DEDUPE!\n",
    "df = pd.read_csv(tokens_csv)\n",
    "print(df.size)\n",
    "\n",
    "# remove df duplicates based on guid \n",
    "df.drop_duplicates(subset=['guid'], inplace=True)\n",
    "print(df.size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_description_data = []\n",
    "tokens_title_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    guid = row['guid']\n",
    "    \n",
    "    tokens_description = eval(row['tokens_description'])\n",
    "    tokens_title = eval(row['tokens_title'])\n",
    "    \n",
    "    for token, lemma  in tokens_description:\n",
    "        tokens_description_data.append((guid, token, lemma))\n",
    "    \n",
    "    for token, lemma  in tokens_title:\n",
    "        tokens_title_data.append((guid, token, lemma))\n",
    "\n",
    "df_tokens_description = pd.DataFrame(tokens_description_data, columns=[\"guid\", \"token\", \"token_lemmatized\"])\n",
    "df_tokens_title = pd.DataFrame(tokens_title_data, columns=[\"guid\", \"token\", \"token_lemmatized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>token</th>\n",
       "      <th>token_lemmatized</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>attributes</td>\n",
       "      <td>attribute</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>exceptional</td>\n",
       "      <td>exceptional</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>growth</td>\n",
       "      <td>growth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>produce</td>\n",
       "      <td>produce</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid        token token_lemmatized  \\\n",
       "0  000026c6-886f-3930-a121-e633a8456b07   attributes        attribute   \n",
       "1  000026c6-886f-3930-a121-e633a8456b07        could            could   \n",
       "2  000026c6-886f-3930-a121-e633a8456b07  exceptional      exceptional   \n",
       "3  000026c6-886f-3930-a121-e633a8456b07       growth           growth   \n",
       "4  000026c6-886f-3930-a121-e633a8456b07      produce          produce   \n",
       "\n",
       "   frequency  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_tokens_description.head()\n",
    "# find guid 367bed80-8d07-3dce-8092-fd53d70578fe with token quarter\n",
    "# df_tokens_description[(df_tokens_description['guid'] == '367bed80-8d07-3dce-8092-fd53d70578fe' ) & (df_tokens_description['token'] == 'quarter')]\n",
    "# aggregate any duplicates and count them and add them to column frequency\n",
    "### OKAY LETS KEEP token_lemmatized HERE BC LEMMAS RELY ON CONTEXT! \n",
    "df_tokens_description = df_tokens_description.groupby(['guid', 'token', 'token_lemmatized']).size().reset_index(name='frequency')\n",
    "# df_tokens_title = df_tokens_title.groupby(['guid', 'token', 'token_lemmatized']).size().reset_index(name='frequency')\n",
    "df_tokens_description.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>token</th>\n",
       "      <th>token_lemmatized</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>000705ff-4abf-355e-bed9-aeb6733f92b3</td>\n",
       "      <td>spending</td>\n",
       "      <td>spend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>000705ff-4abf-355e-bed9-aeb6733f92b3</td>\n",
       "      <td>spending</td>\n",
       "      <td>spending</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     guid     token token_lemmatized  \\\n",
       "522  000705ff-4abf-355e-bed9-aeb6733f92b3  spending            spend   \n",
       "523  000705ff-4abf-355e-bed9-aeb6733f92b3  spending         spending   \n",
       "\n",
       "     frequency  \n",
       "522          1  \n",
       "523          1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 000705ff-4abf-355e-bed9-aeb6733f92b3 with token spending \n",
    "df_tokens_description[(df_tokens_description['guid'] == '000705ff-4abf-355e-bed9-aeb6733f92b3' ) & (df_tokens_description['token'] == 'spending')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1bf7fd925b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.tokens_description\")\n",
    "con.execute(\"INSERT INTO headlines.tokens_description select * from df_tokens_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `tokens_title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>token</th>\n",
       "      <th>token_lemmatized</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>growth</td>\n",
       "      <td>growth</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>reasons</td>\n",
       "      <td>reason</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>roadhouse</td>\n",
       "      <td>roadhouse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>solid</td>\n",
       "      <td>solid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026c6-886f-3930-a121-e633a8456b07</td>\n",
       "      <td>stock</td>\n",
       "      <td>stock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   guid      token token_lemmatized  frequency\n",
       "0  000026c6-886f-3930-a121-e633a8456b07     growth           growth          1\n",
       "1  000026c6-886f-3930-a121-e633a8456b07    reasons           reason          1\n",
       "2  000026c6-886f-3930-a121-e633a8456b07  roadhouse        roadhouse          1\n",
       "3  000026c6-886f-3930-a121-e633a8456b07      solid            solid          1\n",
       "4  000026c6-886f-3930-a121-e633a8456b07      stock            stock          1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens_title = df_tokens_title.groupby(['guid', 'token', 'token_lemmatized']).size().reset_index(name='frequency')\n",
    "df_tokens_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x1bf7fd925b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate headlines.tokens_title\")\n",
    "con.execute(\"INSERT INTO headlines.tokens_title select * from df_tokens_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10k FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"financial_news.db\")\n",
    "finbert_csv = \"sp500_with_all_finbert_scores.csv\"\n",
    "# # SO THIS ONE WILL HAVE DUPES! NEED TO DEDUPE!\n",
    "# tokens_csv = \"tokens.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE if not exists sp500.SEC_Item_Filings_FinBERT (\n",
    "    cik TEXT,\n",
    "    filing_ts TIMESTAMP,\n",
    "    item_filing TEXT,\n",
    "    finbert_description_label TEXT,\n",
    "    finbert_description_score FLOAT,\n",
    "    PRIMARY KEY (cik, filing_ts, item_filing),\n",
    "    FOREIGN KEY (cik, filing_ts, item_filing) \n",
    "        REFERENCES sp500.SEC_Item_Filings (cik, filing_ts, item_filing) \n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS sp500.SEC_Item_Filings_FinBERT;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC_Item_Filings_FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>filing_ts</th>\n",
       "      <th>item_filing</th>\n",
       "      <th>type</th>\n",
       "      <th>item_description</th>\n",
       "      <th>finbert_description</th>\n",
       "      <th>finbert_description_label</th>\n",
       "      <th>finbert_description_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66740</td>\n",
       "      <td>2022-02-09 20:13:29</td>\n",
       "      <td>7</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Item 7. Managements Discussion and Analysis o...</td>\n",
       "      <td>{'label': 'Neutral', 'score': 0.9999117851257324}</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66740</td>\n",
       "      <td>2021-02-04 18:53:11</td>\n",
       "      <td>7</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Item 7. Managements Discussion and Analysis o...</td>\n",
       "      <td>{'label': 'Neutral', 'score': 0.9999666213989258}</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66740</td>\n",
       "      <td>2020-02-06 21:16:31</td>\n",
       "      <td>7</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Item 7. Managements Discussion and Analysis o...</td>\n",
       "      <td>{'label': 'Neutral', 'score': 0.9999666213989258}</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66740</td>\n",
       "      <td>2019-02-07 22:15:37</td>\n",
       "      <td>7</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Item 7. Managements Discussion and Analysis o...</td>\n",
       "      <td>{'label': 'Neutral', 'score': 0.999957799911499}</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66740</td>\n",
       "      <td>2018-02-08 22:14:52</td>\n",
       "      <td>7</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Item 7. Managements Discussion and Analysis o...</td>\n",
       "      <td>{'label': 'Neutral', 'score': 0.999957799911499}</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.999958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cik            filing_ts item_filing  type  \\\n",
       "0  66740  2022-02-09 20:13:29           7  10-K   \n",
       "1  66740  2021-02-04 18:53:11           7  10-K   \n",
       "2  66740  2020-02-06 21:16:31           7  10-K   \n",
       "3  66740  2019-02-07 22:15:37           7  10-K   \n",
       "4  66740  2018-02-08 22:14:52           7  10-K   \n",
       "\n",
       "                                    item_description  \\\n",
       "0   Item 7. Managements Discussion and Analysis o...   \n",
       "1   Item 7. Managements Discussion and Analysis o...   \n",
       "2   Item 7. Managements Discussion and Analysis o...   \n",
       "3   Item 7. Managements Discussion and Analysis o...   \n",
       "4   Item 7. Managements Discussion and Analysis o...   \n",
       "\n",
       "                                 finbert_description  \\\n",
       "0  {'label': 'Neutral', 'score': 0.9999117851257324}   \n",
       "1  {'label': 'Neutral', 'score': 0.9999666213989258}   \n",
       "2  {'label': 'Neutral', 'score': 0.9999666213989258}   \n",
       "3   {'label': 'Neutral', 'score': 0.999957799911499}   \n",
       "4   {'label': 'Neutral', 'score': 0.999957799911499}   \n",
       "\n",
       "  finbert_description_label  finbert_description_score  \n",
       "0                   Neutral                   0.999912  \n",
       "1                   Neutral                   0.999967  \n",
       "2                   Neutral                   0.999967  \n",
       "3                   Neutral                   0.999958  \n",
       "4                   Neutral                   0.999958  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finbert_10k_df = pd.read_csv(finbert_csv)\n",
    "finbert_10k_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_10k_df = finbert_10k_df[['cik', 'filing_ts', 'item_filing', 'finbert_description_label', 'finbert_description_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x23d20b8ebf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"truncate sp500.SEC_Item_Filings_FinBERT\")\n",
    "con.execute(\"INSERT INTO sp500.SEC_Item_Filings_FinBERT select * from finbert_10k_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(\"financial_news.db\")\n",
    "vix = \"SP500/vixGaTechSP25.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE sp500.VIX_Index (\n",
    "    vix_date DATE PRIMARY KEY,\n",
    "    vix_value FLOAT\n",
    ");\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "drop = [\n",
    "    \"DROP TABLE IF EXISTS sp500.VIX_Index;\"\n",
    "]\n",
    "\n",
    "for d in drop:\n",
    "    con.execute(d)\n",
    "    \n",
    "for d in ddl:\n",
    "    con.execute(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vix_date</th>\n",
       "      <th>vix_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-01-02</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-01-03</td>\n",
       "      <td>17.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-01-06</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986-01-07</td>\n",
       "      <td>17.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-01-08</td>\n",
       "      <td>19.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vix_date  vix_value\n",
       "0 1986-01-02      18.07\n",
       "1 1986-01-03      17.96\n",
       "2 1986-01-06      17.05\n",
       "3 1986-01-07      17.39\n",
       "4 1986-01-08      19.97"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vix_df = pd.read_csv(vix, names=[\"vix_date\", \"vix_value\"], parse_dates=[\"vix_date\"], skiprows=1) # first row is the header but not the best\n",
    "vix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vix_date     0\n",
       "vix_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nulls \n",
    "vix_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x23d430d31f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"Truncate sp500.VIX_Index\")\n",
    "con.execute(\"INSERT INTO sp500.VIX_Index select * from vix_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
