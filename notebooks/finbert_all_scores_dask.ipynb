{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference to extracting all labels \n",
    "https://datascience.stackexchange.com/questions/112438/how-to-get-all-3-labels-sentiment-from-finbert-instead-of-the-most-likely-label\n",
    "\n",
    "essentially, us the AutoModelForSequenceClassification to get all raw logits and then apply softmax ourselves \n",
    "\n",
    "normally the pipeline does the softmax and ONLY returns the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Total CPU cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bokeh\n",
    "\n",
    "# !pip install pyarrow==10.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n",
      "Dask version: 2025.2.0\n",
      "PyArrow version: 19.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import pyarrow\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import duckdb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import bokeh\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Dask version:\", dask.__version__)\n",
    "print(\"PyArrow version:\", pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()  # Put model in evaluation mode\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {\"label\": \"NEUTRAL\", \"score\": 1.0, \"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}\n",
    "    \n",
    "    # Tokenize input text\n",
    "    # inputs = tokenizer(text[:512], return_tensors=\"pt\", truncation=True)\n",
    "    # Getting truncation warning. I'ma use tokenizer truncation instead\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits  # Get raw model outputs (logits)\n",
    "    probs = F.softmax(logits, dim=1)  # Apply softmax across dimension 1 (classes)\n",
    "\n",
    "    # Convert to a Python list\n",
    "    probs = probs.numpy()[0]  # Extract probabilities as a NumPy array\n",
    "\n",
    "    # Define label mapping\n",
    "    labels = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "    sentiment_dict = dict(zip(labels, probs))\n",
    "\n",
    "    # Get the highest-probability label\n",
    "    max_label = labels[torch.argmax(logits).item()]\n",
    "    max_score = max(probs)\n",
    "\n",
    "    return {\n",
    "        \"label\": max_label,\n",
    "        \"score\": max_score,\n",
    "        \"positive\": sentiment_dict[\"POSITIVE\"],\n",
    "        \"neutral\": sentiment_dict[\"NEUTRAL\"],\n",
    "        \"negative\": sentiment_dict[\"NEGATIVE\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CANT START A CLIENT AND CLUSTER BEFORE LOADING FINBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n",
      "{'tcp://127.0.0.1:53411': 1, 'tcp://127.0.0.1:53412': 1, 'tcp://127.0.0.1:53413': 1, 'tcp://127.0.0.1:53414': 1, 'tcp://127.0.0.1:53415': 1, 'tcp://127.0.0.1:53416': 1, 'tcp://127.0.0.1:53417': 1, 'tcp://127.0.0.1:53418': 1}\n"
     ]
    }
   ],
   "source": [
    "# Try to avoid PyArrow\n",
    "pd.options.mode.string_storage = \"python\"\n",
    "\n",
    "cluster = LocalCluster(n_workers=num_cores//2, threads_per_worker=1)\n",
    "cluster.adapt(minimum=1, maximum=8)\n",
    "client = Client(cluster)\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "print(client.dashboard_link)\n",
    "print(client.ncores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(r\"C:\\Users\\jovan\\Documents\\CSE 6748 Practicum WITH DATA\\financial_news.db\", read_only=True)\n",
    "# df = con.execute(\"SELECT guid, description, article_title, ticker FROM headlines.articles\").fetchdf()\n",
    "\n",
    "# okay getting a bottle neck at pandas to ddf so ognna write as csv \n",
    "# df.to_csv(\"articles_db.csv\", index=False)\n",
    "\n",
    "# try writing to parquet instead\n",
    "file_name = 'articles_db.parquet'\n",
    "# con.execute(f\"COPY (SELECT guid, description, article_title, ticker FROM headlines.articles) TO '{file_name}' WITH (HEADER, DELIMITER ',');\")\n",
    "con.execute(f\"COPY (SELECT guid, description, article_title, ticker FROM headlines.articles) TO '{file_name}' (FORMAT 'parquet');\")\n",
    "# try partitioning based on date \n",
    "\n",
    "# will try to partition based on date later \n",
    "# output_dir = \"articles_partitioned/\"\n",
    "'''\n",
    "    COPY (\n",
    "        SELECT \n",
    "            guid, \n",
    "            description, \n",
    "            article_title, \n",
    "            ticker, \n",
    "            article_pubdate,\n",
    "            YEAR(article_pubdate) AS year, \n",
    "            MONTH(article_pubdate) AS month\n",
    "        FROM headlines.articles\n",
    "    ) \n",
    "    TO '{output_dir}' \n",
    "    (FORMAT 'parquet', PARTITION_BY (year, month));\n",
    "'''\n",
    "# con.execute(f\"COPY (SELECT guid, description, article_title, ticker, article_pubDate FROM headlines.articles) TO '{output_dir}' (FORMAT 'parquet', PARTITION_BY article_pubDate);\")\n",
    "\n",
    "con.close()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# testing only\n",
    "# filtered_df = df.head(1000)\n",
    "# len(filtered_df)\n",
    "# ddf = dd.read_csv(\"articles_db.csv\", assume_missing=True, dtype={'guid': 'object', 'description': 'object', 'article_title': 'object', 'ticker': 'object'})\n",
    "# read parquet \n",
    "ddf = dd.read_parquet(file_name, engine='pyarrow')\n",
    "# check partitions in ddf \n",
    "print(ddf.npartitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jovan\\anaconda3\\envs\\cs7643-a2\\Lib\\site-packages\\distributed\\client.py:3370: UserWarning: Sending large graph of size 419.26 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Dask DataFrame\n",
    "# ddf = dd.from_pandas(filtered_df, npartitions=8)  # Adjust partitions based on CPU cores\n",
    "# ddf = dd.from_pandas(df, npartitions=8)   # gonna set it to 4 cause 400k rows / 4 = 100k rows per partition \n",
    "# raise ValueError(\"stop here\")\n",
    "# Apply FinBERT sentiment analysis in parallel\n",
    "with ProgressBar():\n",
    "    ddf['finbert_title'] = ddf['article_title'].map(classify_sentiment, meta=(\"x\", \"object\"))\n",
    "    ddf['finbert_title_label'] = ddf['finbert_title'].map(lambda x: x['label'], meta=(\"x\", \"str\"))\n",
    "    ddf['finbert_title_score'] = ddf['finbert_title'].map(lambda x: x['score'], meta=(\"x\", \"float\"))\n",
    "    ddf['finbert_title_positive'] = ddf['finbert_title'].map(lambda x: x['positive'], meta=(\"x\", \"float\"))\n",
    "    ddf['finbert_title_neutral'] = ddf['finbert_title'].map(lambda x: x['neutral'], meta=(\"x\", \"float\"))\n",
    "    ddf['finbert_title_negative'] = ddf['finbert_title'].map(lambda x: x['negative'], meta=(\"x\", \"float\"))\n",
    "\n",
    "    ddf['finbert_description'] = ddf['description'].map(classify_sentiment, meta=(\"x\", \"object\"))\n",
    "    ddf['finbert_description_label'] = ddf['finbert_description'].map(lambda x: x['label'], meta=(\"x\", \"str\"))\n",
    "    ddf['finbert_description_score'] = ddf['finbert_description'].map(lambda x: x['score'], meta=(\"x\", \"float\"))\n",
    "    ddf['finbert_description_positive'] = ddf['finbert_description'].map(lambda x: x['positive'], meta=(\"x\", \"float\"))\n",
    "    ddf['finbert_description_neutral'] = ddf['finbert_description'].map(lambda x: x['neutral'], meta=(\"x\", \"float\"))\n",
    "    ddf['finbert_description_negative'] = ddf['finbert_description'].map(lambda x: x['negative'], meta=(\"x\", \"float\"))\n",
    "\n",
    "# print(\"before compute\")\n",
    "# Convert back to Pandas\n",
    "df_final = ddf.compute()\n",
    "# print(\"after compute\")\n",
    "\n",
    "# Save results\n",
    "df_final.to_csv(\"articles_with_all_finbert_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.close()\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
