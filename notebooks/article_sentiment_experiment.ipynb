{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "Schema: Headlines, Table: Articles\n",
      "Schema: Headlines, Table: Articles_Trading_Day\n",
      "Schema: Headlines, Table: Company_Info_News\n",
      "Schema: Headlines, Table: Market_Article_Summary\n",
      "Schema: Headlines, Table: Market_Data_Daily_Processing\n",
      "Schema: Headlines, Table: Market_Data_Headlines\n",
      "Schema: Headlines, Table: Pricing_News\n",
      "Schema: Headlines, Table: Trading_Calendar\n",
      "Schema: Headlines, Table: Volume_News\n",
      "Schema: SP500, Table: Company_Info\n",
      "Schema: SP500, Table: item1\n",
      "Schema: SP500, Table: item1a\n",
      "Schema: SP500, Table: item7\n",
      "Schema: SP500, Table: Price_Daily\n",
      "Schema: SP500, Table: Price_Weekly\n",
      "Schema: SP500, Table: Price_Weekly_SP500\n",
      "Schema: SP500, Table: SEC_Item_Filings\n",
      "Schema: SP500, Table: Volume_Weekly\n",
      "Schema: SP500, Table: Weekly_Market_Data\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Path to your DuckDB database file\n",
    "db_file_path = r'C:\\Users\\btada\\Documents\\financial_news.db'\n",
    "\n",
    "# Connect to the DuckDB database\n",
    "conn = duckdb.connect(database=db_file_path, read_only=False)\n",
    "\n",
    "# List tables with their schema\n",
    "tables = conn.execute(\"SELECT table_schema, table_name FROM information_schema.tables\").fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for schema, table in tables:\n",
    "    print(f\"Schema: {schema}, Table: {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from table: Articles (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 363820 entries, 0 to 363819\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   guid             363820 non-null  object        \n",
      " 1   ticker           363820 non-null  object        \n",
      " 2   description      363820 non-null  object        \n",
      " 3   article_link     363820 non-null  object        \n",
      " 4   article_pubDate  363820 non-null  datetime64[us]\n",
      " 5   article_title    363820 non-null  object        \n",
      " 6   language         363820 non-null  object        \n",
      " 7   lastBuildDate    355790 non-null  datetime64[us]\n",
      " 8   link             363820 non-null  object        \n",
      " 9   title            363820 non-null  object        \n",
      "dtypes: datetime64[us](2), object(8)\n",
      "memory usage: 27.8+ MB\n",
      "\n",
      "Head:\n",
      "                                   guid ticker  \\\n",
      "0  76ceb11d-33eb-3af8-82f1-74e4068911f5      A   \n",
      "1  56dc485e-c740-3fcc-ab3a-4e0d707a8f4d      A   \n",
      "2  367bed80-8d07-3dce-8092-fd53d70578fe      A   \n",
      "3  7bf92827-a505-3d56-98a3-4c9d60794e64      A   \n",
      "4  8e5bdc52-73a9-30b1-ae97-493cd82da360      A   \n",
      "\n",
      "                                         description  \\\n",
      "0  Agilent (A) adds a water immersion and confoca...   \n",
      "1  SANTA CLARA, Calif., December 07, 2023--Agilen...   \n",
      "2  Artisan Partners, an investment management com...   \n",
      "3  Generally speaking the aim of active stock pic...   \n",
      "4  SANTA CLARA, Calif., December 04, 2023--Agilen...   \n",
      "\n",
      "                                        article_link     article_pubDate  \\\n",
      "0  https://finance.yahoo.com/news/agilent-enhance... 2023-12-07 09:46:00   \n",
      "1  https://finance.yahoo.com/news/agilent-resolve... 2023-12-07 08:00:00   \n",
      "2  https://finance.yahoo.com/news/why-artisan-par... 2023-12-06 02:46:05   \n",
      "3  https://finance.yahoo.com/news/agilent-technol... 2023-12-05 06:00:35   \n",
      "4  https://finance.yahoo.com/news/agilent-biotek-... 2023-12-04 08:00:00   \n",
      "\n",
      "                                       article_title language  \\\n",
      "0  Agilent (A) Enhances BioTek Cytation C10 With ...    en-US   \n",
      "1  Agilent Resolve Raman Receives Multiple Recogn...    en-US   \n",
      "2  Hereâs Why Artisan Partners Mid Cap Fund Har...    en-US   \n",
      "3  Agilent Technologies' (NYSE:A) 14% CAGR outpac...    en-US   \n",
      "4  Agilent BioTek Cytation C10 Confocal Imaging R...    en-US   \n",
      "\n",
      "        lastBuildDate                              link  \\\n",
      "0 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
      "1 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
      "2 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
      "3 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
      "4 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
      "\n",
      "                    title  \n",
      "0  Yahoo! Finance: A News  \n",
      "1  Yahoo! Finance: A News  \n",
      "2  Yahoo! Finance: A News  \n",
      "3  Yahoo! Finance: A News  \n",
      "4  Yahoo! Finance: A News  \n",
      "\n",
      "Loading data from table: Articles_Trading_Day (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 363820 entries, 0 to 363819\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   guid                 363820 non-null  object        \n",
      " 1   ticker               363820 non-null  object        \n",
      " 2   mapped_trading_date  363820 non-null  datetime64[us]\n",
      " 3   description          363820 non-null  object        \n",
      " 4   article_link         363820 non-null  object        \n",
      " 5   article_pubDate      363820 non-null  datetime64[us]\n",
      " 6   article_title        363820 non-null  object        \n",
      " 7   language             363820 non-null  object        \n",
      " 8   lastBuildDate        355790 non-null  datetime64[us]\n",
      " 9   link                 363820 non-null  object        \n",
      " 10  title                363820 non-null  object        \n",
      "dtypes: datetime64[us](3), object(8)\n",
      "memory usage: 30.5+ MB\n",
      "\n",
      "Head:\n",
      "                                   guid ticker mapped_trading_date  \\\n",
      "0  fbf0a995-81b2-3064-9b08-cffc9dc3e9f8    DOX          2024-06-07   \n",
      "1  c32cbe5b-3c17-30a6-902f-50bf661413d8    DXC          2024-06-12   \n",
      "2  fccb6cab-5ac8-32b9-9ab2-d10bcf5fb56c    DRI          2024-07-18   \n",
      "3  666cac47-dfb3-3f18-a289-bb37221e14be    DOV          2024-07-24   \n",
      "4  a3f13a5a-830d-31eb-add2-5872918c73c7    DRS          2024-07-30   \n",
      "\n",
      "                                         description  \\\n",
      "0  Amdocs (DOX) reported earnings 30 days ago. Wh...   \n",
      "1  DXC Technology's (DXC) attractive valuation an...   \n",
      "2  Chuy's, established in Austin, Texas in 1982, ...   \n",
      "3  PSG Biotech, part of PSG and Dover (NYSE: DOV)...   \n",
      "4  Leonardo DRS, Inc. (DRS) delivered earnings an...   \n",
      "\n",
      "                                        article_link     article_pubDate  \\\n",
      "0  https://finance.yahoo.com/news/amdocs-dox-down... 2024-06-07 11:30:48   \n",
      "1  https://finance.yahoo.com/news/why-dxc-technol... 2024-06-12 04:37:00   \n",
      "2  https://www.verdictfoodservice.com/news/darden... 2024-07-18 04:16:16   \n",
      "3  https://finance.yahoo.com/news/psg-biotech-lau... 2024-07-24 16:15:00   \n",
      "4  https://finance.yahoo.com/news/leonardo-drs-in... 2024-07-30 08:40:06   \n",
      "\n",
      "                                       article_title language  \\\n",
      "0  Amdocs (DOX) Down 6.1% Since Last Earnings Rep...    en-US   \n",
      "1  Why Is DXC Technology (DXC) on the Acquisition...    en-US   \n",
      "2  Darden Restaurants to acquire Chuyâs Holding...    en-US   \n",
      "3  PSG Biotech Launches New Integral Display Tran...    en-US   \n",
      "4  Leonardo DRS, Inc. (DRS) Surpasses Q2 Earnings...    en-US   \n",
      "\n",
      "        lastBuildDate                                link  \\\n",
      "0 2024-08-02 09:01:51  http://finance.yahoo.com/q/h?s=DOX   \n",
      "1 2024-08-02 09:01:43  http://finance.yahoo.com/q/h?s=DXC   \n",
      "2 2024-08-02 09:01:54  http://finance.yahoo.com/q/h?s=DRI   \n",
      "3 2024-08-02 09:01:31  http://finance.yahoo.com/q/h?s=DOV   \n",
      "4 2024-08-02 09:01:30  http://finance.yahoo.com/q/h?s=DRS   \n",
      "\n",
      "                      title  \n",
      "0  Yahoo! Finance: DOX News  \n",
      "1  Yahoo! Finance: DXC News  \n",
      "2  Yahoo! Finance: DRI News  \n",
      "3  Yahoo! Finance: DOV News  \n",
      "4  Yahoo! Finance: DRS News  \n",
      "\n",
      "Loading data from table: Company_Info_News (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1298 entries, 0 to 1297\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ticker       1298 non-null   object\n",
      " 1   name         1298 non-null   object\n",
      " 2   subindustry  1298 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 30.6+ KB\n",
      "\n",
      "Head:\n",
      "  ticker                    name subindustry\n",
      "0    DRI  Darden Restaurants Inc    25301040\n",
      "1   INGR  Ingredion Incorporated    30202010\n",
      "2    PPL                PPL Corp    55101010\n",
      "3   FSLR         First Solar Inc    45301020\n",
      "4      J    Jacobs Solutions Inc    20202020\n",
      "\n",
      "Loading data from table: Market_Article_Summary (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 325 entries, 0 to 324\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   trading_date   325 non-null    datetime64[us]\n",
      " 1   article_count  325 non-null    int32         \n",
      "dtypes: datetime64[us](1), int32(1)\n",
      "memory usage: 3.9 KB\n",
      "\n",
      "Head:\n",
      "  trading_date  article_count\n",
      "0   2023-12-15            481\n",
      "1   2024-02-29            954\n",
      "2   2024-03-07            814\n",
      "3   2024-03-18           1173\n",
      "4   2024-05-21           1028\n",
      "\n",
      "Loading data from table: Market_Data_Daily_Processing (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228448 entries, 0 to 228447\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   trading_day_date  228448 non-null  datetime64[us]\n",
      " 1   ticker            228448 non-null  object        \n",
      " 2   price             227684 non-null  float32       \n",
      " 3   volume            227684 non-null  float64       \n",
      "dtypes: datetime64[us](1), float32(1), float64(1), object(1)\n",
      "memory usage: 6.1+ MB\n",
      "\n",
      "Head:\n",
      "  trading_day_date ticker      price     volume\n",
      "0       2024-02-05   EXEL  21.400000  1182448.0\n",
      "1       2024-02-06   EXEL  21.830000  1910829.0\n",
      "2       2024-02-07   EXEL  20.180000  3376200.0\n",
      "3       2024-02-08   EXEL  20.170000  3079018.0\n",
      "4       2024-02-09   EXEL  20.219999  2423668.0\n",
      "\n",
      "Loading data from table: Market_Data_Headlines (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228448 entries, 0 to 228447\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   trading_day_date  228448 non-null  datetime64[us]\n",
      " 1   ticker            228448 non-null  object        \n",
      " 2   price             227684 non-null  float32       \n",
      " 3   volume            227684 non-null  float64       \n",
      " 4   headline_count    228448 non-null  int32         \n",
      "dtypes: datetime64[us](1), float32(1), float64(1), int32(1), object(1)\n",
      "memory usage: 7.0+ MB\n",
      "\n",
      "Head:\n",
      "  trading_day_date ticker       price     volume  headline_count\n",
      "0       2024-03-18   CRWD  321.059998  2729961.0               4\n",
      "1       2024-05-08    CRM  278.970001  3282656.0               5\n",
      "2       2024-08-05   CROX  121.050003  1820840.0               4\n",
      "3       2024-05-17    CSX   33.520000  8218481.0               1\n",
      "4       2024-07-10   CSGP   71.839996  3584276.0               1\n",
      "\n",
      "Loading data from table: Pricing_News (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228448 entries, 0 to 228447\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   trading_day_date  228448 non-null  datetime64[us]\n",
      " 1   ticker            228448 non-null  object        \n",
      " 2   price             227684 non-null  float32       \n",
      "dtypes: datetime64[us](1), float32(1), object(1)\n",
      "memory usage: 4.4+ MB\n",
      "\n",
      "Head:\n",
      "  trading_day_date ticker       price\n",
      "0       2023-12-01    DRI  159.910004\n",
      "1       2023-12-04    DRI  161.000000\n",
      "2       2023-12-05    DRI  160.039993\n",
      "3       2023-12-06    DRI  161.259995\n",
      "4       2023-12-07    DRI  161.089996\n",
      "\n",
      "Loading data from table: Trading_Calendar (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 177 entries, 0 to 176\n",
      "Data columns (total 1 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   trading_date  177 non-null    datetime64[us]\n",
      "dtypes: datetime64[us](1)\n",
      "memory usage: 1.5 KB\n",
      "\n",
      "Head:\n",
      "  trading_date\n",
      "0   2023-12-01\n",
      "1   2023-12-04\n",
      "2   2023-12-05\n",
      "3   2023-12-06\n",
      "4   2023-12-07\n",
      "\n",
      "Loading data from table: Volume_News (Schema: Headlines)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 229746 entries, 0 to 229745\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   trading_day_date  229746 non-null  datetime64[us]\n",
      " 1   ticker            229746 non-null  object        \n",
      " 2   volume            228970 non-null  float64       \n",
      "dtypes: datetime64[us](1), float64(1), object(1)\n",
      "memory usage: 5.3+ MB\n",
      "\n",
      "Head:\n",
      "  trading_day_date ticker     volume\n",
      "0       2023-12-01    DRI   803319.0\n",
      "1       2023-12-04    DRI  1026513.0\n",
      "2       2023-12-05    DRI   725093.0\n",
      "3       2023-12-06    DRI   749486.0\n",
      "4       2023-12-07    DRI   810034.0\n",
      "\n",
      "Loading data from table: Company_Info (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1010 entries, 0 to 1009\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   cik          1010 non-null   object\n",
      " 1   ticker       1010 non-null   object\n",
      " 2   name         1010 non-null   object\n",
      " 3   subindustry  1010 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.7+ KB\n",
      "\n",
      "Head:\n",
      "       cik ticker                 name subindustry\n",
      "0   738076   COMS            3Com Corp    45201020\n",
      "1    66740    MMM                3M Co    20105010\n",
      "2    91142    AOS     A. O. Smith Corp    20102010\n",
      "3     1800    ABT  Abbott Laboratories    35101010\n",
      "4  1551152   ABBV           AbbVie Inc    35201010\n",
      "\n",
      "Loading data from table: item1 (Schema: SP500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4996b92db57f4ec7ba4312ecf43fbc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14996 entries, 0 to 14995\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   company    14996 non-null  object        \n",
      " 1   filing_ts  14996 non-null  datetime64[us]\n",
      " 2   link       14996 non-null  object        \n",
      " 3   type       14996 non-null  object        \n",
      " 4   cik        14996 non-null  object        \n",
      " 5   item1      14996 non-null  object        \n",
      "dtypes: datetime64[us](1), object(5)\n",
      "memory usage: 703.1+ KB\n",
      "\n",
      "Head:\n",
      "  company           filing_ts  \\\n",
      "0   3M CO 2022-02-09 20:13:29   \n",
      "1   3M CO 2021-02-04 18:53:11   \n",
      "2   3M CO 2020-02-06 21:16:31   \n",
      "3   3M CO 2019-02-07 22:15:37   \n",
      "4   3M CO 2018-02-08 22:14:52   \n",
      "\n",
      "                                                link  type    cik  \\\n",
      "0  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "1  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "2  https://www.sec.gov/ix?doc=/Archives/edgar/dat...  10-K  66740   \n",
      "3  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "4  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "\n",
      "                                               item1  \n",
      "0   Item 1. Business. 3M Company was incorporated...  \n",
      "1   Item 1. Busines s.  3M Company was incorporat...  \n",
      "2   Item 1. Busines s.  3M Company was incorporat...  \n",
      "3   Item 1. Busines s.  3M Company was incorporat...  \n",
      "4   Item 1. Busines s.  3M Company was incorporat...  \n",
      "\n",
      "Loading data from table: item1a (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14996 entries, 0 to 14995\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   company    14996 non-null  object        \n",
      " 1   filing_ts  14996 non-null  datetime64[us]\n",
      " 2   link       14996 non-null  object        \n",
      " 3   type       14996 non-null  object        \n",
      " 4   cik        14996 non-null  object        \n",
      " 5   item1a     14996 non-null  object        \n",
      "dtypes: datetime64[us](1), object(5)\n",
      "memory usage: 703.1+ KB\n",
      "\n",
      "Head:\n",
      "  company           filing_ts  \\\n",
      "0   3M CO 2022-02-09 20:13:29   \n",
      "1   3M CO 2021-02-04 18:53:11   \n",
      "2   3M CO 2020-02-06 21:16:31   \n",
      "3   3M CO 2019-02-07 22:15:37   \n",
      "4   3M CO 2018-02-08 22:14:52   \n",
      "\n",
      "                                                link  type    cik  \\\n",
      "0  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "1  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "2  https://www.sec.gov/ix?doc=/Archives/edgar/dat...  10-K  66740   \n",
      "3  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "4  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "\n",
      "                                              item1a  \n",
      "0   Item 1A. Risk Factors Provided below is a cau...  \n",
      "1   Item 1A. Risk Factors  Provided below is a ca...  \n",
      "2   Item 1A. Risk Factors  Provided below is a ca...  \n",
      "3   Item 1A. Risk Factors  Provided below is a ca...  \n",
      "4   Item 1A. Risk Factor s.  Provided below is a ...  \n",
      "\n",
      "Loading data from table: item7 (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14996 entries, 0 to 14995\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   company    14996 non-null  object        \n",
      " 1   filing_ts  14996 non-null  datetime64[us]\n",
      " 2   link       14996 non-null  object        \n",
      " 3   type       14996 non-null  object        \n",
      " 4   cik        14996 non-null  object        \n",
      " 5   item7      14996 non-null  object        \n",
      "dtypes: datetime64[us](1), object(5)\n",
      "memory usage: 703.1+ KB\n",
      "\n",
      "Head:\n",
      "  company           filing_ts  \\\n",
      "0   3M CO 2022-02-09 20:13:29   \n",
      "1   3M CO 2021-02-04 18:53:11   \n",
      "2   3M CO 2020-02-06 21:16:31   \n",
      "3   3M CO 2019-02-07 22:15:37   \n",
      "4   3M CO 2018-02-08 22:14:52   \n",
      "\n",
      "                                                link  type    cik  \\\n",
      "0  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "1  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "2  https://www.sec.gov/ix?doc=/Archives/edgar/dat...  10-K  66740   \n",
      "3  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "4  https://www.sec.gov/Archives/edgar/data/66740/...  10-K  66740   \n",
      "\n",
      "                                               item7  \n",
      "0   Item 7. Managements Discussion and Analysis o...  \n",
      "1   Item 7. Managements Discussion and Analysis o...  \n",
      "2   Item 7. Managements Discussion and Analysis o...  \n",
      "3   Item 7. Managements Discussion and Analysis o...  \n",
      "4   Item 7. Managements Discussion and Analysis o...  \n",
      "\n",
      "Loading data from table: Price_Daily (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5423582 entries, 0 to 5423581\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Dtype         \n",
      "---  ------            -----         \n",
      " 0   trading_day_date  datetime64[us]\n",
      " 1   cik               object        \n",
      " 2   price             float32       \n",
      "dtypes: datetime64[us](1), float32(1), object(1)\n",
      "memory usage: 103.4+ MB\n",
      "\n",
      "Head:\n",
      "  trading_day_date      cik  price\n",
      "0       2000-01-03  1534701    NaN\n",
      "1       2000-01-04  1534701    NaN\n",
      "2       2000-01-05  1534701    NaN\n",
      "3       2000-01-06  1534701    NaN\n",
      "4       2000-01-07  1534701    NaN\n",
      "\n",
      "Loading data from table: Price_Weekly (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   trading_week_date  0 non-null      datetime64[us]\n",
      " 1   cik                0 non-null      object        \n",
      " 2   price              0 non-null      float32       \n",
      "dtypes: datetime64[us](1), float32(1), object(1)\n",
      "memory usage: 132.0+ bytes\n",
      "\n",
      "Head:\n",
      "Empty DataFrame\n",
      "Columns: [trading_week_date, cik, price]\n",
      "Index: []\n",
      "\n",
      "Loading data from table: Price_Weekly_SP500 (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1214 entries, 0 to 1213\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   trading_week_date   1214 non-null   datetime64[us]\n",
      " 1   SP500CapWeighted    1214 non-null   float32       \n",
      " 2   SP500EqualWeighted  1214 non-null   float32       \n",
      "dtypes: datetime64[us](1), float32(2)\n",
      "memory usage: 19.1 KB\n",
      "\n",
      "Head:\n",
      "  trading_week_date  SP500CapWeighted  SP500EqualWeighted\n",
      "0        2000-01-07       1441.469971         1429.777832\n",
      "1        2000-01-14       1465.150024         1440.599854\n",
      "2        2000-01-21       1441.359985         1405.479858\n",
      "3        2000-01-28       1360.160034         1337.249878\n",
      "4        2000-02-04       1424.369995         1377.540894\n",
      "\n",
      "Loading data from table: SEC_Item_Filings (Schema: SP500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db8da713ac64d18857d5931d1ea7387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44988 entries, 0 to 44987\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cik               44988 non-null  object        \n",
      " 1   filing_ts         44988 non-null  datetime64[us]\n",
      " 2   item_filing       44988 non-null  object        \n",
      " 3   company           44988 non-null  object        \n",
      " 4   link              44988 non-null  object        \n",
      " 5   type              44988 non-null  object        \n",
      " 6   item_description  44988 non-null  object        \n",
      "dtypes: datetime64[us](1), object(6)\n",
      "memory usage: 2.4+ MB\n",
      "\n",
      "Head:\n",
      "     cik           filing_ts item_filing company  \\\n",
      "0  66740 2022-02-09 20:13:29           7   3M CO   \n",
      "1  66740 2021-02-04 18:53:11           7   3M CO   \n",
      "2  66740 2020-02-06 21:16:31           7   3M CO   \n",
      "3  66740 2019-02-07 22:15:37           7   3M CO   \n",
      "4  66740 2018-02-08 22:14:52           7   3M CO   \n",
      "\n",
      "                                                link  type  \\\n",
      "0  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
      "1  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
      "2  https://www.sec.gov/ix?doc=/Archives/edgar/dat...  10-K   \n",
      "3  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
      "4  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
      "\n",
      "                                    item_description  \n",
      "0   Item 7. Managements Discussion and Analysis o...  \n",
      "1   Item 7. Managements Discussion and Analysis o...  \n",
      "2   Item 7. Managements Discussion and Analysis o...  \n",
      "3   Item 7. Managements Discussion and Analysis o...  \n",
      "4   Item 7. Managements Discussion and Analysis o...  \n",
      "\n",
      "Loading data from table: Volume_Weekly (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4774662 entries, 0 to 4774661\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Dtype         \n",
      "---  ------             -----         \n",
      " 0   trading_week_date  datetime64[us]\n",
      " 1   cik                object        \n",
      " 2   volume             float32       \n",
      "dtypes: datetime64[us](1), float32(1), object(1)\n",
      "memory usage: 91.1+ MB\n",
      "\n",
      "Head:\n",
      "  trading_week_date      cik  volume\n",
      "0        2000-01-07  1534701     NaN\n",
      "1        2000-01-14  1534701     NaN\n",
      "2        2000-01-21  1534701     NaN\n",
      "3        2000-01-28  1534701     NaN\n",
      "4        2000-02-04  1534701     NaN\n",
      "\n",
      "Loading data from table: Weekly_Market_Data (Schema: SP500)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   trading_week_date  0 non-null      datetime64[us]\n",
      " 1   cik                0 non-null      object        \n",
      " 2   price              0 non-null      float32       \n",
      " 3   volume             0 non-null      float32       \n",
      "dtypes: datetime64[us](1), float32(2), object(1)\n",
      "memory usage: 132.0+ bytes\n",
      "\n",
      "Head:\n",
      "Empty DataFrame\n",
      "Columns: [trading_week_date, cik, price, volume]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Get the list of tables\n",
    "tables = conn.execute(\"SELECT table_schema, table_name FROM information_schema.tables\").fetchall()\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each table and load into a DataFrame\n",
    "for schema, table in tables:\n",
    "    full_table_name = f'\"{schema}\".\"{table}\"' if schema != 'main' else f'\"{table}\"'\n",
    "    \n",
    "    print(f\"\\nLoading data from table: {table} (Schema: {schema})\")\n",
    "    \n",
    "    # Load table into a DataFrame\n",
    "    df = conn.execute(f\"SELECT * FROM {full_table_name}\").fetchdf()\n",
    "    \n",
    "    # Store in dictionary\n",
    "    dataframes[table] = df\n",
    "    \n",
    "    # Print dataframe info and first few rows\n",
    "    print(\"\\nInfo:\")\n",
    "    df.info()\n",
    "    print(\"\\nHead:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Seq_num</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Strong_Modal</th>\n",
       "      <th>Weak_Modal</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>664</td>\n",
       "      <td>2.690000e-08</td>\n",
       "      <td>1.860000e-08</td>\n",
       "      <td>4.050000e-06</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.210000e-10</td>\n",
       "      <td>8.230000e-12</td>\n",
       "      <td>9.020000e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3.640000e-10</td>\n",
       "      <td>1.110000e-10</td>\n",
       "      <td>5.160000e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1.170000e-09</td>\n",
       "      <td>6.330000e-10</td>\n",
       "      <td>1.560000e-07</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>9349</td>\n",
       "      <td>3.790000e-07</td>\n",
       "      <td>3.830000e-07</td>\n",
       "      <td>3.460000e-05</td>\n",
       "      <td>1239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Seq_num  Word Count  Word Proportion  Average Proportion  \\\n",
       "0   AARDVARK        1         664     2.690000e-08        1.860000e-08   \n",
       "1  AARDVARKS        2           3     1.210000e-10        8.230000e-12   \n",
       "2      ABACI        3           9     3.640000e-10        1.110000e-10   \n",
       "3      ABACK        4          29     1.170000e-09        6.330000e-10   \n",
       "4     ABACUS        5        9349     3.790000e-07        3.830000e-07   \n",
       "\n",
       "        Std Dev  Doc Count  Negative  Positive  Uncertainty  Litigious  \\\n",
       "0  4.050000e-06        131         0         0            0          0   \n",
       "1  9.020000e-09          1         0         0            0          0   \n",
       "2  5.160000e-08          7         0         0            0          0   \n",
       "3  1.560000e-07         28         0         0            0          0   \n",
       "4  3.460000e-05       1239         0         0            0          0   \n",
       "\n",
       "   Strong_Modal  Weak_Modal  Constraining  Complexity  Syllables     Source  \n",
       "0             0           0             0           0          2  12of12inf  \n",
       "1             0           0             0           0          2  12of12inf  \n",
       "2             0           0             0           0          3  12of12inf  \n",
       "3             0           0             0           0          2  12of12inf  \n",
       "4             0           0             0           0          3  12of12inf  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Loughran-McDonald Master Dictionary\n",
    "file_path = r\"C:\\Users\\btada\\Documents\\Loughran-McDonald_MasterDictionary_1993-2023.csv\"\n",
    "df_dict = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "df_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandonments',\n",
       " 'abandons',\n",
       " 'abdicated',\n",
       " 'abdicates',\n",
       " 'abdicating',\n",
       " 'abdication',\n",
       " 'abdications',\n",
       " 'aberrant',\n",
       " 'aberration',\n",
       " 'aberrational',\n",
       " 'aberrations',\n",
       " 'abetting',\n",
       " 'abeyance',\n",
       " 'abeyances',\n",
       " 'abnormal',\n",
       " 'abnormalities']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter words where any of the key columns have nonzero values (indicating high-risk classification)\n",
    "high_risk_words = df_dict[\n",
    "    (df_dict[\"Negative\"] != 0) | (df_dict[\"Uncertainty\"] != 0) | (df_dict[\"Litigious\"] != 0)\n",
    "][\"Word\"].str.lower().tolist()\n",
    "\n",
    "# Display a sample of high-risk words\n",
    "high_risk_words[:20]  # Show the first 20 words for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543ad9cc39204899b23ad19923d15d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 228448 entries, 0 to 228447\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   trading_day_date  228448 non-null  datetime64[us]\n",
      " 1   ticker            228448 non-null  object        \n",
      " 2   price             227684 non-null  float32       \n",
      " 3   volume            227684 non-null  float64       \n",
      "dtypes: datetime64[us](1), float32(1), float64(1), object(1)\n",
      "memory usage: 6.1+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1214 entries, 0 to 1213\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   trading_week_date   1214 non-null   datetime64[us]\n",
      " 1   SP500CapWeighted    1214 non-null   float32       \n",
      " 2   SP500EqualWeighted  1214 non-null   float32       \n",
      "dtypes: datetime64[us](1), float32(2)\n",
      "memory usage: 19.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                   guid ticker  \\\n",
       " 0  76ceb11d-33eb-3af8-82f1-74e4068911f5      A   \n",
       " 1  56dc485e-c740-3fcc-ab3a-4e0d707a8f4d      A   \n",
       " 2  367bed80-8d07-3dce-8092-fd53d70578fe      A   \n",
       " 3  7bf92827-a505-3d56-98a3-4c9d60794e64      A   \n",
       " 4  8e5bdc52-73a9-30b1-ae97-493cd82da360      A   \n",
       " \n",
       "                                          description  \\\n",
       " 0  Agilent (A) adds a water immersion and confoca...   \n",
       " 1  SANTA CLARA, Calif., December 07, 2023--Agilen...   \n",
       " 2  Artisan Partners, an investment management com...   \n",
       " 3  Generally speaking the aim of active stock pic...   \n",
       " 4  SANTA CLARA, Calif., December 04, 2023--Agilen...   \n",
       " \n",
       "                                         article_link     article_pubDate  \\\n",
       " 0  https://finance.yahoo.com/news/agilent-enhance... 2023-12-07 09:46:00   \n",
       " 1  https://finance.yahoo.com/news/agilent-resolve... 2023-12-07 08:00:00   \n",
       " 2  https://finance.yahoo.com/news/why-artisan-par... 2023-12-06 02:46:05   \n",
       " 3  https://finance.yahoo.com/news/agilent-technol... 2023-12-05 06:00:35   \n",
       " 4  https://finance.yahoo.com/news/agilent-biotek-... 2023-12-04 08:00:00   \n",
       " \n",
       "                                        article_title language  \\\n",
       " 0  Agilent (A) Enhances BioTek Cytation C10 With ...    en-US   \n",
       " 1  Agilent Resolve Raman Receives Multiple Recogn...    en-US   \n",
       " 2  Hereâs Why Artisan Partners Mid Cap Fund Har...    en-US   \n",
       " 3  Agilent Technologies' (NYSE:A) 14% CAGR outpac...    en-US   \n",
       " 4  Agilent BioTek Cytation C10 Confocal Imaging R...    en-US   \n",
       " \n",
       "         lastBuildDate                              link  \\\n",
       " 0 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
       " 1 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
       " 2 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
       " 3 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
       " 4 2023-12-08 13:16:21  http://finance.yahoo.com/q/h?s=A   \n",
       " \n",
       "                     title  \n",
       " 0  Yahoo! Finance: A News  \n",
       " 1  Yahoo! Finance: A News  \n",
       " 2  Yahoo! Finance: A News  \n",
       " 3  Yahoo! Finance: A News  \n",
       " 4  Yahoo! Finance: A News  ,\n",
       "      cik           filing_ts item_filing company  \\\n",
       " 0  66740 2022-02-09 20:13:29           7   3M CO   \n",
       " 1  66740 2021-02-04 18:53:11           7   3M CO   \n",
       " 2  66740 2020-02-06 21:16:31           7   3M CO   \n",
       " 3  66740 2019-02-07 22:15:37           7   3M CO   \n",
       " 4  66740 2018-02-08 22:14:52           7   3M CO   \n",
       " \n",
       "                                                 link  type  \\\n",
       " 0  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
       " 1  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
       " 2  https://www.sec.gov/ix?doc=/Archives/edgar/dat...  10-K   \n",
       " 3  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
       " 4  https://www.sec.gov/Archives/edgar/data/66740/...  10-K   \n",
       " \n",
       "                                     item_description  \n",
       " 0   Item 7. Managements Discussion and Analysis o...  \n",
       " 1   Item 7. Managements Discussion and Analysis o...  \n",
       " 2   Item 7. Managements Discussion and Analysis o...  \n",
       " 3   Item 7. Managements Discussion and Analysis o...  \n",
       " 4   Item 7. Managements Discussion and Analysis o...  ,\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load relevant tables containing textual data\n",
    "articles_df = conn.execute(\"SELECT * FROM Headlines.Articles\").fetchdf()\n",
    "sec_filings_df = conn.execute(\"SELECT * FROM SP500.SEC_Item_Filings\").fetchdf()\n",
    "\n",
    "\n",
    "# Load relevant tables containing textual data\n",
    "market_daily_df = conn.execute(\"SELECT * FROM Headlines.Market_Data_Daily_Processing\").fetchdf()\n",
    "sp_index_df = conn.execute(\"SELECT * FROM SP500.Price_Weekly_SP500\").fetchdf()\n",
    "\n",
    "# Display a sample of the loaded data\n",
    "articles_df.head(), sec_filings_df.head(), market_daily_df.info(), sp_index_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btada\\AppData\\Local\\Temp\\ipykernel_29416\\4283390692.py:17: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  articles_df[[\"cleaned_title\", \"cleaned_description\"]] = articles_df[[\"article_title\", \"description\"]].fillna(\"\").applymap(clean_text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('could', 27997),\n",
       "  ('may', 22185),\n",
       "  ('against', 13978),\n",
       "  ('might', 8907),\n",
       "  ('closed', 5756),\n",
       "  ('cut', 5586),\n",
       "  ('challenges', 5400),\n",
       "  ('decline', 5016),\n",
       "  ('nearly', 4722),\n",
       "  ('risk', 4413),\n",
       "  ('closing', 4377),\n",
       "  ('loss', 4326),\n",
       "  ('miss', 4286),\n",
       "  ('worst', 4093),\n",
       "  ('late', 3890),\n",
       "  ('concerns', 3784),\n",
       "  ('contract', 3560),\n",
       "  ('volatility', 3072),\n",
       "  ('court', 3011),\n",
       "  ('losses', 2645)],\n",
       " 0.020860139115045574)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import chain  # More efficient list flattening\n",
    "\n",
    "# Precompile translation table for punctuation removal\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Vectorized function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Avoid processing NaNs\n",
    "        return text.lower().translate(translator)\n",
    "    return \"\"\n",
    "\n",
    "# Apply text cleaning in one step\n",
    "articles_df[[\"cleaned_title\", \"cleaned_description\"]] = articles_df[[\"article_title\", \"description\"]].fillna(\"\").applymap(clean_text)\n",
    "\n",
    "# Tokenize using vectorized Pandas `.str.split()`\n",
    "articles_df[\"title_tokens\"] = articles_df[\"cleaned_title\"].str.split()\n",
    "articles_df[\"description_tokens\"] = articles_df[\"cleaned_description\"].str.split()\n",
    "\n",
    "# Flatten all words using `chain.from_iterable`\n",
    "all_words = list(chain.from_iterable(articles_df[\"title_tokens\"].dropna())) + \\\n",
    "            list(chain.from_iterable(articles_df[\"description_tokens\"].dropna()))\n",
    "\n",
    "# Count high-risk word occurrences efficiently\n",
    "high_risk_counts = Counter(word for word in all_words if word in high_risk_words)\n",
    "\n",
    "# Compute Risk Score (normalized count of high-risk words)\n",
    "total_words = len(all_words)\n",
    "risk_score = sum(high_risk_counts.values()) / total_words if total_words > 0 else 0\n",
    "\n",
    "# Display the top high-risk words and computed risk score\n",
    "high_risk_counts.most_common(20), risk_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "may              12112.383471\n",
      "could             8279.719298\n",
      "approximately     6372.431899\n",
      "risk              3960.678067\n",
      "regulations       3452.323338\n",
      "believe           3422.433610\n",
      "loss              3216.652283\n",
      "laws              3132.649623\n",
      "adversely         3026.447780\n",
      "impairment        2965.985837\n",
      "regulatory        2937.069210\n",
      "contracts         2707.913941\n",
      "risks             2605.192253\n",
      "losses            2396.414515\n",
      "adverse           2353.137494\n",
      "assumptions       2152.732892\n",
      "contract          2010.029761\n",
      "claims            1985.525997\n",
      "restructuring     1783.816232\n",
      "regulation        1704.993892\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert high-risk words into a set for fast lookup\n",
    "high_risk_set = set(high_risk_words)\n",
    "\n",
    "# Define vectorizer with high-risk words as vocabulary\n",
    "vectorizer = TfidfVectorizer(vocabulary=high_risk_set, lowercase=True, token_pattern=r\"\\b\\w+\\b\")\n",
    "\n",
    "# Transform SEC filings text into a sparse matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(sec_filings_df[\"item_description\"].fillna(\"\"))\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Compute total risk score (sum of TF-IDF values per document)\n",
    "sec_filings_df[\"risk_score\"] = tfidf_df.sum(axis=1)\n",
    "\n",
    "# Display top high-risk words (most frequent across all filings)\n",
    "word_frequencies = tfidf_df.sum().sort_values(ascending=False).head(20)\n",
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import time\n",
    "\n",
    "# #  Enable GPU for SpaCy\n",
    "# spacy.require_gpu()\n",
    "\n",
    "# #  Load SpaCy NLP model (now using GPU)\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# #  Function to extract Named Entities related to risk\n",
    "# def extract_risk_entities(text):\n",
    "#     if any(word in text for word in high_risk_words):  # Only process if high-risk words exist\n",
    "#         doc = nlp(text)  \n",
    "#         return [ent.text for ent in doc.ents if ent.label_ in [\"LAW\", \"MONEY\", \"ORG\", \"GPE\", \"EVENT\"]]\n",
    "#     return []  \n",
    "\n",
    "# #  Run SpaCy and measure runtime\n",
    "# start_time = time.time()\n",
    "# articles_df[\"spacy_risk_entities\"] = articles_df[\"cleaned_title\"].apply(extract_risk_entities)\n",
    "# spacy_runtime = time.time() - start_time\n",
    "\n",
    "# print(f\"SpaCy Processing Time with GPU: {spacy_runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import time\n",
    "\n",
    "# Load SpaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract Named Entities related to risk\n",
    "def extract_risk_entities(text):\n",
    "    if any(word in text for word in high_risk_words):  # Only process if high-risk words exist\n",
    "        doc = nlp(text)\n",
    "        return [ent.text for ent in doc.ents if ent.label_ in [\"LAW\", \"MONEY\", \"ORG\", \"GPE\", \"EVENT\"]]\n",
    "    return []  # Skip processing if no high-risk words are found\n",
    "\n",
    "# Run SpaCy and measure runtime\n",
    "start_time = time.time()\n",
    "articles_df[\"spacy_risk_entities\"] = articles_df[\"cleaned_title\"].apply(extract_risk_entities)\n",
    "spacy_runtime = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT Processing Time: 3155.49 seconds\n",
      "FinBERT detected 204046 high-risk articles\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['trading_day_date'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# ✅ Merge SpaCy & FinBERT risk scores into market data\u001b[39;00m\n\u001b[0;32m     50\u001b[0m articles_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinbert_negative\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m articles_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m market_daily_with_risk \u001b[38;5;241m=\u001b[39m market_daily_df\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m---> 52\u001b[0m     \u001b[43marticles_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrading_day_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mticker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspacy_risk_entities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinbert_negative\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     53\u001b[0m     on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrading_day_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     54\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# ✅ Ensure `spacy_risk_entities` is properly formatted before `.apply(len)`\u001b[39;00m\n\u001b[0;32m     58\u001b[0m market_daily_with_risk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy_risk_entities\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m market_daily_with_risk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy_risk_entities\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\btada\\Documents\\OMSAPracticum\\new_env\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\btada\\Documents\\OMSAPracticum\\new_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\btada\\Documents\\OMSAPracticum\\new_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['trading_day_date'] not in index\""
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Load FinBERT Model & Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.half()  # Use FP16 for faster inference\n",
    "\n",
    "# Optimized batch processing function\n",
    "def get_finbert_sentiments(text_list, batch_size=16):\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i : i + batch_size]  \n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        results.extend(scores.tolist())\n",
    "\n",
    "    return results  \n",
    "\n",
    "# ✅ Process entire dataset in one batch\n",
    "text_data = articles_df[\"cleaned_title\"].dropna().tolist()\n",
    "start_time = time.time()\n",
    "finbert_scores = get_finbert_sentiments(text_data, batch_size=16)\n",
    "finbert_runtime = time.time() - start_time\n",
    "print(f\"FinBERT Processing Time: {finbert_runtime:.2f} seconds\")\n",
    "\n",
    "# Convert results to DataFrame & merge correctly\n",
    "finbert_scores_df = pd.DataFrame(finbert_scores, columns=[\"negative\", \"neutral\", \"positive\"])\n",
    "articles_df = pd.concat([articles_df.reset_index(drop=True), finbert_scores_df], axis=1)\n",
    "\n",
    "# ✅ Compute % of articles marked as negative risk by FinBERT\n",
    "finbert_negative_count = (articles_df[\"negative\"] > 0.5).sum()\n",
    "print(f\"FinBERT detected {finbert_negative_count} high-risk articles\")\n",
    "\n",
    "# ✅ Merge SpaCy & FinBERT risk scores into market data\n",
    "articles_df[\"finbert_negative\"] = articles_df[\"negative\"].reset_index(drop=True)\n",
    "market_daily_with_risk = market_daily_df.merge(\n",
    "    articles_df[[\"trading_day_date\", \"ticker\", \"spacy_risk_entities\", \"finbert_negative\"]],\n",
    "    on=[\"trading_day_date\", \"ticker\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# ✅ Ensure `spacy_risk_entities` is properly formatted before `.apply(len)`\n",
    "market_daily_with_risk[\"spacy_risk_entities\"] = market_daily_with_risk[\"spacy_risk_entities\"].fillna(\"\").apply(len)\n",
    "\n",
    "# ✅ Compute correlation with stock returns\n",
    "market_daily_with_risk[\"return\"] = market_daily_with_risk.groupby(\"ticker\")[\"price\"].pct_change()\n",
    "correlation_spacy = market_daily_with_risk[\"spacy_risk_entities\"].corr(market_daily_with_risk[\"return\"])\n",
    "correlation_finbert = market_daily_with_risk[\"finbert_negative\"].corr(market_daily_with_risk[\"return\"])\n",
    "\n",
    "print(f\"Correlation (SpaCy Risk Entities vs. Market Returns): {correlation_spacy:.4f}\")\n",
    "print(f\"Correlation (FinBERT Negative Sentiment vs. Market Returns): {correlation_finbert:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (SpaCy Risk Entities vs. Market Returns): 0.0000\n",
      "Correlation (FinBERT Negative Sentiment vs. Market Returns): -0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btada\\AppData\\Local\\Temp\\ipykernel_29416\\654181502.py:14: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  market_daily_with_risk[\"return\"] = market_daily_with_risk.groupby(\"ticker\")[\"price\"].pct_change()\n"
     ]
    }
   ],
   "source": [
    "# Merge SpaCy & FinBERT risk scores into market data\n",
    "articles_df[\"finbert_negative\"] = articles_df[\"negative\"].reset_index(drop=True)\n",
    "market_daily_with_risk = market_daily_df.merge(\n",
    "    articles_df[[\"article_pubDate\", \"ticker\", \"spacy_risk_entities\", \"finbert_negative\"]],\n",
    "    left_on=[\"trading_day_date\", \"ticker\"],  # Use `trading_day_date` from market data\n",
    "    right_on=[\"article_pubDate\", \"ticker\"],  # Use `article_pubDate` from articles\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Ensure `spacy_risk_entities` is properly formatted\n",
    "market_daily_with_risk[\"spacy_risk_entities\"] = market_daily_with_risk[\"spacy_risk_entities\"].fillna(\"\").apply(len)\n",
    "\n",
    "# Compute correlation with stock returns\n",
    "market_daily_with_risk[\"return\"] = market_daily_with_risk.groupby(\"ticker\")[\"price\"].pct_change()\n",
    "correlation_spacy = market_daily_with_risk[\"spacy_risk_entities\"].corr(market_daily_with_risk[\"return\"])\n",
    "correlation_finbert = market_daily_with_risk[\"finbert_negative\"].corr(market_daily_with_risk[\"return\"])\n",
    "\n",
    "print(f\"Correlation (SpaCy Risk Entities vs. Market Returns): {correlation_spacy:.4f}\")\n",
    "print(f\"Correlation (FinBERT Negative Sentiment vs. Market Returns): {correlation_finbert:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagged Correlation (FinBERT Negative Sentiment vs. Market Returns): 0.0296\n",
      "                     trading_day_date     price    volume  article_pubDate  \\\n",
      "trading_day_date             1.000000  0.001699 -0.005365         0.008618   \n",
      "price                        0.001699  1.000000 -0.007750        -0.000997   \n",
      "volume                      -0.005365 -0.007750  1.000000         0.030580   \n",
      "article_pubDate              0.008618 -0.000997  0.030580         1.000000   \n",
      "spacy_risk_entities          0.003672 -0.000271  0.003786         0.288390   \n",
      "finbert_negative            -0.100182  0.078674 -0.101011        -0.095736   \n",
      "return                      -0.006188  0.000229  0.027392         0.001168   \n",
      "lagged_return               -0.060546 -0.001809  0.020466         0.001253   \n",
      "\n",
      "                     spacy_risk_entities  finbert_negative    return  \\\n",
      "trading_day_date                0.003672         -0.100182 -0.006188   \n",
      "price                          -0.000271          0.078674  0.000229   \n",
      "volume                          0.003786         -0.101011  0.027392   \n",
      "article_pubDate                 0.288390         -0.095736  0.001168   \n",
      "spacy_risk_entities             1.000000          0.017413  0.006713   \n",
      "finbert_negative                0.017413          1.000000 -0.115509   \n",
      "return                          0.006713         -0.115509  1.000000   \n",
      "lagged_return                   0.003012          0.045926 -0.074582   \n",
      "\n",
      "                     lagged_return  \n",
      "trading_day_date         -0.060546  \n",
      "price                    -0.001809  \n",
      "volume                    0.020466  \n",
      "article_pubDate           0.001253  \n",
      "spacy_risk_entities       0.003012  \n",
      "finbert_negative          0.045926  \n",
      "return                   -0.074582  \n",
      "lagged_return             1.000000  \n"
     ]
    }
   ],
   "source": [
    "market_daily_with_risk[\"lagged_return\"] = market_daily_with_risk.groupby(\"ticker\")[\"return\"].shift(-1)\n",
    "lagged_correlation = market_daily_with_risk[\"finbert_negative\"].corr(market_daily_with_risk[\"lagged_return\"])\n",
    "print(f\"Lagged Correlation (FinBERT Negative Sentiment vs. Market Returns): {lagged_correlation:.4f}\")\n",
    "\n",
    "\n",
    "# Ensure 'trading_day_date' is a datetime column\n",
    "market_daily_with_risk[\"trading_day_date\"] = pd.to_datetime(market_daily_with_risk[\"trading_day_date\"])\n",
    "\n",
    "# Group by 'ticker' and the week period of 'trading_day_date'\n",
    "weekly_sentiment = market_daily_with_risk.groupby(\n",
    "    [\"ticker\", market_daily_with_risk[\"trading_day_date\"].dt.to_period(\"W\")]\n",
    ").mean()\n",
    "\n",
    "# Print correlation matrix for weekly sentiment\n",
    "print(weekly_sentiment.corr())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_daily_with_risk[\"combined_risk_score\"] = (\n",
    "    market_daily_with_risk[\"spacy_risk_entities\"] +\n",
    "    market_daily_with_risk[\"finbert_negative\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Combined Risk Score vs. Market Returns): -0.0581\n"
     ]
    }
   ],
   "source": [
    "combined_correlation = market_daily_with_risk[\"combined_risk_score\"].corr(market_daily_with_risk[\"return\"])\n",
    "print(f\"Correlation (Combined Risk Score vs. Market Returns): {combined_correlation:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R² Score: -0.3893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and target\n",
    "features = market_daily_with_risk[[\"spacy_risk_entities\", \"finbert_negative\", \"combined_risk_score\"]]\n",
    "target = market_daily_with_risk[\"return\"]\n",
    "\n",
    "# Drop NaN rows from features or target\n",
    "features = features.dropna()\n",
    "target = target.loc[features.index]  # Align target with features\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_score = rf_model.score(X_test, y_test)\n",
    "print(f\"Random Forest R² Score: {rf_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation (Weighted Risk Score vs. Market Returns): -0.0508\n"
     ]
    }
   ],
   "source": [
    "market_daily_with_risk[\"weighted_risk_score\"] = (\n",
    "    0.7 * market_daily_with_risk[\"finbert_negative\"] +\n",
    "    0.3 * market_daily_with_risk[\"spacy_risk_entities\"]\n",
    ")\n",
    "weighted_correlation = market_daily_with_risk[\"weighted_risk_score\"].corr(market_daily_with_risk[\"return\"])\n",
    "print(f\"Correlation (Weighted Risk Score vs. Market Returns): {weighted_correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly Correlation (Combined Risk Score vs. Market Returns): 0.0067\n"
     ]
    }
   ],
   "source": [
    "weekly_data = market_daily_with_risk.groupby(\n",
    "    [market_daily_with_risk[\"trading_day_date\"].dt.to_period(\"W\"), \"ticker\"]\n",
    ").mean()\n",
    "weekly_correlation = weekly_data[\"combined_risk_score\"].corr(weekly_data[\"return\"])\n",
    "print(f\"Weekly Correlation (Combined Risk Score vs. Market Returns): {weekly_correlation:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
