{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Kh0HGLWq_0hMA3f7GCNYIazXOyC8GSQ0","authorship_tag":"ABX9TyODDv7ndo2wc4bVj843IK2W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"7nECbYHCG9Ah"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTHcrE1N1pZ7","executionInfo":{"status":"ok","timestamp":1737129344984,"user_tz":300,"elapsed":8477,"user":{"displayName":"Brian Adams","userId":"16135405832952140457"}},"outputId":"d7f342fb-78fd-4b6e-81df-e2a75d19388d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","\n","# Base folder paths\n","base_path = '/content/drive/MyDrive/OMSA Practicum'\n","sp500_path = os.path.join(base_path, 'SP500')\n","multicap_news_path = os.path.join(base_path, 'MultiCap_News')\n"],"metadata":{"id":"0cF4sOSm6Xke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Function to read all files in a folder\n","def read_files_from_folder(folder_path):\n","    files_data = {}\n","    for filename in os.listdir(folder_path):\n","        file_path = os.path.join(folder_path, filename)\n","        if filename.endswith('.csv'):\n","            files_data[filename] = pd.read_csv(file_path)\n","        elif filename.endswith('.txt'):\n","            with open(file_path, 'r') as file:\n","                files_data[filename] = file.read()\n","    return files_data\n","\n","# Read SP500 files\n","sp500_data = read_files_from_folder(sp500_path)\n","\n","# Read MultiCap_News files\n","multicap_news_data = read_files_from_folder(multicap_news_path)\n","\n","# Output examples\n","print(\"SP500 Data Keys:\", sp500_data.keys())\n","print(\"MultiCap News Data Keys:\", multicap_news_data.keys())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9SW5R1f6bKk","executionInfo":{"status":"ok","timestamp":1737132294184,"user_tz":300,"elapsed":20255,"user":{"displayName":"Brian Adams","userId":"16135405832952140457"}},"outputId":"a2578711-7a90-44ea-b4a8-42796f3a2dc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SP500 Data Keys: dict_keys(['price.csv', 'price_daily.csv', 'price_SP500.csv', 'company_info_sp500.txt', 'sp500_item1_sec_filings_0.txt', 'sp500_item1a_sec_filings_0.txt', 'sp500_item7_sec_filings_0.txt', 'volume.csv'])\n","MultiCap News Data Keys: dict_keys([])\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import io\n","\n","# Dictionary to store processed dataframes\n","processed_dataframes = {}\n","\n","# Function to process already loaded DataFrames\n","def handle_existing_dataframe(df, file_name):\n","    processed_dataframes[file_name] = df\n","    print(f\"Stored preloaded DataFrame '{file_name}' with shape {df.shape}\")\n","\n","# Function to process raw CSV files in chunks and store with names\n","def process_csv_in_chunks(file_content, file_name, chunksize=10000):\n","    try:\n","        chunk_iter = pd.read_csv(io.StringIO(file_content), chunksize=chunksize)\n","        for i, chunk in enumerate(chunk_iter):\n","            df_name = f\"{file_name}_chunk_{i+1}\"\n","            processed_dataframes[df_name] = chunk\n","            print(f\"Stored chunk {i + 1} of {file_name} as '{df_name}' with shape {chunk.shape}\")\n","    except Exception as e:\n","        print(f\"Error processing CSV in chunks for {file_name}: {e}\")\n","\n","# Function to process TXT files into DataFrames\n","def process_txt_to_dataframe(content, file_name, delimiter=None, chunksize=5000):\n","    try:\n","        if delimiter:\n","            # Process structured TXT file as a DataFrame\n","            chunk_iter = pd.read_csv(io.StringIO(content), delimiter=delimiter, chunksize=chunksize)\n","            for i, chunk in enumerate(chunk_iter):\n","                df_name = f\"{file_name}_chunk_{i+1}\"\n","                processed_dataframes[df_name] = chunk\n","                print(f\"Stored chunk {i + 1} of TXT file {file_name} as '{df_name}' with shape {chunk.shape}\")\n","        else:\n","            # Handle unstructured TXT files (store as lines for now)\n","            lines = content.splitlines()\n","            processed_dataframes[file_name] = pd.DataFrame({'line': lines})\n","            print(f\"Stored TXT file '{file_name}' as DataFrame with {len(lines)} lines\")\n","    except Exception as e:\n","        print(f\"Error processing TXT file {file_name}: {e}\")\n","\n","# Iterate through the keys in sp500_data\n","for key, content in sp500_data.items():\n","    print(f\"\\nExploring: {key}\\n\" + \"-\" * 50)\n","\n","    if isinstance(content, pd.DataFrame):\n","        # If already a DataFrame, handle directly\n","        handle_existing_dataframe(content, key)\n","    elif key.endswith('.csv'):\n","        # If CSV content is raw text, process it in chunks\n","        print(f\"Chunked processing for {key}:\")\n","        process_csv_in_chunks(content, key, chunksize=5000)\n","    elif key.endswith('.txt'):\n","        # Process TXT files\n","        print(f\"Processing TXT file {key}:\")\n","        delimiter = '|'  # Example: Adjust as needed based on your file's structure\n","        process_txt_to_dataframe(content, key, delimiter=delimiter, chunksize=5000)\n","    else:\n","        print(f\"Unsupported file format: {key}\")\n","\n","# Display the stored dataframes\n","print(\"\\nStored DataFrames:\")\n","for name, data in processed_dataframes.items():\n","    if isinstance(data, pd.DataFrame):\n","        print(f\"DataFrame '{name}' with shape {data.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8MeepCp-HYA","executionInfo":{"status":"ok","timestamp":1737132498188,"user_tz":300,"elapsed":37474,"user":{"displayName":"Brian Adams","userId":"16135405832952140457"}},"outputId":"278255f3-478e-4511-af28-1925ee42e948"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Exploring: price.csv\n","--------------------------------------------------\n","Stored preloaded DataFrame 'price.csv' with shape (1214, 926)\n","\n","Exploring: price_daily.csv\n","--------------------------------------------------\n","Stored preloaded DataFrame 'price_daily.csv' with shape (5857, 927)\n","\n","Exploring: price_SP500.csv\n","--------------------------------------------------\n","Stored preloaded DataFrame 'price_SP500.csv' with shape (1214, 3)\n","\n","Exploring: company_info_sp500.txt\n","--------------------------------------------------\n","Processing TXT file company_info_sp500.txt:\n","Stored chunk 1 of TXT file company_info_sp500.txt as 'company_info_sp500.txt_chunk_1' with shape (0, 3034)\n","\n","Exploring: sp500_item1_sec_filings_0.txt\n","--------------------------------------------------\n","Processing TXT file sp500_item1_sec_filings_0.txt:\n","Stored chunk 1 of TXT file sp500_item1_sec_filings_0.txt as 'sp500_item1_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","Stored chunk 2 of TXT file sp500_item1_sec_filings_0.txt as 'sp500_item1_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","Stored chunk 3 of TXT file sp500_item1_sec_filings_0.txt as 'sp500_item1_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","\n","Exploring: sp500_item1a_sec_filings_0.txt\n","--------------------------------------------------\n","Processing TXT file sp500_item1a_sec_filings_0.txt:\n","Stored chunk 1 of TXT file sp500_item1a_sec_filings_0.txt as 'sp500_item1a_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","Stored chunk 2 of TXT file sp500_item1a_sec_filings_0.txt as 'sp500_item1a_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","Stored chunk 3 of TXT file sp500_item1a_sec_filings_0.txt as 'sp500_item1a_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","\n","Exploring: sp500_item7_sec_filings_0.txt\n","--------------------------------------------------\n","Processing TXT file sp500_item7_sec_filings_0.txt:\n","Stored chunk 1 of TXT file sp500_item7_sec_filings_0.txt as 'sp500_item7_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","Stored chunk 2 of TXT file sp500_item7_sec_filings_0.txt as 'sp500_item7_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","Stored chunk 3 of TXT file sp500_item7_sec_filings_0.txt as 'sp500_item7_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","\n","Exploring: volume.csv\n","--------------------------------------------------\n","Stored preloaded DataFrame 'volume.csv' with shape (1214, 3934)\n","\n","Stored DataFrames:\n","DataFrame 'price.csv' with shape (1214, 926)\n","DataFrame 'price_daily.csv' with shape (5857, 927)\n","DataFrame 'price_SP500.csv' with shape (1214, 3)\n","DataFrame 'company_info_sp500.txt_chunk_1' with shape (0, 3034)\n","DataFrame 'sp500_item1_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","DataFrame 'sp500_item1_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","DataFrame 'sp500_item1_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","DataFrame 'sp500_item1a_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","DataFrame 'sp500_item1a_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","DataFrame 'sp500_item1a_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","DataFrame 'sp500_item7_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","DataFrame 'sp500_item7_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","DataFrame 'sp500_item7_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","DataFrame 'volume.csv' with shape (1214, 3934)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"UTxcFtq3-HLW"}},{"cell_type":"markdown","source":[],"metadata":{"id":"VaNpP2O9Dc0o"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Function to perform EDA on a single DataFrame\n","def perform_eda(df, name):\n","    print(f\"\\nEDA for '{name}'\\n\" + \"-\" * 50)\n","\n","    # 1. Basic Information\n","    print(f\"Shape: {df.shape}\")\n","    print(f\"Columns: {df.columns.tolist()}\\n\")\n","    print(f\"Data Types:\\n{df.dtypes}\\n\")\n","\n","    # Missing values\n","    print(f\"Missing values:\\n{df.isnull().sum()}\\n\")\n","\n","    # Duplicate rows\n","    duplicate_count = df.duplicated().sum()\n","    print(f\"Number of duplicate rows: {duplicate_count}\\n\")\n","\n","    # Descriptive statistics (limited to smaller datasets for better readability)\n","    if df.shape[1] <= 50:  # Limit columns for large DataFrames\n","        print(f\"Descriptive Statistics:\\n{df.describe(include='all')}\\n\")\n","\n","    # Memory usage\n","    memory_usage = df.memory_usage(deep=True).sum() / (1024 ** 2)\n","    print(f\"Memory Usage: {memory_usage:.2f} MB\\n\")\n","\n","    # # Correlation heatmap for numerical data\n","    # numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n","    # if len(numerical_cols) > 1:\n","    #     corr = df[numerical_cols].corr()\n","    #     plt.figure(figsize=(10, 8))\n","    #     sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n","    #     plt.title(f\"Correlation Matrix for {name}\")\n","    #     plt.show()\n","\n","    # Top and bottom rows\n","    print(f\"First 5 rows:\\n{df.head()}\\n\")\n","    print(f\"Last 5 rows:\\n{df.tail()}\\n\")\n","\n","# Display stored DataFrames\n","print(\"\\nStored DataFrames:\")\n","for name, data in processed_dataframes.items():\n","    if isinstance(data, pd.DataFrame):\n","        print(f\"DataFrame '{name}' with shape {data.shape}\")\n","\n","# Specify the DataFrame you want to analyze\n","selected_dataframe_name = 'company_info_sp500.txt_chunk_1'  # Change this to the desired DataFrame name\n","\n","# Perform EDA for the selected DataFrame\n","if selected_dataframe_name in processed_dataframes:\n","    selected_dataframe = processed_dataframes[selected_dataframe_name]\n","    if not selected_dataframe.empty:\n","        perform_eda(selected_dataframe, selected_dataframe_name)\n","    else:\n","        print(f\"The selected DataFrame '{selected_dataframe_name}' is empty.\")\n","else:\n","    print(f\"DataFrame '{selected_dataframe_name}' not found in processed data.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMqDy5-mI3XW","executionInfo":{"status":"ok","timestamp":1737133632967,"user_tz":300,"elapsed":135,"user":{"displayName":"Brian Adams","userId":"16135405832952140457"}},"outputId":"3e1e2157-e454-48c7-f1ed-8b6e340e038e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Stored DataFrames:\n","DataFrame 'price.csv' with shape (1214, 926)\n","DataFrame 'price_daily.csv' with shape (5857, 927)\n","DataFrame 'price_SP500.csv' with shape (1214, 3)\n","DataFrame 'company_info_sp500.txt_chunk_1' with shape (0, 3034)\n","DataFrame 'sp500_item1_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","DataFrame 'sp500_item1_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","DataFrame 'sp500_item1_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","DataFrame 'sp500_item1a_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","DataFrame 'sp500_item1a_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","DataFrame 'sp500_item1a_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","DataFrame 'sp500_item7_sec_filings_0.txt_chunk_1' with shape (5000, 6)\n","DataFrame 'sp500_item7_sec_filings_0.txt_chunk_2' with shape (5000, 6)\n","DataFrame 'sp500_item7_sec_filings_0.txt_chunk_3' with shape (4996, 6)\n","DataFrame 'volume.csv' with shape (1214, 3934)\n","The selected DataFrame 'company_info_sp500.txt_chunk_1' is empty.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ue04mqjmI8GQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **SP500 Datasets Overview**\n","\n","\n","---\n","\n","### **Overview of `price.csv`**\n","\n","#### **Structure**:\n","- **Rows**: 1,214  \n","- **Columns**: 926  \n","- **Key Columns**:\n","  - `Date`: Contains date information.\n","  - Company-specific columns (by CIK or ticker): Represent various financial metrics, primarily stock prices or related values.\n","\n","#### **Key Characteristics**:\n","- **Data Types**:\n","  - `Date`: `object`\n","  - Other columns: `float64`\n","- **Missing Data**:\n","  - Significant missing values in many columns, e.g., `1534701` has 640 missing values, `null` has 314 missing values.\n","  - No missing values in the `Date` column.\n","- **Duplicates**:\n","  - No duplicate rows were detected.\n","- **Memory Usage**:\n","  - 8.65 MB.\n","\n","#### **First and Last Rows**:\n","- Example from **first row** (2000-01-07):\n","  - `1534701`: `NaN`\n","  - `1341439`: `25.84375`\n","  - `null`: `50.07373`\n","  - `792985`: `17.8125`\n","- Example from **last row** (2023-04-06):\n","  - `1534701`: `102.84`\n","  - `1341439`: `95.92`\n","  - `null`: `NaN`\n","  - `792985`: `NaN`\n","\n","#### **Insights**:\n","- The dataset spans a long time period (2000–2023).\n","- The data contains stock prices (or related metrics) for multiple companies, indexed by their identifiers.\n","- Missing values might indicate periods of inactivity, non-trading days, or missing data points for specific companies.\n","\n","#### **Use**:\n","- **Trend Analysis**:\n","  - Track historical stock price trends for individual companies.\n","- **Market-Wide Insights**:\n","  - Compare stock performance across companies over time.\n","- **Event Impact Studies**:\n","  - Assess the impact of significant financial or global events on stock prices.\n","\n","#### **Challenges**:\n","1. **High Dimensionality**:\n","   - 926 columns make direct analysis and visualization difficult.\n","2. **Missing Values**:\n","   - Need to handle missing values effectively for accurate analysis.\n","3. **Time Period Coverage**:\n","   - Ensure that analysis considers consistent date ranges for comparisons.\n","\n","---\n","\n","### **Overview of `sp500_item1_sec_filings_0.txt`**\n","\n","#### **1. Structure**\n","- **Shape**:\n","  - **Chunk 1**: (5000, 6)\n","  - **Chunk 2**: (5000, 6)\n","  - **Chunk 3**: (4996, 6)\n","  - **Total Rows**: 14,996\n","  - **Columns**: ['company', 'date', 'link', 'type', 'cik', 'item1']\n","\n","#### **2. Data Characteristics**\n","- **Data Types**:\n","  - `company`: Object (String)\n","  - `date`: Object (Datetime-like String)\n","  - `link`: Object (String, URLs to SEC filings)\n","  - `type`: Object (String, e.g., \"10-K\")\n","  - `cik`: Integer (Central Index Key for companies)\n","  - `item1`: Object (String, Item 1 description text)\n","\n","#### **3. Missing Values**\n","- **`item1` Field**: Some missing values across all chunks:\n","  - **Chunk 1**: 99 missing rows\n","  - **Chunk 2**: 95 missing rows\n","  - **Chunk 3**: 80 missing rows\n","  - **Total Missing Rows**: 274 (approx. 1.83% of total rows)\n","\n","#### **4. Unique Values**\n","- **`company`**:\n","  - Total unique companies: 445 (Chunk 1), 444 (Chunk 2), 430 (Chunk 3)\n","- **`date`**:\n","  - Total unique dates: 4774 (Chunk 1), 4754 (Chunk 2), 4734 (Chunk 3)\n","- **`link`**:\n","  - Total unique links: Nearly unique across all chunks.\n","- **`type`**:\n","  - Consistently \"10-K\" across all chunks.\n","\n","#### **5. Memory Usage**\n","- **Chunk 1**: 245.74 MB\n","- **Chunk 2**: 245.16 MB\n","- **Chunk 3**: 227.96 MB\n","- **Total Memory Usage**: ~718.86 MB\n","\n","#### **6. Descriptive Statistics**\n","- **Most Frequent Company**:\n","  - Chunk 1: BERKSHIRE HATHAWAY INC (23 occurrences)\n","  - Chunk 2: NUCOR CORP (23 occurrences)\n","  - Chunk 3: O'REILLY AUTOMOTIVE INC (23 occurrences)\n","- **Most Frequent Item in `item1`**:\n","  - \"ITEM 1. BUSINESS\" appears as the most common text across all chunks.\n","\n","#### **7. Key Observations**\n","- **Consistency**: Data is clean with no duplicate rows and consistent column names across chunks.\n","- **Use**:\n","  - Analyzing trends in Item 1 descriptions.\n","  - Linking company filings to SEC events.\n","  - Exploring historical changes in business descriptions for individual companies.\n","\n","---\n","\n","### **Overview of `volume.csv`**\n","\n","- **Structure**:\n","  - `Date`: Represents the trading dates.\n","  - Company-specific columns (by CIK or ticker): Each column represents the trading volume for a specific company or security, measured in shares traded per day.\n","  - **Total Columns**: 3,934 including `Date`.\n","  - **Total Rows**: 1,214\n","\n","- **Key Characteristics**:\n","  - **Missing Values**: Vary significantly across columns; some have complete data while others have substantial gaps.\n","  - **No Duplicates**: The dataset does not contain duplicate rows.\n","  - **Memory Usage**: 36.51 MB, indicating a substantial dataset.\n","  - **Numerical Data**: Includes the trading volumes, which are numerical (float64).\n","  - **Date Range**: Extends from 2000-01-07 to 2023-04-06.\n","\n","- **Use**:\n","  - **Market Activity Analysis**: Evaluate liquidity and activity levels across different securities over time.\n","  - **Sector/Company-Specific Trends**: Identify patterns in trading volume for specific companies or industries.\n","  - **Volatility Assessment**: Use trading volume as a proxy for market sentiment or volatility spikes.\n","  - **Event Analysis**: Correlate significant market events with changes in trading volume for specific companies or the entire market.\n","\n","- **Potential Challenges**:\n","  - **High Dimensionality**: With 3,934 columns, visualization and analysis might require dimensionality reduction techniques or filtering for significant columns.\n","  - **Missing Data**: Requires imputation or handling strategies, especially for columns with substantial gaps.\n","  - **Performance**: Computational resource needs could escalate due to the dataset's size.\n","\n","---\n","\n","### **Overview of `price_daily.csv`**\n","\n","#### **1. Structure**\n","- **Shape**: (5857, 927)\n","  - Rows represent daily observations.\n","  - Columns include `Date`, company-specific metrics identified by unique IDs (CIKs or tickers), and S&P500 indices (`SP500CapWeighted` and `SP500EqualWeighted`).\n","\n","#### **2. Data Characteristics**\n","- **Data Types**:\n","  - `Date`: Object (String format, should be parsed as datetime for analysis).\n","  - Numerical Data: Float64, representing daily stock prices or metrics for various companies and indices.\n","  - 927 columns in total.\n","\n","#### **3. Missing Values**\n","- **Summary**:\n","  - Missing data is prevalent across many company-specific columns, reflecting data gaps or inactive periods.\n","  - Notable columns with no missing data:\n","    - `Date`\n","    - `SP500CapWeighted`\n","    - `SP500EqualWeighted`\n","    - Some major company-specific IDs.\n","  - Other columns have substantial missing values, such as:\n","    - `1534701`: 3,088 missing rows.\n","    - `792985`: 2,320 missing rows.\n","    - Total missing data varies widely by column.\n","\n","#### **4. Key Columns**\n","- **Indices**:\n","  - `SP500CapWeighted`: S&P 500 Capital-Weighted Index.\n","  - `SP500EqualWeighted`: S&P 500 Equal-Weighted Index.\n","- **Date**: Identifies daily observations.\n","- **Company-Specific Columns**:\n","  - Columns represent stock prices or related metrics tied to company identifiers (CIKs).\n","\n","#### **5. Memory Usage**\n","- Approximately 41.75 MB.\n","\n","#### **6. Descriptive Statistics**\n","- **Numerical Summary**:\n","  - Columns have varying ranges, depending on the company and type of metric.\n","  - Many metrics have a mean and standard deviation consistent with financial data.\n","  - Example:\n","    - `SP500CapWeighted`:\n","      - Mean: ~1899.35\n","      - Std Dev: ~987.55\n","      - Min: ~676.53\n","      - Max: ~4796.56\n","    - `SP500EqualWeighted`:\n","      - Mean: ~4427.23\n","      - Std Dev: ~3058.53\n","      - Min: ~1056.15\n","      - Max: ~12522.94\n","\n","#### **7. First and Last Observations**\n","- **First Date**: January 3, 2000.\n","- **Last Date**: April 13, 2023.\n","- Data spans over two decades of daily observations, providing a rich dataset for longitudinal analysis.\n","\n","#### **8. Key Observations**\n","- **Consistency**: No duplicate rows.\n","- **Use**:\n","  - **Market Analysis**:\n","    - Track daily market performance using S&P 500 indices.\n","  - **Company Performance**:\n","    - Analyze stock trends for individual companies.\n","  - **Volatility and Risk Assessment**:\n","    - Evaluate daily fluctuations in stock prices.\n","  - **Economic Event Correlation**:\n","    - Link market changes to macroeconomic or company-specific events."],"metadata":{"id":"OsEoFK56LuOQ"}},{"cell_type":"code","source":[],"metadata":{"id":"s_xS7632ReJb"},"execution_count":null,"outputs":[]}]}
