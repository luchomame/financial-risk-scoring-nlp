{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "CSV_OUTPUT_PATH = Path(os.getenv(\"CSV_OUTPUT_PATH\"))\n",
    "CSV_FILE = os.getenv(\"CLEANED_ARTICLES_FILENAME\")\n",
    "\n",
    "MODEL_OUTPUT_PATH = Path(os.getenv(\"MODEL_OUTPUT_PATH\"))\n",
    "NP_OUTPUT_PATH = Path(os.getenv(\"NP_OUTPUT_PATH\"))\n",
    "MODEL_FILE = os.getenv(\"MODEL_FILENAME\")\n",
    "EMBEDDED_ARTICLES_FILENAME = os.getenv(\"EMBEDDED_ARTICLES_FILENAME\")\n",
    "\n",
    "\n",
    "CLEANED_ARTICLES_FILENAME=os.getenv(\"CLEANED_ARTICLES_FILENAME\")\n",
    "EMBEDDED_ARTICLES_FILENAME=os.getenv(\"EMBEDDED_ARTICLES_FILENAME\")\n",
    "MODEL_FILENAME=os.getenv(\"MODEL_FILENAME\")\n",
    "GROUPED_RISK_TAGS_CSV=os.getenv(\"GROUPED_RISK_TAGS_CSV\")\n",
    "ARTICLES_WITH_RISK_TAGS_CSV=os.getenv(\"ARTICLES_WITH_RISK_TAGS_CSV\")\n",
    "LM_LEXICON_FILENAME=os.getenv(\"LM_LEXICON_FILENAME\")\n",
    "STOCK_RISK_EXPOSURE_CSV=os.getenv(\"STOCK_RISK_EXPOSURE_CSV\")\n",
    "HDBSCAN_SUBCLUSTER_LABELS=os.getenv(\"HDBSCAN_SUBCLUSTER_LABELS\")\n",
    "HDBSCAN_RISK_TAGS=os.getenv(\"HDBSCAN_RISK_TAGS\")\n",
    "HDBSCAN_GROUPED_RISK_TAGS=os.getenv(\"HDBSCAN_GROUPED_RISK_TAGS\")\n",
    "CLUSTER_FINBERT=os.getenv(\"CLUSTER_FINBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For HDBSCAN I only saved the cluster IDs not the vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved clusters\n",
    "data_with_clusters = np.load(f\"{CSV_OUTPUT_PATH}/{CLUSTER_FINBERT}\")\n",
    "# article_vectors = data_with_clusters[:, :-1]  # Extract article vectors\n",
    "cluster_labels = data_with_clusters  # Extract cluster labels\n",
    "\n",
    "# Count articles in each cluster\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "\n",
    "# Print cluster sizes\n",
    "# print(\"Cluster Distribution:\\n\", cluster_counts)\n",
    "\n",
    "# # Plot cluster sizes\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# cluster_counts.plot(kind=\"bar\", color=\"steelblue\")\n",
    "# plt.xlabel(\"Cluster ID\")\n",
    "# plt.ylabel(\"Number of Articles\")\n",
    "# plt.title(\"HDBSCAN Finbert Cluster Distribution\")\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363820"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_with_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# Load cleaned dataset \n",
    "df_cleaned = pd.read_csv(f\"{CSV_OUTPUT_PATH}/{CLEANED_ARTICLES_FILENAME}\")\n",
    "# remove dupes from title \n",
    "# df_cleaned = df_cleaned.drop_duplicates(subset=['article_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363820, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Top Words in Cluster 0:**\n",
      "[('stock', 159), ('earning', 80), ('company', 70), ('2024', 58), ('$', 56), ('market', 55), ('%', 46), ('buy', 45), ('ai', 44), (' ', 41), ('year', 41), ('investor', 37), ('share', 35), ('report', 35), ('quarter', 35)]\n",
      "\n",
      "**Top Words in Cluster 10:**\n",
      "[('stock', 190), ('earning', 85), ('company', 79), ('$', 72), ('market', 69), (' ', 68), ('2024', 68), ('%', 66), ('share', 54), ('inc', 52), ('investor', 44), ('year', 44), ('buy', 43), ('report', 43), ('new', 40)]\n",
      "\n",
      "**Top Words in Cluster 2:**\n",
      "[('stock', 193), ('earning', 116), ('2024', 92), ('company', 84), (' ', 69), ('%', 63), ('investor', 60), ('$', 60), ('quarter', 54), ('look', 52), ('market', 52), ('buy', 51), ('report', 50), ('inc', 50), ('new', 45)]\n",
      "\n",
      "**Top Words in Cluster 13:**\n",
      "[('stock', 131), ('earning', 68), ('company', 50), ('inc', 46), ('ai', 45), ('$', 45), ('market', 44), ('2024', 44), ('%', 34), ('buy', 32), (' ', 30), ('report', 29), ('investor', 29), ('share', 29), ('year', 27)]\n",
      "\n",
      "**Top Words in Cluster 14:**\n",
      "[('stock', 387), ('$', 159), ('2024', 151), ('earning', 136), ('market', 127), ('company', 125), ('%', 115), ('inc', 108), (' ', 98), ('investor', 91), ('buy', 89), ('report', 86), ('year', 83), ('share', 80), ('ai', 79)]\n"
     ]
    }
   ],
   "source": [
    "# Convert token strings back to lists (if stored as strings)\n",
    "df_cleaned[\"tokens\"] = df_cleaned[\"tokens\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Add DBSCAN cluster labels to dataset\n",
    "df_cleaned[\"cluster\"] = cluster_labels.astype(int)  # Ensure int type for easier grouping\n",
    "\n",
    "# Function to get most common words per cluster\n",
    "def get_top_words(cluster_id, n=15):\n",
    "    words = [word for tokens in df_cleaned[df_cleaned[\"cluster\"] == cluster_id][\"tokens\"] for word in tokens]\n",
    "    word_counts = Counter(words).most_common(n)\n",
    "    return word_counts\n",
    "\n",
    "# Analyze a few sample clusters\n",
    "sample_clusters = [0, 10, 2, 13, 14]  # Adjust based on distribution\n",
    "for cluster_id in sample_clusters:\n",
    "    print(f\"\\n**Top Words in Cluster {cluster_id}:**\")\n",
    "    print(get_top_words(cluster_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Loughran-McDonald Lexicon\n",
    "lm_dict_path = f'{CSV_OUTPUT_PATH}/{LM_LEXICON_FILENAME}'\n",
    "lm_df = pd.read_csv(lm_dict_path)\n",
    "\n",
    "# Define sentiment categories\n",
    "sentiment_categories = ['Negative', 'Positive', 'Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal']\n",
    "lm_sentiment_dict = {category: set(lm_df[lm_df[category] > 0]['Word'].str.lower()) for category in sentiment_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_risk_theme(cluster_id):\n",
    "    words = [word for tokens in df_cleaned[df_cleaned[\"cluster\"] == cluster_id][\"tokens\"] for word in tokens]\n",
    "    word_counts = Counter(words)\n",
    "    total_words = sum(word_counts.values())\n",
    "\n",
    "    category_proportions = {}\n",
    "    for category, word_set in lm_sentiment_dict.items():\n",
    "        category_count = sum(count for word, count in word_counts.items() if word in word_set)\n",
    "        category_proportions[category] = category_count / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Custom thresholds\n",
    "    low_freq_categories = ['Litigious', 'Strong_Modal', 'Weak_Modal','Uncertainty']\n",
    "    threshold = {cat: 0.01 if cat in low_freq_categories else 0.05 for cat in category_proportions}\n",
    "\n",
    "    # Apply threshold check\n",
    "    valid_categories = {cat: score for cat, score in category_proportions.items() if score > threshold[cat]}\n",
    "\n",
    "    if valid_categories:\n",
    "        dominant_category = max(valid_categories, key=valid_categories.get)\n",
    "        return dominant_category\n",
    "    else:\n",
    "        return 'Mixed Sentiment'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Cluster Risk Labels:**\n",
      "Cluster -1: Mixed Sentiment\n",
      "Cluster 1: Mixed Sentiment\n",
      "Cluster 2: Mixed Sentiment\n",
      "Cluster 3: Mixed Sentiment\n",
      "Cluster 4: Mixed Sentiment\n",
      "Cluster 5: Mixed Sentiment\n",
      "Cluster 6: Mixed Sentiment\n",
      "Cluster 7: Mixed Sentiment\n",
      "Cluster 8: Mixed Sentiment\n",
      "Cluster 9: Mixed Sentiment\n",
      "Cluster 10: Mixed Sentiment\n",
      "Cluster 11: Mixed Sentiment\n",
      "Cluster 12: Mixed Sentiment\n",
      "Cluster 13: Mixed Sentiment\n",
      "Cluster 14: Mixed Sentiment\n",
      "Cluster 15: Mixed Sentiment\n",
      "Cluster 16: Mixed Sentiment\n",
      "Cluster 17: Mixed Sentiment\n",
      "Cluster 18: Mixed Sentiment\n",
      "Cluster 19: Mixed Sentiment\n",
      "Cluster 20: Mixed Sentiment\n",
      "Cluster 21: Mixed Sentiment\n",
      "Cluster 22: Mixed Sentiment\n",
      "Cluster 23: Mixed Sentiment\n",
      "Cluster 24: Mixed Sentiment\n",
      "Cluster 25: Mixed Sentiment\n",
      "Cluster 26: Mixed Sentiment\n",
      "Cluster 27: Mixed Sentiment\n",
      "Cluster 28: Mixed Sentiment\n",
      "Cluster 29: Mixed Sentiment\n",
      "Cluster 30: Mixed Sentiment\n",
      "Cluster 31: Mixed Sentiment\n",
      "Cluster 32: Mixed Sentiment\n",
      "Cluster 33: Mixed Sentiment\n",
      "Cluster 34: Mixed Sentiment\n",
      "Cluster 35: Mixed Sentiment\n",
      "Cluster 36: Mixed Sentiment\n",
      "Cluster 37: Mixed Sentiment\n",
      "Cluster 38: Mixed Sentiment\n",
      "Cluster 39: Mixed Sentiment\n",
      "Cluster 40: Mixed Sentiment\n",
      "Cluster 41: Mixed Sentiment\n",
      "Cluster 42: Mixed Sentiment\n",
      "Cluster 43: Mixed Sentiment\n",
      "Cluster 44: Mixed Sentiment\n",
      "Cluster 45: Mixed Sentiment\n",
      "Cluster 46: Mixed Sentiment\n",
      "Cluster 47: Mixed Sentiment\n",
      "Cluster 48: Mixed Sentiment\n",
      "Cluster 49: Mixed Sentiment\n",
      "Cluster 50: Mixed Sentiment\n",
      "Cluster 51: Mixed Sentiment\n",
      "Cluster 52: Mixed Sentiment\n",
      "Cluster 53: Mixed Sentiment\n",
      "Cluster 54: Mixed Sentiment\n",
      "Cluster 55: Mixed Sentiment\n",
      "Cluster 56: Mixed Sentiment\n",
      "Cluster 57: Mixed Sentiment\n",
      "Cluster 58: Mixed Sentiment\n",
      "Cluster 59: Mixed Sentiment\n",
      "Cluster 60: Mixed Sentiment\n",
      "Cluster 61: Mixed Sentiment\n",
      "Cluster 62: Mixed Sentiment\n",
      "Cluster 63: Mixed Sentiment\n",
      "Cluster 64: Mixed Sentiment\n",
      "Cluster 65: Mixed Sentiment\n",
      "Cluster 66: Mixed Sentiment\n",
      "Cluster 67: Mixed Sentiment\n",
      "Cluster 68: Mixed Sentiment\n",
      "Cluster 69: Mixed Sentiment\n",
      "Cluster 70: Mixed Sentiment\n",
      "Cluster 71: Mixed Sentiment\n",
      "Cluster 72: Mixed Sentiment\n",
      "Cluster 73: Mixed Sentiment\n",
      "Cluster 74: Mixed Sentiment\n",
      "Cluster 75: Mixed Sentiment\n",
      "Cluster 76: Mixed Sentiment\n",
      "Cluster 77: Mixed Sentiment\n",
      "Cluster 78: Mixed Sentiment\n",
      "Cluster 79: Mixed Sentiment\n",
      "Cluster 80: Mixed Sentiment\n",
      "Cluster 81: Mixed Sentiment\n",
      "Cluster 82: Mixed Sentiment\n",
      "Cluster 83: Mixed Sentiment\n",
      "Cluster 84: Mixed Sentiment\n",
      "Cluster 85: Mixed Sentiment\n",
      "Cluster 86: Mixed Sentiment\n",
      "Cluster 87: Mixed Sentiment\n",
      "Cluster 88: Mixed Sentiment\n",
      "Cluster 89: Mixed Sentiment\n",
      "Cluster 90: Mixed Sentiment\n",
      "Cluster 91: Mixed Sentiment\n",
      "Cluster 92: Mixed Sentiment\n",
      "Cluster 93: Mixed Sentiment\n",
      "Cluster 94: Mixed Sentiment\n",
      "Cluster 95: Mixed Sentiment\n",
      "Cluster 96: Mixed Sentiment\n",
      "Cluster 97: Mixed Sentiment\n",
      "Cluster 98: Mixed Sentiment\n",
      "Cluster 99: Mixed Sentiment\n",
      "Cluster 100: Mixed Sentiment\n",
      "Cluster 101: Mixed Sentiment\n",
      "Cluster 102: Mixed Sentiment\n",
      "Cluster 103: Mixed Sentiment\n",
      "Cluster 104: Mixed Sentiment\n"
     ]
    }
   ],
   "source": [
    "# Function to assign a risk theme to a cluster\n",
    "# def assign_risk_theme(cluster_id):\n",
    "#     words = [word for tokens in df_cleaned[df_cleaned[\"cluster\"] == cluster_id][\"tokens\"] for word in tokens]\n",
    "#     word_counts = Counter(words)\n",
    "#     total_words = sum(word_counts.values())\n",
    "\n",
    "#     category_proportions = {}\n",
    "#     for category, word_set in lm_sentiment_dict.items():\n",
    "#         category_count = sum(count for word, count in word_counts.items() if word in word_set)\n",
    "#         category_proportions[category] = category_count / total_words if total_words > 0 else 0\n",
    "\n",
    "#     # Determine dominant category\n",
    "#     dominant_category = max(category_proportions, key=category_proportions.get)\n",
    "#     return dominant_category if category_proportions[dominant_category] > 0.01 else 'Mixed Sentiment'\n",
    "\n",
    "\n",
    "# Assign risk labels to all clusters\n",
    "# cluster_risk_labels = {cluster_id: assign_risk_theme(cluster_id) for cluster_id in cluster_counts.index}\n",
    "filtered_clusters = [c for c in cluster_counts.index if c != 0]\n",
    "cluster_risk_labels = {cluster_id: assign_risk_theme(cluster_id) for cluster_id in filtered_clusters}\n",
    "\n",
    "# Print risk labels\n",
    "print(\"\\n**Cluster Risk Labels:**\")\n",
    "for cluster_id, risk_theme in cluster_risk_labels.items():\n",
    "    print(f\"Cluster {cluster_id}: {risk_theme}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Top Words in Cluster 12:**\n",
      "[('stock', 746), ('company', 305), ('$', 304), ('earning', 296), ('2024', 292), ('market', 268), (' ', 261), ('%', 232), ('inc', 225), ('ai', 199), ('investor', 193), ('buy', 187), ('year', 181), ('report', 173), ('quarter', 166), ('share', 162), ('look', 153), ('growth', 138), ('announce', 135), ('new', 134)]\n",
      "\n",
      "**Top Words in Cluster 14:**\n",
      "[('stock', 387), ('$', 159), ('2024', 151), ('earning', 136), ('market', 127), ('company', 125), ('%', 115), ('inc', 108), (' ', 98), ('investor', 91), ('buy', 89), ('report', 86), ('year', 83), ('share', 80), ('ai', 79), ('quarter', 78), ('growth', 76), ('look', 72), ('announce', 71), ('result', 65)]\n"
     ]
    }
   ],
   "source": [
    "# Function to get top words per cluster\n",
    "def get_top_words(cluster_id, n=20):\n",
    "    words = [word for tokens in df_cleaned[df_cleaned[\"cluster\"] == cluster_id][\"tokens\"] for word in tokens]\n",
    "    word_counts = Counter(words).most_common(n)\n",
    "    return word_counts\n",
    "\n",
    "# Check high-risk clusters\n",
    "high_risk_clusters = [12, 14]  # Uncertainty and Negative clusters\n",
    "for cluster_id in high_risk_clusters:\n",
    "    print(f\"\\n**Top Words in Cluster {cluster_id}:**\")\n",
    "    print(get_top_words(cluster_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Stock-Level Risk Exposure:**\n",
      "        Mixed Sentiment  Unknown  Mixed Sentiment  Mixed Sentiment  \\\n",
      "ticker                                                               \n",
      "A                   221        0                0                1   \n",
      "AA                  106        0                0                1   \n",
      "AAL                 402        0                0                0   \n",
      "AAON                 73        0                1                0   \n",
      "AAP                  83        0                0                0   \n",
      "AAPL               4503        6                1                2   \n",
      "ABBV                777        2                1                1   \n",
      "ABCB                 37        0                0                0   \n",
      "ABG                  76        0                0                0   \n",
      "ABNB                700        2                1                0   \n",
      "ABT                 512        2                2                0   \n",
      "ACA                  73        0                0                0   \n",
      "ACAD                 43        0                0                0   \n",
      "ACGL                171        1                1                2   \n",
      "ACHC                 74        0                0                0   \n",
      "ACI                 347        0                0                0   \n",
      "ACIW                 63        0                0                0   \n",
      "ACLS                 94        0                0                0   \n",
      "ACLX                 33        0                0                0   \n",
      "ACM                 153        1                0                0   \n",
      "\n",
      "        Mixed Sentiment  Mixed Sentiment  Mixed Sentiment  Mixed Sentiment  \\\n",
      "ticker                                                                       \n",
      "A                     1                2                0                2   \n",
      "AA                    0                0                0                0   \n",
      "AAL                   0                1                0                0   \n",
      "AAON                  1                0                0                0   \n",
      "AAP                   0                0                0                0   \n",
      "AAPL                  6               33                3                7   \n",
      "ABBV                  2                5                0                4   \n",
      "ABCB                  0                0                0                0   \n",
      "ABG                   0                2                0                0   \n",
      "ABNB                  1                1                0                1   \n",
      "ABT                   0                5                0                1   \n",
      "ACA                   1                0                0                0   \n",
      "ACAD                  0                0                0                0   \n",
      "ACGL                  0                2                0                0   \n",
      "ACHC                  0                0                0                0   \n",
      "ACI                   0                4                0                0   \n",
      "ACIW                  0                2                0                0   \n",
      "ACLS                  0                1                0                0   \n",
      "ACLX                  0                1                0                0   \n",
      "ACM                   0                1                0                1   \n",
      "\n",
      "        Mixed Sentiment  Mixed Sentiment  ...  Mixed Sentiment  \\\n",
      "ticker                                    ...                    \n",
      "A                     0                1  ...                1   \n",
      "AA                    0                0  ...                0   \n",
      "AAL                   0                2  ...                1   \n",
      "AAON                  0                2  ...                0   \n",
      "AAP                   0                0  ...                0   \n",
      "AAPL                  5                7  ...                5   \n",
      "ABBV                  0                0  ...                3   \n",
      "ABCB                  0                0  ...                0   \n",
      "ABG                   0                0  ...                0   \n",
      "ABNB                  0                1  ...                2   \n",
      "ABT                   0                0  ...                0   \n",
      "ACA                   0                0  ...                0   \n",
      "ACAD                  0                0  ...                0   \n",
      "ACGL                  0                0  ...                0   \n",
      "ACHC                  0                0  ...                0   \n",
      "ACI                   1                1  ...                0   \n",
      "ACIW                  0                0  ...                0   \n",
      "ACLS                  0                0  ...                0   \n",
      "ACLX                  0                0  ...                1   \n",
      "ACM                   0                0  ...                0   \n",
      "\n",
      "        Mixed Sentiment  Mixed Sentiment  Mixed Sentiment  Mixed Sentiment  \\\n",
      "ticker                                                                       \n",
      "A                     0                0                0                1   \n",
      "AA                    0                0                0                0   \n",
      "AAL                   0                0                0                2   \n",
      "AAON                  1                0                1                1   \n",
      "AAP                   0                0                0                0   \n",
      "AAPL                  3                3                3               20   \n",
      "ABBV                  1                0                1                8   \n",
      "ABCB                  0                1                0                0   \n",
      "ABG                   0                0                0                0   \n",
      "ABNB                  0                1                2                4   \n",
      "ABT                   1                0                1                7   \n",
      "ACA                   0                0                0                0   \n",
      "ACAD                  0                0                0                0   \n",
      "ACGL                  0                0                0                0   \n",
      "ACHC                  0                0                0                0   \n",
      "ACI                   0                0                0                2   \n",
      "ACIW                  0                0                0                1   \n",
      "ACLS                  0                0                0                1   \n",
      "ACLX                  0                0                0                0   \n",
      "ACM                   0                0                0                0   \n",
      "\n",
      "        Mixed Sentiment  Mixed Sentiment  Mixed Sentiment  Mixed Sentiment  \\\n",
      "ticker                                                                       \n",
      "A                     1                0                3                0   \n",
      "AA                    0                0                0                1   \n",
      "AAL                   0                0                1                1   \n",
      "AAON                  0                0                0                0   \n",
      "AAP                   1                0                0                0   \n",
      "AAPL                  8                0               22                4   \n",
      "ABBV                  3                0                4                1   \n",
      "ABCB                  0                0                0                0   \n",
      "ABG                   0                0                0                0   \n",
      "ABNB                  3                0                4                1   \n",
      "ABT                   1                0                0                0   \n",
      "ACA                   0                0                0                0   \n",
      "ACAD                  0                0                0                0   \n",
      "ACGL                  0                0                0                0   \n",
      "ACHC                  0                0                1                0   \n",
      "ACI                   1                0                1                0   \n",
      "ACIW                  0                0                0                0   \n",
      "ACLS                  0                0                1                0   \n",
      "ACLX                  0                0                0                0   \n",
      "ACM                   0                0                0                0   \n",
      "\n",
      "        Mixed Sentiment  \n",
      "ticker                   \n",
      "A                     0  \n",
      "AA                    0  \n",
      "AAL                   0  \n",
      "AAON                  1  \n",
      "AAP                   0  \n",
      "AAPL                  9  \n",
      "ABBV                  2  \n",
      "ABCB                  0  \n",
      "ABG                   0  \n",
      "ABNB                  2  \n",
      "ABT                   1  \n",
      "ACA                   1  \n",
      "ACAD                  1  \n",
      "ACGL                  1  \n",
      "ACHC                  0  \n",
      "ACI                   2  \n",
      "ACIW                  0  \n",
      "ACLS                  0  \n",
      "ACLX                  0  \n",
      "ACM                   0  \n",
      "\n",
      "[20 rows x 106 columns]\n"
     ]
    }
   ],
   "source": [
    "# Count how many articles per stock belong to each risk cluster\n",
    "stock_risk_exposure = df_cleaned.groupby([\"ticker\", \"cluster\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Merge risk labels\n",
    "stock_risk_exposure.columns = [cluster_risk_labels.get(cluster_id, \"Unknown\") for cluster_id in stock_risk_exposure.columns]\n",
    "\n",
    "# Save results\n",
    "stock_risk_exposure.to_csv(f\"{CSV_OUTPUT_PATH}/{STOCK_RISK_EXPOSURE_CSV}\")\n",
    "\n",
    "# Show a sample\n",
    "print(\"\\n**Stock-Level Risk Exposure:**\")\n",
    "print(stock_risk_exposure.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for each category\n",
    "low_freq_categories = ['Uncertainty', 'Litigious', 'Strong_Modal', 'Weak_Modal']\n",
    "category_thresholds = {cat: 0.01 if cat in low_freq_categories else 0.01 for cat in sentiment_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def assign_risk_themes_multi(cluster_id):\n",
    "    words = [word for tokens in df_cleaned[df_cleaned[\"cluster\"] == cluster_id][\"tokens\"] for word in tokens]\n",
    "    word_counts = Counter(words)\n",
    "    total_words = sum(word_counts.values())\n",
    "\n",
    "    category_proportions = {}\n",
    "    for category, word_set in lm_sentiment_dict.items():\n",
    "        category_count = sum(count for word, count in word_counts.items() if word in word_set)\n",
    "        category_proportions[category] = category_count / total_words if total_words > 0 else 0\n",
    "\n",
    "    # Select all categories that pass their respective thresholds\n",
    "    selected_categories = [\n",
    "        cat for cat, prop in category_proportions.items()\n",
    "        if prop > category_thresholds.get(cat, 0.05)\n",
    "    ]\n",
    "\n",
    "    return selected_categories if selected_categories else ['Mixed Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Multi-Label Cluster Risk Tags:**\n",
      "Cluster -1: ['Negative', 'Positive']\n",
      "Cluster 0: ['Negative', 'Positive']\n",
      "Cluster 1: ['Negative', 'Positive']\n",
      "Cluster 2: ['Negative', 'Positive']\n",
      "Cluster 3: ['Negative', 'Positive']\n",
      "Cluster 4: ['Negative', 'Positive']\n",
      "Cluster 5: ['Negative', 'Positive']\n",
      "Cluster 6: ['Negative', 'Positive']\n",
      "Cluster 7: ['Negative', 'Positive']\n",
      "Cluster 8: ['Negative', 'Positive']\n",
      "Cluster 9: ['Negative', 'Positive']\n",
      "Cluster 10: ['Negative', 'Positive']\n",
      "Cluster 11: ['Negative', 'Positive']\n",
      "Cluster 12: ['Negative', 'Positive']\n",
      "Cluster 13: ['Negative', 'Positive']\n",
      "Cluster 14: ['Negative', 'Positive']\n",
      "Cluster 15: ['Negative', 'Positive']\n",
      "Cluster 16: ['Negative', 'Positive']\n",
      "Cluster 17: ['Negative', 'Positive']\n",
      "Cluster 18: ['Negative', 'Positive']\n",
      "Cluster 19: ['Negative', 'Positive']\n",
      "Cluster 20: ['Negative', 'Positive']\n",
      "Cluster 21: ['Negative', 'Positive']\n",
      "Cluster 22: ['Negative', 'Positive']\n",
      "Cluster 23: ['Negative', 'Positive']\n",
      "Cluster 24: ['Negative', 'Positive']\n",
      "Cluster 25: ['Negative', 'Positive']\n",
      "Cluster 26: ['Negative', 'Positive']\n",
      "Cluster 27: ['Negative', 'Positive']\n",
      "Cluster 28: ['Negative', 'Positive']\n",
      "Cluster 29: ['Negative', 'Positive']\n",
      "Cluster 30: ['Negative', 'Positive']\n",
      "Cluster 31: ['Negative', 'Positive']\n",
      "Cluster 32: ['Negative', 'Positive']\n",
      "Cluster 33: ['Negative', 'Positive']\n",
      "Cluster 34: ['Negative', 'Positive']\n",
      "Cluster 35: ['Negative', 'Positive']\n",
      "Cluster 36: ['Negative', 'Positive']\n",
      "Cluster 37: ['Negative', 'Positive']\n",
      "Cluster 38: ['Negative', 'Positive']\n",
      "Cluster 39: ['Negative', 'Positive']\n",
      "Cluster 40: ['Negative', 'Positive']\n",
      "Cluster 41: ['Negative', 'Positive']\n",
      "Cluster 42: ['Negative', 'Positive']\n",
      "Cluster 43: ['Negative', 'Positive']\n",
      "Cluster 44: ['Negative', 'Positive']\n",
      "Cluster 45: ['Negative', 'Positive']\n",
      "Cluster 46: ['Negative', 'Positive']\n",
      "Cluster 47: ['Negative', 'Positive']\n",
      "Cluster 48: ['Negative', 'Positive']\n",
      "Cluster 49: ['Negative', 'Positive']\n",
      "Cluster 50: ['Negative', 'Positive']\n",
      "Cluster 51: ['Negative', 'Positive']\n",
      "Cluster 52: ['Negative', 'Positive']\n",
      "Cluster 53: ['Negative', 'Positive']\n",
      "Cluster 54: ['Negative', 'Positive']\n",
      "Cluster 55: ['Negative', 'Positive']\n",
      "Cluster 56: ['Negative', 'Positive']\n",
      "Cluster 57: ['Negative', 'Positive']\n",
      "Cluster 58: ['Negative', 'Positive']\n",
      "Cluster 59: ['Negative', 'Positive']\n",
      "Cluster 60: ['Negative', 'Positive']\n",
      "Cluster 61: ['Negative', 'Positive']\n",
      "Cluster 62: ['Negative', 'Positive']\n",
      "Cluster 63: ['Negative', 'Positive']\n",
      "Cluster 64: ['Negative', 'Positive']\n",
      "Cluster 65: ['Negative', 'Positive']\n",
      "Cluster 66: ['Negative', 'Positive']\n",
      "Cluster 67: ['Negative', 'Positive']\n",
      "Cluster 68: ['Negative', 'Positive']\n",
      "Cluster 69: ['Negative', 'Positive']\n",
      "Cluster 70: ['Negative', 'Positive']\n",
      "Cluster 71: ['Negative', 'Positive']\n",
      "Cluster 72: ['Negative', 'Positive']\n",
      "Cluster 73: ['Negative', 'Positive']\n",
      "Cluster 74: ['Negative', 'Positive']\n",
      "Cluster 75: ['Negative', 'Positive']\n",
      "Cluster 76: ['Negative', 'Positive']\n",
      "Cluster 77: ['Negative', 'Positive']\n",
      "Cluster 78: ['Negative', 'Positive']\n",
      "Cluster 79: ['Negative', 'Positive']\n",
      "Cluster 80: ['Negative', 'Positive']\n",
      "Cluster 81: ['Negative', 'Positive']\n",
      "Cluster 82: ['Negative', 'Positive']\n",
      "Cluster 83: ['Negative', 'Positive']\n",
      "Cluster 84: ['Negative', 'Positive']\n",
      "Cluster 85: ['Negative', 'Positive']\n",
      "Cluster 86: ['Negative', 'Positive']\n",
      "Cluster 87: ['Negative', 'Positive']\n",
      "Cluster 88: ['Negative', 'Positive']\n",
      "Cluster 89: ['Negative', 'Positive']\n",
      "Cluster 90: ['Negative', 'Positive']\n",
      "Cluster 91: ['Negative', 'Positive']\n",
      "Cluster 92: ['Negative', 'Positive']\n",
      "Cluster 93: ['Negative', 'Positive']\n",
      "Cluster 94: ['Negative', 'Positive']\n",
      "Cluster 95: ['Negative', 'Positive']\n",
      "Cluster 96: ['Negative', 'Positive']\n",
      "Cluster 97: ['Negative', 'Positive']\n",
      "Cluster 98: ['Negative', 'Positive']\n",
      "Cluster 99: ['Negative', 'Positive']\n",
      "Cluster 100: ['Negative', 'Positive']\n",
      "Cluster 101: ['Negative', 'Positive']\n",
      "Cluster 102: ['Negative', 'Positive']\n",
      "Cluster 103: ['Negative', 'Positive']\n",
      "Cluster 104: ['Negative', 'Positive']\n"
     ]
    }
   ],
   "source": [
    "cluster_risk_labels_multi = {\n",
    "    cluster_id: assign_risk_themes_multi(cluster_id)\n",
    "    for cluster_id in cluster_counts.index\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n**Multi-Label Cluster Risk Tags:**\")\n",
    "for cluster_id, risk_tags in cluster_risk_labels_multi.items():\n",
    "    print(f\"Cluster {cluster_id}: {risk_tags}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_tags\n",
      "Negative    363820\n",
      "Positive    363820\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>mapped_trading_date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>description</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>cluster</th>\n",
       "      <th>risk_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DXC</td>\n",
       "      <td>2024-07-11</td>\n",
       "      <td>DXC Technology to Report Fiscal First Quarter ...</td>\n",
       "      <td>ASHBURN, Va., July 10, 2024--DXC Technology (N...</td>\n",
       "      <td>DXC Technology to Report Fiscal First Quarter ...</td>\n",
       "      <td>dxc technology to report fiscal first quarter ...</td>\n",
       "      <td>[dxc, technology, report, fiscal, quarter, 202...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DXC</td>\n",
       "      <td>2024-07-11</td>\n",
       "      <td>DXC Technology to Report Fiscal First Quarter ...</td>\n",
       "      <td>ASHBURN, Va., July 10, 2024--DXC Technology (N...</td>\n",
       "      <td>DXC Technology to Report Fiscal First Quarter ...</td>\n",
       "      <td>dxc technology to report fiscal first quarter ...</td>\n",
       "      <td>[dxc, technology, report, fiscal, quarter, 202...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRI</td>\n",
       "      <td>2024-07-17</td>\n",
       "      <td>Darden to buy Chuyâs for more than $600M</td>\n",
       "      <td>The all-cash transaction will add a Tex-Mex ch...</td>\n",
       "      <td>Darden to buy Chuyâs for more than $600M The...</td>\n",
       "      <td>darden to buy chuyâs for more than $600m the a...</td>\n",
       "      <td>[darden, buy, chuyâs, $, 600, m, allcash, tran...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRI</td>\n",
       "      <td>2024-07-17</td>\n",
       "      <td>Darden to buy Chuyâs for more than $600M</td>\n",
       "      <td>The all-cash transaction will add a Tex-Mex ch...</td>\n",
       "      <td>Darden to buy Chuyâs for more than $600M The...</td>\n",
       "      <td>darden to buy chuyâs for more than $600m the a...</td>\n",
       "      <td>[darden, buy, chuyâs, $, 600, m, allcash, tran...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DVA</td>\n",
       "      <td>2024-07-19</td>\n",
       "      <td>Update: DaVita to Pay $34.5 Million for Settle...</td>\n",
       "      <td>Update: DaVita to Pay $34.5 Million for Settle...</td>\n",
       "      <td>Update: DaVita to Pay $34.5 Million for Settle...</td>\n",
       "      <td>update davita to pay $345 million for settleme...</td>\n",
       "      <td>[update, davita, pay, $, 345, million, settlem...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DVA</td>\n",
       "      <td>2024-07-19</td>\n",
       "      <td>Update: DaVita to Pay $34.5 Million for Settle...</td>\n",
       "      <td>Update: DaVita to Pay $34.5 Million for Settle...</td>\n",
       "      <td>Update: DaVita to Pay $34.5 Million for Settle...</td>\n",
       "      <td>update davita to pay $345 million for settleme...</td>\n",
       "      <td>[update, davita, pay, $, 345, million, settlem...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DRI</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>Analysts Have Made A Financial Statement On Da...</td>\n",
       "      <td>Last week saw the newest yearly earnings relea...</td>\n",
       "      <td>Analysts Have Made A Financial Statement On Da...</td>\n",
       "      <td>analysts have made a financial statement on da...</td>\n",
       "      <td>[analyst, financial, statement, darden, restau...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DRI</td>\n",
       "      <td>2024-07-22</td>\n",
       "      <td>Analysts Have Made A Financial Statement On Da...</td>\n",
       "      <td>Last week saw the newest yearly earnings relea...</td>\n",
       "      <td>Analysts Have Made A Financial Statement On Da...</td>\n",
       "      <td>analysts have made a financial statement on da...</td>\n",
       "      <td>[analyst, financial, statement, darden, restau...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVA</td>\n",
       "      <td>2024-07-23</td>\n",
       "      <td>DaVita HealthCare (DVA) Laps the Stock Market:...</td>\n",
       "      <td>The latest trading day saw DaVita HealthCare (...</td>\n",
       "      <td>DaVita HealthCare (DVA) Laps the Stock Market:...</td>\n",
       "      <td>davita healthcare dva laps the stock market he...</td>\n",
       "      <td>[davita, healthcare, dva, lap, stock, market, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVA</td>\n",
       "      <td>2024-07-23</td>\n",
       "      <td>DaVita HealthCare (DVA) Laps the Stock Market:...</td>\n",
       "      <td>The latest trading day saw DaVita HealthCare (...</td>\n",
       "      <td>DaVita HealthCare (DVA) Laps the Stock Market:...</td>\n",
       "      <td>davita healthcare dva laps the stock market he...</td>\n",
       "      <td>[davita, healthcare, dva, lap, stock, market, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker mapped_trading_date  \\\n",
       "0    DXC          2024-07-11   \n",
       "0    DXC          2024-07-11   \n",
       "1    DRI          2024-07-17   \n",
       "1    DRI          2024-07-17   \n",
       "2    DVA          2024-07-19   \n",
       "2    DVA          2024-07-19   \n",
       "3    DRI          2024-07-22   \n",
       "3    DRI          2024-07-22   \n",
       "4    DVA          2024-07-23   \n",
       "4    DVA          2024-07-23   \n",
       "\n",
       "                                       article_title  \\\n",
       "0  DXC Technology to Report Fiscal First Quarter ...   \n",
       "0  DXC Technology to Report Fiscal First Quarter ...   \n",
       "1         Darden to buy Chuyâs for more than $600M   \n",
       "1         Darden to buy Chuyâs for more than $600M   \n",
       "2  Update: DaVita to Pay $34.5 Million for Settle...   \n",
       "2  Update: DaVita to Pay $34.5 Million for Settle...   \n",
       "3  Analysts Have Made A Financial Statement On Da...   \n",
       "3  Analysts Have Made A Financial Statement On Da...   \n",
       "4  DaVita HealthCare (DVA) Laps the Stock Market:...   \n",
       "4  DaVita HealthCare (DVA) Laps the Stock Market:...   \n",
       "\n",
       "                                         description  \\\n",
       "0  ASHBURN, Va., July 10, 2024--DXC Technology (N...   \n",
       "0  ASHBURN, Va., July 10, 2024--DXC Technology (N...   \n",
       "1  The all-cash transaction will add a Tex-Mex ch...   \n",
       "1  The all-cash transaction will add a Tex-Mex ch...   \n",
       "2  Update: DaVita to Pay $34.5 Million for Settle...   \n",
       "2  Update: DaVita to Pay $34.5 Million for Settle...   \n",
       "3  Last week saw the newest yearly earnings relea...   \n",
       "3  Last week saw the newest yearly earnings relea...   \n",
       "4  The latest trading day saw DaVita HealthCare (...   \n",
       "4  The latest trading day saw DaVita HealthCare (...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  DXC Technology to Report Fiscal First Quarter ...   \n",
       "0  DXC Technology to Report Fiscal First Quarter ...   \n",
       "1  Darden to buy Chuyâs for more than $600M The...   \n",
       "1  Darden to buy Chuyâs for more than $600M The...   \n",
       "2  Update: DaVita to Pay $34.5 Million for Settle...   \n",
       "2  Update: DaVita to Pay $34.5 Million for Settle...   \n",
       "3  Analysts Have Made A Financial Statement On Da...   \n",
       "3  Analysts Have Made A Financial Statement On Da...   \n",
       "4  DaVita HealthCare (DVA) Laps the Stock Market:...   \n",
       "4  DaVita HealthCare (DVA) Laps the Stock Market:...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  dxc technology to report fiscal first quarter ...   \n",
       "0  dxc technology to report fiscal first quarter ...   \n",
       "1  darden to buy chuyâs for more than $600m the a...   \n",
       "1  darden to buy chuyâs for more than $600m the a...   \n",
       "2  update davita to pay $345 million for settleme...   \n",
       "2  update davita to pay $345 million for settleme...   \n",
       "3  analysts have made a financial statement on da...   \n",
       "3  analysts have made a financial statement on da...   \n",
       "4  davita healthcare dva laps the stock market he...   \n",
       "4  davita healthcare dva laps the stock market he...   \n",
       "\n",
       "                                              tokens  cluster risk_tags  \n",
       "0  [dxc, technology, report, fiscal, quarter, 202...       -1  Negative  \n",
       "0  [dxc, technology, report, fiscal, quarter, 202...       -1  Positive  \n",
       "1  [darden, buy, chuyâs, $, 600, m, allcash, tran...       -1  Negative  \n",
       "1  [darden, buy, chuyâs, $, 600, m, allcash, tran...       -1  Positive  \n",
       "2  [update, davita, pay, $, 345, million, settlem...       -1  Negative  \n",
       "2  [update, davita, pay, $, 345, million, settlem...       -1  Positive  \n",
       "3  [analyst, financial, statement, darden, restau...       -1  Negative  \n",
       "3  [analyst, financial, statement, darden, restau...       -1  Positive  \n",
       "4  [davita, healthcare, dva, lap, stock, market, ...       -1  Negative  \n",
       "4  [davita, healthcare, dva, lap, stock, market, ...       -1  Positive  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned[\"risk_tags\"] = df_cleaned[\"cluster\"].map(cluster_risk_labels_multi)\n",
    "df_exploded = df_cleaned.explode(\"risk_tags\")\n",
    "\n",
    "print(df_exploded[\"risk_tags\"].value_counts())\n",
    "df_exploded.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>risk_tags</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>\"Bad News\" is Good News for these 3 Stocks</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Barneyâs World\" to Premiere Monday, October 14 on Max</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Big 3\" carmakers face \"tough choices\" to hit inventory targets - Wells Fargo</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Big Short\" Investor Michael Burry Has 10% of His Portfolio in 2 \"Magnificent Seven\" AI Stocks</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Communications for Good\" Provides Maximus Foundation Grantees With Professional Support</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Decoded\": Groundbreaking Chinese Psychological Thriller Film Set for Global Release on August 22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"EXPERIENCE MORE AT SEAâ¢\" WITH NORWEGIAN CRUISE LINE WHERE THERE IS MORE TO SEE, MORE TO DO AND MORE TO ENJOY</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Empowering American Cities\" Initiative Reveals Economic Insights, 2025 Outlook</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"FAN FIRST\" PROGRAM BEST PLACE FOR KNICKS AND RANGERS FANS TO GET PLAYOFF TICKETS AT FACE VALUE</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Finish Your Story\" in WWEÂ® 2K24 Deluxe Edition and Forty Years of WrestleMania Edition Now Available Worldwide</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "risk_tags                                           Negative  Positive\n",
       "article_title                                                         \n",
       "\"Bad News\" is Good News for these 3 Stocks                 1         1\n",
       "\"Barneyâs World\" to Premiere Monday, October ...         1         1\n",
       "\"Big 3\" carmakers face \"tough choices\" to hit i...         1         1\n",
       "\"Big Short\" Investor Michael Burry Has 10% of H...         2         2\n",
       "\"Communications for Good\" Provides Maximus Foun...         1         1\n",
       "\"Decoded\": Groundbreaking Chinese Psychological...         1         1\n",
       "\"EXPERIENCE MORE AT SEAâ¢\" WITH NORWEGIAN CRUI...         1         1\n",
       "\"Empowering American Cities\" Initiative Reveals...         1         1\n",
       "\"FAN FIRST\" PROGRAM BEST PLACE FOR KNICKS AND R...         1         1\n",
       "\"Finish Your Story\" in WWEÂ® 2K24 Deluxe Editio...         1         1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_risk_exposure = (\n",
    "    df_exploded\n",
    "    .groupby([\"article_title\", \"risk_tags\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index(axis=1)  \n",
    ")\n",
    "\n",
    "stock_risk_exposure.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize...maybe not needed?\n",
    "# stock_risk_percent = stock_risk_exposure.div(stock_risk_exposure.sum(axis=1), axis=0)\n",
    "# stock_risk_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'mapped_trading_date', 'article_title', 'description',\n",
       "       'full_text', 'clean_text', 'tokens', 'cluster', 'risk_tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join with Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(f\"{CSV_OUTPUT_PATH}/{CLEANED_ARTICLES_FILENAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = df_original.merge(\n",
    "    stock_risk_exposure,\n",
    "    how=\"left\",\n",
    "    left_on=\"article_title\",\n",
    "    right_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ticker', 'mapped_trading_date', 'article_title', 'description',\n",
      "       'full_text', 'clean_text', 'tokens', 'Negative', 'Positive'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>mapped_trading_date</th>\n",
       "      <th>article_title</th>\n",
       "      <th>description</th>\n",
       "      <th>full_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363810</th>\n",
       "      <td>MRK</td>\n",
       "      <td>2024-07-15</td>\n",
       "      <td>The Zacks Analyst Blog Highlights Exxon  Mobil...</td>\n",
       "      <td>are included in this Analyst Blog Exxon Mobil,...</td>\n",
       "      <td>The Zacks Analyst Blog Highlights Exxon  Mobil...</td>\n",
       "      <td>the zacks analyst blog highlights exxon mobil ...</td>\n",
       "      <td>['zack', 'analyst', 'blog', 'highlight', 'exxo...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363811</th>\n",
       "      <td>MOH</td>\n",
       "      <td>2024-07-17</td>\n",
       "      <td>Would Trump Ease Up on Medicare? Wall Street T...</td>\n",
       "      <td>Investors bet that if Donald Trump is elected ...</td>\n",
       "      <td>Would Trump Ease Up on Medicare? Wall Street T...</td>\n",
       "      <td>would trump ease up on medicare wall street th...</td>\n",
       "      <td>['trump', 'ease', 'medicare', 'wall', 'street'...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363812</th>\n",
       "      <td>MRK</td>\n",
       "      <td>2024-07-18</td>\n",
       "      <td>Insiders At Merck Sold US$12m In Stock, Alludi...</td>\n",
       "      <td>Over the past year, many Merck &amp; Co., Inc. ( N...</td>\n",
       "      <td>Insiders At Merck Sold US$12m In Stock, Alludi...</td>\n",
       "      <td>insiders at merck sold us$12m in stock alludin...</td>\n",
       "      <td>['insider', 'merck', 'sell', 'us$12', 'm', 'st...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363813</th>\n",
       "      <td>MO</td>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>Investors Heavily Search Altria Group, Inc. (M...</td>\n",
       "      <td>Zacks.com users have recently been watching Al...</td>\n",
       "      <td>Investors Heavily Search Altria Group, Inc. (M...</td>\n",
       "      <td>investors heavily search altria group inc mo h...</td>\n",
       "      <td>['investor', 'heavily', 'search', 'altria', 'g...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363814</th>\n",
       "      <td>MOD</td>\n",
       "      <td>2024-07-29</td>\n",
       "      <td>Is Modine (MOD) a Solid Growth Stock? 3 Reason...</td>\n",
       "      <td>Modine (MOD) possesses solid growth attributes...</td>\n",
       "      <td>Is Modine (MOD) a Solid Growth Stock? 3 Reason...</td>\n",
       "      <td>is modine mod a solid growth stock 3 reasons t...</td>\n",
       "      <td>['modine', 'mod', 'solid', 'growth', 'stock', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363815</th>\n",
       "      <td>MORN</td>\n",
       "      <td>2024-07-30</td>\n",
       "      <td>Morningstar Retirement Launches New Morningsta...</td>\n",
       "      <td>CHICAGO, July 30, 2024--Morningstar Retirement...</td>\n",
       "      <td>Morningstar Retirement Launches New Morningsta...</td>\n",
       "      <td>morningstar retirement launches new morningsta...</td>\n",
       "      <td>['morningstar', 'retirement', 'launch', 'new',...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363816</th>\n",
       "      <td>MOD</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>Modine Reports First Quarter Fiscal 2025 Results</td>\n",
       "      <td>Modine (NYSE: MOD), a diversified global leade...</td>\n",
       "      <td>Modine Reports First Quarter Fiscal 2025 Resul...</td>\n",
       "      <td>modine reports first quarter fiscal 2025 resul...</td>\n",
       "      <td>['modine', 'report', 'quarter', 'fiscal', '202...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363817</th>\n",
       "      <td>MO</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>Company News for Aug 1, 2024</td>\n",
       "      <td>Companies in The News Are: AMD, ANET, MA, MO</td>\n",
       "      <td>Company News for Aug 1, 2024 Companies in The ...</td>\n",
       "      <td>company news for aug 1 2024 companies in the n...</td>\n",
       "      <td>['company', 'news', 'aug', '1', '2024', 'compa...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363818</th>\n",
       "      <td>MPWR</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>Monolithic Power (MPWR) Beats Q2 Earnings and ...</td>\n",
       "      <td>Monolithic (MPWR) delivered earnings and reven...</td>\n",
       "      <td>Monolithic Power (MPWR) Beats Q2 Earnings and ...</td>\n",
       "      <td>monolithic power mpwr beats q2 earnings and re...</td>\n",
       "      <td>['monolithic', 'power', 'mpwr', 'beat', 'q2', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363819</th>\n",
       "      <td>MOH</td>\n",
       "      <td>2024-08-15</td>\n",
       "      <td>Michael Burry's Strategic Emphasis on Shift4 P...</td>\n",
       "      <td>Insight into the Latest 13F Filing and Key Por...</td>\n",
       "      <td>Michael Burry's Strategic Emphasis on Shift4 P...</td>\n",
       "      <td>michael burrys strategic emphasis on shift4 pa...</td>\n",
       "      <td>['michael', 'burry', 'strategic', 'emphasis', ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker mapped_trading_date  \\\n",
       "363810    MRK          2024-07-15   \n",
       "363811    MOH          2024-07-17   \n",
       "363812    MRK          2024-07-18   \n",
       "363813     MO          2024-07-24   \n",
       "363814    MOD          2024-07-29   \n",
       "363815   MORN          2024-07-30   \n",
       "363816    MOD          2024-07-31   \n",
       "363817     MO          2024-08-01   \n",
       "363818   MPWR          2024-08-02   \n",
       "363819    MOH          2024-08-15   \n",
       "\n",
       "                                            article_title  \\\n",
       "363810  The Zacks Analyst Blog Highlights Exxon  Mobil...   \n",
       "363811  Would Trump Ease Up on Medicare? Wall Street T...   \n",
       "363812  Insiders At Merck Sold US$12m In Stock, Alludi...   \n",
       "363813  Investors Heavily Search Altria Group, Inc. (M...   \n",
       "363814  Is Modine (MOD) a Solid Growth Stock? 3 Reason...   \n",
       "363815  Morningstar Retirement Launches New Morningsta...   \n",
       "363816   Modine Reports First Quarter Fiscal 2025 Results   \n",
       "363817                       Company News for Aug 1, 2024   \n",
       "363818  Monolithic Power (MPWR) Beats Q2 Earnings and ...   \n",
       "363819  Michael Burry's Strategic Emphasis on Shift4 P...   \n",
       "\n",
       "                                              description  \\\n",
       "363810  are included in this Analyst Blog Exxon Mobil,...   \n",
       "363811  Investors bet that if Donald Trump is elected ...   \n",
       "363812  Over the past year, many Merck & Co., Inc. ( N...   \n",
       "363813  Zacks.com users have recently been watching Al...   \n",
       "363814  Modine (MOD) possesses solid growth attributes...   \n",
       "363815  CHICAGO, July 30, 2024--Morningstar Retirement...   \n",
       "363816  Modine (NYSE: MOD), a diversified global leade...   \n",
       "363817       Companies in The News Are: AMD, ANET, MA, MO   \n",
       "363818  Monolithic (MPWR) delivered earnings and reven...   \n",
       "363819  Insight into the Latest 13F Filing and Key Por...   \n",
       "\n",
       "                                                full_text  \\\n",
       "363810  The Zacks Analyst Blog Highlights Exxon  Mobil...   \n",
       "363811  Would Trump Ease Up on Medicare? Wall Street T...   \n",
       "363812  Insiders At Merck Sold US$12m In Stock, Alludi...   \n",
       "363813  Investors Heavily Search Altria Group, Inc. (M...   \n",
       "363814  Is Modine (MOD) a Solid Growth Stock? 3 Reason...   \n",
       "363815  Morningstar Retirement Launches New Morningsta...   \n",
       "363816  Modine Reports First Quarter Fiscal 2025 Resul...   \n",
       "363817  Company News for Aug 1, 2024 Companies in The ...   \n",
       "363818  Monolithic Power (MPWR) Beats Q2 Earnings and ...   \n",
       "363819  Michael Burry's Strategic Emphasis on Shift4 P...   \n",
       "\n",
       "                                               clean_text  \\\n",
       "363810  the zacks analyst blog highlights exxon mobil ...   \n",
       "363811  would trump ease up on medicare wall street th...   \n",
       "363812  insiders at merck sold us$12m in stock alludin...   \n",
       "363813  investors heavily search altria group inc mo h...   \n",
       "363814  is modine mod a solid growth stock 3 reasons t...   \n",
       "363815  morningstar retirement launches new morningsta...   \n",
       "363816  modine reports first quarter fiscal 2025 resul...   \n",
       "363817  company news for aug 1 2024 companies in the n...   \n",
       "363818  monolithic power mpwr beats q2 earnings and re...   \n",
       "363819  michael burrys strategic emphasis on shift4 pa...   \n",
       "\n",
       "                                                   tokens  Negative  Positive  \n",
       "363810  ['zack', 'analyst', 'blog', 'highlight', 'exxo...         3         3  \n",
       "363811  ['trump', 'ease', 'medicare', 'wall', 'street'...         5         5  \n",
       "363812  ['insider', 'merck', 'sell', 'us$12', 'm', 'st...         3         3  \n",
       "363813  ['investor', 'heavily', 'search', 'altria', 'g...         5         5  \n",
       "363814  ['modine', 'mod', 'solid', 'growth', 'stock', ...         1         1  \n",
       "363815  ['morningstar', 'retirement', 'launch', 'new',...         1         1  \n",
       "363816  ['modine', 'report', 'quarter', 'fiscal', '202...         1         1  \n",
       "363817  ['company', 'news', 'aug', '1', '2024', 'compa...         4         4  \n",
       "363818  ['monolithic', 'power', 'mpwr', 'beat', 'q2', ...         1         1  \n",
       "363819  ['michael', 'burry', 'strategic', 'emphasis', ...         2         2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_risk.columns)\n",
    "df_risk.tail(10)#[['article_title', 'ticker', 'Negative', 'Positive', 'Uncertainty']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Uncertainty', 'Mixed Sentiment'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# group by ticker and sum the risk tags\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df_risk_grouped \u001b[38;5;241m=\u001b[39m df_risk\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m----> 3\u001b[0m df_risk_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdf_risk_grouped\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNegative\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPositive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUncertainty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMixed Sentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m df_risk_grouped\n",
      "File \u001b[0;32m~/scratch/testpy/lib/python3.9/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/scratch/testpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/testpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Uncertainty', 'Mixed Sentiment'] not in index\""
     ]
    }
   ],
   "source": [
    "# group by ticker and sum the risk tags\n",
    "df_risk_grouped = df_risk.groupby('ticker').sum()\n",
    "df_risk_grouped = df_risk_grouped[['Negative', 'Positive', 'Uncertainty', 'Mixed Sentiment']]\n",
    "df_risk_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_risk and df_risk_grouped\n",
    "df_risk.to_csv(f\"{CSV_OUTPUT_PATH}/{HDBSCAN_RISK_TAGS}\")\n",
    "df_risk_grouped.to_csv(f\"{CSV_OUTPUT_PATH}/{HDBSCAN_GROUPED_RISK_TAGS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testpy)",
   "language": "python",
   "name": "testpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
