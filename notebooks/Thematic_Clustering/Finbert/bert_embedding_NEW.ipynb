{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30873, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "finbert_model = AutoModel.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finbert_model.to(device)\n",
    "finbert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_bert(df):\n",
    "    # Keep relevant columns\n",
    "    df = df[['guid', 'ticker', 'article_pubDate', 'article_title', 'description']].copy()\n",
    "\n",
    "    # Combine title and description into one string\n",
    "    df['full_text'] = df['article_title'].fillna('') + \" \" + df['description'].fillna('')\n",
    "\n",
    "    # Optional: Drop rows with no text at all\n",
    "    df = df[df['full_text'].str.strip() != '']\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_finbert_embedding(text):\n",
    "    inputs = finbert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move tensors to device\n",
    "    with torch.no_grad():\n",
    "        outputs = finbert_model(**inputs)\n",
    "    # move tensors back to CPU for numpy conversion\n",
    "    outputs = outputs.last_hidden_state[:, 0, :].squeeze().cpu()  # Move outputs to CPU to avoid GPU memory issues\n",
    "    # Use [CLS] token representation as sentence embedding\n",
    "    return outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_articles_with_tickers(base_path=\"Word2Vec/retry/new_data/articles/\"):\n",
    "    all_dfs = []\n",
    "\n",
    "    # Grab all CSVs under ticker subfolders\n",
    "    csv_files = glob(os.path.join(base_path, \"*/data_*.csv\"))\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        # Extract the ticker from folder name\n",
    "        ticker = os.path.basename(os.path.dirname(file_path)).replace(\"ticker=\", \"\")\n",
    "\n",
    "        # Load CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Add ticker column\n",
    "        df['ticker'] = ticker\n",
    "\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    # Concatenate all into a single DataFrame\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Optional: reorder columns\n",
    "    columns = ['ticker', 'guid', 'description', 'article_title', 'article_pubDate']\n",
    "    return combined_df[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_articles_with_tickers()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crap wrong ones. I need to use the dataset from the DB\n",
    "df_bert = preprocess_for_bert(df)\n",
    "# df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert['embedding'] = df_bert['full_text'].apply(get_finbert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [0.6570171, -0.31303525, -0.4963823, 0.0334446...\n",
       "1         [0.6575914, -0.39113572, -1.2864994, 1.0594414...\n",
       "2         [0.4497182, -0.5984597, -0.58793783, 0.4912451...\n",
       "3         [-0.3111353, -0.11022892, 0.055871136, 0.10007...\n",
       "4         [-0.04455838, -0.42751977, 0.0028711278, -0.14...\n",
       "                                ...                        \n",
       "363815    [9.2989336e-05, -0.76329875, -0.8743198, -0.48...\n",
       "363816    [-1.2596151, -0.43909767, -0.17488584, 0.78091...\n",
       "363817    [-0.26690745, -0.5215517, -0.9235606, 1.122709...\n",
       "363818    [-0.18933877, -0.966446, -0.66913235, -0.42958...\n",
       "363819    [-0.3939313, -0.4886067, -1.2324507, 0.643593,...\n",
       "Name: embedding, Length: 363820, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bert['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings = np.stack(df_bert['embedding'].values)\n",
    "np.save(\"Word2Vec/retry/Finbert/finbert_embeddings.npy\", embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testpy)",
   "language": "python",
   "name": "testpy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
