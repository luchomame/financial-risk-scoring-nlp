{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaea71f-7ac3-4fe9-af94-a630b03b9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBRegressor\n",
    "import pandas_market_calendars as mcal\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92a5577-937d-4f47-ac5d-3480b1ecd0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hilun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hilun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure stopwords and tokenizer are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643f033a-3010-48a3-a45a-ed20ace345e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will extract all ticker names and save it in a list \n",
    "\n",
    "# Database connection\n",
    "db_file_path = r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\financial_news.db\"\n",
    "conn = duckdb.connect(database=db_file_path, read_only=False)    \n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT DISTINCT ticker FROM \"Headlines\".\"Daily_Price_Movement\";\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and fetch results into a DataFrame\n",
    "ticker_a = conn.execute(query).fetchdf()\n",
    "\n",
    "# Convert to a unique ticker list\n",
    "# unique_tickers = ticker_a[\"ticker\"].tolist()\n",
    "# ticker_all=unique_tickers\n",
    "# ticker_file=pd.read_csv('ticker with over1000 AC.csv')\n",
    "# ticker_all = ticker_file[\"symbol\"]  # Extract column\n",
    "\n",
    "#ticker_all=['AAPL'] #uncomment this for troubleshooting with one stock\n",
    "\n",
    "# **Close Database Connection**\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d284eee-ce05-4db8-9b4b-1bd2eb9e815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYSE trading calendar\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "start_time = time.time()  # Start time measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d693af-7e4c-4fc7-88e4-ebf635a46bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the next trading day\n",
    "def next_trading_day(date):\n",
    "    date = pd.Timestamp(date)\n",
    "    while len(nyse.valid_days(start_date=date.strftime('%Y-%m-%d'), end_date=date.strftime('%Y-%m-%d'))) == 0:\n",
    "        date += pd.Timedelta(days=1)\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf28c636-c1e5-4945-abf2-648b0622ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text tokenization including NER removal\n",
    "def tokenize_text(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # Extract named entities\n",
    "    named_entities = {ent.text.lower() for ent in doc.ents}\n",
    "\n",
    "    # Tokenize and filter words\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [\n",
    "        word for word in words \n",
    "        if word.isalpha() and len(word) > 2 and \n",
    "        word not in stop_words and \n",
    "        word not in named_entities  # Remove named entities\n",
    "    ]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3282c3de-4a5b-473c-83c5-52fde43b683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate bollinger band\n",
    "def bb(price, lookback):\n",
    "    std=price.rolling(window=lookback, min_periods=lookback).std()\n",
    "    sma=price.rolling(window=lookback, min_periods=lookback).mean()\n",
    "    bottom=sma-(2*std)\n",
    "    top=sma+(2*std)\n",
    "    return sma, top, bottom\n",
    "\n",
    "#function to calculate relative strength index\n",
    "def get_rsi(price, lookback):\n",
    "    daily_ret=price.diff()\n",
    "    up = daily_ret.clip(lower=0)\n",
    "    down = -1 * daily_ret.clip(upper=0)\n",
    "    sma_up = up.rolling(window = lookback).mean()\n",
    "    sma_down = down.rolling(window = lookback).mean()  \n",
    "    rs=sma_up/sma_down\n",
    "    rsi=100-(100/(1+rs))\n",
    "    return rsi        \t   \t\t  \t\t \t\t\t  \t\t \t\t\t     \t\t\t  \t \n",
    "\n",
    "#function to calculate stochastic osillator\n",
    "def get_so(price, lookback):\n",
    "    high = price.rolling(window=lookback).max()\n",
    "    low = price.rolling(window=lookback).min()\n",
    "    K = 100*(price-low)/(high-low)\n",
    "    D = K.rolling(window=3).mean()\n",
    "\n",
    "    return(K,D)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc84f0d-11df-4576-9c8b-9c09976cea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This the main function to process a single ticker\n",
    "def process_ticker(ticker, nyse, stop_words, result_df):\n",
    "    print(f\"Processing {ticker}...\")\n",
    "    # Database connection\n",
    "    # I chose to open and close a db connection inside this function to avoid issues\n",
    "    # I encountered inaccurate results when I opened and close a connect outside of loop\n",
    "\n",
    "    #*****************************************************************************\n",
    "    #change this to your local path\n",
    "    db_file_path = r\"C:\\Users\\hilun\\OneDrive\\Desktop\\OMS\\Practitam\\financial_news.db\" \n",
    "    #*****************************************************************************\n",
    "    conn = duckdb.connect(database=db_file_path, read_only=False)    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            a.mapped_trading_date AS publish_date,\n",
    "            a.description, \n",
    "            a.article_pubDate,\n",
    "            dpm.close_price\n",
    "        FROM \"Headlines\".\"Articles_Trading_Day\" a\n",
    "        INNER JOIN \"Headlines\".\"Daily_Price_Movement\" dpm\n",
    "        ON a.mapped_trading_date = dpm.trading_date  \n",
    "        WHERE a.ticker = ?\n",
    "        AND dpm.ticker = ?;\n",
    "    \"\"\"\n",
    "    \n",
    "    news_df = conn.execute(query, [ticker, ticker]).fetchdf()\n",
    "    article_count=len(news_df)\n",
    "    if news_df.empty:\n",
    "        print(f\"Skipping {ticker}: No data found.\")\n",
    "        return\n",
    "\n",
    "    news_df[\"publish_date\"] = pd.to_datetime(news_df[\"publish_date\"]).dt.date\n",
    "    news_df[\"description\"] = news_df[\"description\"].fillna(\"\")\n",
    "\n",
    "    # Group descriptions by date\n",
    "    news_df = news_df.groupby(\"publish_date\", as_index=False).agg({\n",
    "        \"description\": lambda x: \" \".join(x),\n",
    "        \"close_price\": \"first\",\n",
    "    })\n",
    "\n",
    "    # Adjust for non-trading days\n",
    "    news_df[\"adjusted_date\"] = news_df[\"publish_date\"].apply(next_trading_day)\n",
    "\n",
    "    # Re-group after adjusting trading days\n",
    "    news_df = news_df.groupby(\"adjusted_date\", as_index=False).agg({\n",
    "        \"description\": lambda x: \" \".join(x),\n",
    "        \"close_price\": \"last\",\n",
    "    })\n",
    "    \n",
    "    news_df[\"price_change_percentage\"] = ((news_df[\"close_price\"].shift(-1) - news_df[\"close_price\"]) / news_df[\"close_price\"]) * 100\n",
    "    news_df[\"tokenized_words\"] = news_df[\"description\"].astype(str).apply(tokenize_text)\n",
    "\n",
    "    # Calculate token scores\n",
    "    unique_words = set(word for words_list in news_df[\"tokenized_words\"] for word in words_list)\n",
    "    w_count=len(unique_words)\n",
    "    word_scores = {word: [] for word in unique_words}\n",
    "\n",
    "    for _, row in news_df.iterrows():\n",
    "        words_list = row[\"tokenized_words\"]\n",
    "        price_change = row[\"price_change_percentage\"]\n",
    "        total_words = len(words_list)\n",
    "\n",
    "        if total_words > 0:\n",
    "            word_counts = {word: words_list.count(word) / total_words for word in words_list}\n",
    "            for word, ratio in word_counts.items():\n",
    "                word_scores[word].append(ratio * price_change)\n",
    "\n",
    "    tk_info = pd.DataFrame({\n",
    "        \"word\": list(word_scores.keys()),\n",
    "        \"score\": [np.mean(scores) if scores else 0 for scores in word_scores.values()]\n",
    "    }).dropna()\n",
    "    token_scores_dict = dict(zip(tk_info[\"word\"], tk_info[\"score\"]))\n",
    "    \n",
    "    def calculate_token_score(tokens):\n",
    "        return sum(token_scores_dict.get(token, 0) for token in tokens)\n",
    "\n",
    "    news_df[\"token_score\"] = news_df[\"tokenized_words\"].apply(calculate_token_score)\n",
    "\n",
    "    # Ensure no missing values in price change\n",
    "    news_df = news_df.dropna()\n",
    "    \n",
    "    # Define the SQL query\n",
    "    query = \"\"\"\n",
    "        SELECT trading_day_date, price \n",
    "        FROM Headlines.Pricing_News\n",
    "        WHERE ticker = ?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query and load the data into a pandas DataFrame\n",
    "    df_p=conn.execute(query,[ticker]).fetchdf()\n",
    "\n",
    "    # Adjust for non-trading days\n",
    "    df_p = df_p.rename(columns={\"trading_day_date\": \"trading_date\"})\n",
    "    df_p = df_p.rename(columns={\"price\": \"close_price\"})\n",
    "\n",
    "    df_p[\"adjusted_trading_date\"] =df_p[\"trading_date\"].apply(next_trading_day)\n",
    "    df_p = df_p.drop_duplicates(subset=['adjusted_trading_date'], keep='last')\n",
    "\n",
    "    # Ensure 'trading_date' is in datetime format\n",
    "    df_p['trading_date'] = pd.to_datetime(df_p['trading_date'])\n",
    "    news_df['adjusted_date'] = pd.to_datetime(news_df['adjusted_date'])  # Ensure news_df dates are also datetime\n",
    "\n",
    "    if news_df['adjusted_date'].empty:\n",
    "        print(f\"Skipping {ticker}: No data found.\")\n",
    "        return\n",
    "    \n",
    "    # Get start_date and end_date\n",
    "    start_date = news_df['adjusted_date'].iloc[0]  # First news date\n",
    "    end_date = news_df['adjusted_date'].iloc[-1]   # Last news date\n",
    "    \n",
    "    # Compute the date range\n",
    "    date_lower_bound = start_date - pd.Timedelta(days=30)\n",
    "    date_upper_bound = end_date + pd.Timedelta(days=30)\n",
    "    \n",
    "    #print(date_lower_bound, date_upper_bound )\n",
    "    \n",
    "    # Filter df_p to include only trading dates within this range\n",
    "    df_p = df_p[(df_p['trading_date'] >= date_lower_bound) & (df_p['trading_date'] <= date_upper_bound)]\n",
    "    # Convert to a pandas Series\n",
    "    price = df_p['close_price']\n",
    "    \n",
    "    #add BB columns: SMA, Bottom, Top\n",
    "    sma, bot, top=bb(price,14)\n",
    "    rsi=get_rsi(price,4)\n",
    "    #Stochastic Oscillator\n",
    "    K, D=get_so(price,14)\n",
    "    \n",
    "    df_p['bb_sma']=sma\n",
    "    df_p['bb_bottom']=bot\n",
    "    df_p['bb_top']=top\n",
    "    df_p['rsi']=rsi\n",
    "    df_p['K']=K\n",
    "    df_p['D']=D\n",
    "\n",
    "    df_p['sma_future_7'] = df_p['close_price'].rolling(window=7, min_periods=1).mean().shift(-7)\n",
    "    \n",
    "    df_p = df_p.reset_index(drop=True)  # Reset index and remove old index\n",
    "    #handling edge cases for the last 7 days\n",
    "    t_len=len(df_p)\n",
    "    for i in range(6):\n",
    "        future_values = df_p['close_price'].iloc[t_len-1-i:t_len]  # Get up to 6 future values\n",
    "        df_p.at[t_len-i-2, 'sma_future_7'] = future_values.mean()  # Assign safely  # Compute mean of available values\n",
    "\n",
    "    news_df = news_df.merge(\n",
    "    df_p[['adjusted_trading_date', 'bb_sma', 'bb_bottom', 'bb_top', 'rsi', 'K','D', 'sma_future_7']],\n",
    "    left_on='adjusted_date',\n",
    "    right_on='adjusted_trading_date',\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    news_df = news_df.drop(columns=['adjusted_trading_date'])\n",
    "    \n",
    "    #calculate the trigger column\n",
    "    news_df[\"bb_trigger\"] = 0  # Default value\n",
    "    news_df.loc[(news_df[\"close_price\"].shift(1) > news_df[\"bb_top\"].shift(1)) & (news_df[\"close_price\"] < news_df[\"bb_top\"]), \"bb_trigger\"] = -1\n",
    "    news_df.loc[(news_df[\"close_price\"].shift(1) < news_df[\"bb_bottom\"].shift(1)) & (news_df[\"close_price\"] > news_df[\"bb_bottom\"]), \"bb_trigger\"] = 1\n",
    "    # Calculate the RSI trigger column\n",
    "    news_df[\"rsi_trigger\"] = 0  # Default value\n",
    "    news_df.loc[news_df[\"rsi\"] > 70, \"rsi_trigger\"] = -1\n",
    "    news_df.loc[news_df[\"rsi\"] < 30, \"rsi_trigger\"] = 1 \n",
    "    \n",
    "    news_df['bb_sma']=sma\n",
    "    news_df['bb_bottom']=bot\n",
    "    news_df['bb_top']=top\n",
    "    news_df['rsi']=rsi\n",
    "    news_df['K']=K\n",
    "    news_df['D']=D\n",
    "\n",
    "    #************************************************************\n",
    "    #Make sure you add this file to your local path\n",
    "    #add vix column\n",
    "    vix_df = pd.read_csv(\"vixGaTechSP25.csv\")\n",
    "    #************************************************************\n",
    "    \n",
    "    # Rename columns (assumes first column is date and second is VIX value)\n",
    "    vix_df = vix_df.rename(columns={vix_df.columns[0]: \"adjusted_date\", vix_df.columns[1]: \"vix\"})\n",
    "    \n",
    "    # Convert both date columns to datetime\n",
    "    news_df[\"adjusted_date\"] = pd.to_datetime(news_df[\"adjusted_date\"])\n",
    "    vix_df[\"adjusted_date\"] = pd.to_datetime(vix_df[\"adjusted_date\"])\n",
    "\n",
    "    # Now merge\n",
    "    news_df = pd.merge(news_df, vix_df, on=\"adjusted_date\", how=\"left\")\n",
    "   \n",
    "    #select single variable\n",
    "    #X_combined = news_df[\"token_score\"].values.reshape(-1, 1)  # Use token score as feature\n",
    "    \n",
    "    # Select multiple feature columns\n",
    "    X_combined = news_df[[\"token_score\", \"vix\"]].values \n",
    "    X_combined = np.array(X_combined).reshape(-1, 2)  # 4 features   \n",
    "    # X_combined = news_df[[\"rsi\",\"K\",\"D\",'bb_sma','bb_bottom','bb_top']].values \n",
    "    # X_combined = np.array(X_combined).reshape(-1, 6)  # 4 features\n",
    "    y = news_df[\"price_change_percentage\"].values\n",
    "\n",
    "    # Train-Test Split\n",
    "    split_index = int(len(news_df) * 0.8)\n",
    "    X_train, X_test = X_combined[:split_index], X_combined[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    # Train XGBoost Model\n",
    "    xgb_model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    #4 average groups with 6 categories\n",
    "    ave = news_df[\"price_change_percentage\"].abs().mean()\n",
    "    if ave > 3:\n",
    "        l = [7, 3.5, 0, -3.5, -7]\n",
    "    elif ave >= 2:\n",
    "        l = [4, 2, 0, -2, -4]\n",
    "    elif ave >= 1:\n",
    "        l = [3, 1.5, 0, -1.5, -3]\n",
    "    else:\n",
    "        l = [2.5, 1.25, 0, -1.25, -2.5]\n",
    "\n",
    "    # Categorization/Classification function\n",
    "    def categorize_value(x):\n",
    "        if x > l[0]:\n",
    "            return 0\n",
    "        elif l[1] <= x <= l[0]:\n",
    "            return 1\n",
    "        elif l[2] <= x < l[1]:\n",
    "            return 2\n",
    "        elif l[3] <= x < l[2]:\n",
    "            return 3\n",
    "        elif l[4] <= x < l[3]:\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "        \n",
    "    start_row = split_index - 1\n",
    "\n",
    "    # Apply categorization\n",
    "    news_df.loc[news_df.index > start_row, \"actual category\"] = news_df.loc[news_df.index > start_row, \"price_change_percentage\"].apply(categorize_value)\n",
    "    news_df[\"predicted_price_change\"] = np.concatenate([np.full(split_index, np.nan), y_pred])\n",
    "    news_df.loc[news_df.index > start_row, \"predicted category\"] = news_df.loc[news_df.index > start_row, \"predicted_price_change\"].apply(categorize_value)\n",
    "\n",
    "    # Compute classification accuracy\n",
    "    df_filtered = news_df.iloc[split_index:].reset_index(drop=True)[[\"actual category\", \"predicted category\"]]\n",
    "    accuracy = accuracy_score(df_filtered[\"actual category\"], df_filtered[\"predicted category\"])\n",
    "    \n",
    "    true_p = ((news_df[\"actual category\"] > 3) & (news_df[\"predicted category\"] > 3)).sum()\n",
    "    false_neg=((news_df[\"actual category\"] > 3) & (news_df[\"predicted category\"] < 4)).sum()\n",
    "    false_p=((news_df[\"actual category\"] < 4) & (news_df[\"predicted category\"] > 3)).sum()\n",
    "    true_neg=((news_df[\"actual category\"] <4) & (news_df[\"predicted category\"] <4)).sum()\n",
    "    precision=true_p/(true_p+false_p)\n",
    "    recall=true_p/(true_p+false_neg)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    # Store results\n",
    "    result_df.loc[len(result_df)] = [ticker, mae, r2, accuracy, article_count,w_count, len(news_df),precision,recall, f1,true_p,false_neg,false_p,true_neg]\n",
    "    \n",
    "    # **Close Database Connection**\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa83cc3-31a5-44e9-9315-43ff8ad3d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL...\n",
      "Execution Time: 92.62148 seconds\n",
      "  symbol       MAE  r-square  all categories classification accuracy  \\\n",
      "0   AAPL  0.410427  0.762253                                0.759259   \n",
      "\n",
      "   article count  word count  total days used for analysis  \\\n",
      "0           5547       12082                           267   \n",
      "\n",
      "   Price drop precision  price drop recall  price drop f1 score  \\\n",
      "0                   1.0                1.0                  1.0   \n",
      "\n",
      "   true positive  false negative  false positive  true negative  \n",
      "0              5               0               0             49  \n"
     ]
    }
   ],
   "source": [
    "# Define column names\n",
    "columns = [\"symbol\", \"MAE\", \"r-square\", \"all categories classification accuracy\",\"article count\",\"word count\", \"total days used for analysis\",'Price drop precision','price drop recall','price drop f1 score','true positive','false negative','false positive','true negative']\n",
    "result_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# **Main Execution Loop**\n",
    "for ticker in ticker_all:\n",
    "    process_ticker(ticker, nyse, stop_words, result_df)\n",
    "\n",
    "end_time = time.time()  # End time measurement\n",
    "elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "\n",
    "print(f\"Execution Time: {elapsed_time:.5f} seconds\")  # Print execution time\n",
    "\n",
    "print(result_df)\n",
    "# **Save Final Results**\n",
    "result_df.to_csv(\"result_data_words_stock_score.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
