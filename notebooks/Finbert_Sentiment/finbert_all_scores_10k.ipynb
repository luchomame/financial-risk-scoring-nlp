{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference to extracting all labels \n",
    "https://datascience.stackexchange.com/questions/112438/how-to-get-all-3-labels-sentiment-from-finbert-instead-of-the-most-likely-label\n",
    "\n",
    "essentially, us the AutoModelForSequenceClassification to get all raw logits and then apply softmax ourselves \n",
    "\n",
    "normally the pipeline does the softmax and ONLY returns the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Total CPU cores available: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bokeh\n",
    "\n",
    "# !pip install pyarrow==10.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n",
      "Dask version: 2025.2.0\n",
      "PyArrow version: 19.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import pyarrow\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import duckdb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import bokeh\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Dask version:\", dask.__version__)\n",
    "print(\"PyArrow version:\", pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()  # Put model in evaluation mode\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {\"label\": \"NEUTRAL\", \"score\": 1.0, \"positive\": 0.0, \"neutral\": 1.0, \"negative\": 0.0}\n",
    "    \n",
    "    # Tokenize input text\n",
    "    # inputs = tokenizer(text[:512], return_tensors=\"pt\", truncation=True)\n",
    "    # Getting truncation warning. I'ma use tokenizer truncation instead\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits  # Get raw model outputs (logits)\n",
    "    probs = F.softmax(logits, dim=1)  # Apply softmax across dimension 1 (classes)\n",
    "\n",
    "    # Convert to a Python list\n",
    "    probs = probs.numpy()[0]  # Extract probabilities as a NumPy array\n",
    "\n",
    "    # Define label mapping\n",
    "    labels = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "    sentiment_dict = dict(zip(labels, probs))\n",
    "\n",
    "    # Get the highest-probability label\n",
    "    max_label = labels[torch.argmax(logits).item()]\n",
    "    max_score = max(probs)\n",
    "\n",
    "    return {\n",
    "        \"label\": max_label,\n",
    "        \"score\": max_score,\n",
    "        \"positive\": sentiment_dict[\"POSITIVE\"],\n",
    "        \"neutral\": sentiment_dict[\"NEUTRAL\"],\n",
    "        \"negative\": sentiment_dict[\"NEGATIVE\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CANT START A CLIENT AND CLUSTER BEFORE LOADING FINBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n",
      "{'tcp://127.0.0.1:54731': 1, 'tcp://127.0.0.1:54732': 1, 'tcp://127.0.0.1:54733': 1, 'tcp://127.0.0.1:54734': 1, 'tcp://127.0.0.1:54735': 1, 'tcp://127.0.0.1:54736': 1, 'tcp://127.0.0.1:54737': 1, 'tcp://127.0.0.1:54738': 1, 'tcp://127.0.0.1:54739': 1, 'tcp://127.0.0.1:54740': 1}\n"
     ]
    }
   ],
   "source": [
    "# Try to avoid PyArrow\n",
    "pd.options.mode.string_storage = \"python\"\n",
    "\n",
    "# cluster = LocalCluster(n_workers=num_cores//2, threads_per_worker=1)\n",
    "cluster = LocalCluster(n_workers=10, threads_per_worker=1) # upping to full CPU cores when not using my laptop\n",
    "\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "client = Client(cluster)\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "print(client.dashboard_link)\n",
    "print(client.ncores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "DB_PATH = Path(os.getenv(\"DB_PATH\"))\n",
    "DB_FILE = os.getenv(\"DB_FILE\")\n",
    "duckdb_path = DB_PATH / DB_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b12c44eff44598a0ed6a0c045d102b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "con = duckdb.connect(duckdb_path, read_only=True)\n",
    "\n",
    "# try writing to parquet instead\n",
    "df = con.execute(\"SELECT cik, filing_ts, item_filing, type, item_description FROM sp500.sec_item_filings\").fetchdf()\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# testing only\n",
    "# filtered_df = df.head(1000)\n",
    "# len(filtered_df)\n",
    "# ddf = dd.read_csv(\"articles_db.csv\", assume_missing=True, dtype={'guid': 'object', 'description': 'object', 'article_title': 'object', 'ticker': 'object'})\n",
    "# read parquet \n",
    "# ddf = dd.read_parquet(file_name, engine='pyarrow')\n",
    "\n",
    "# read from articles_partitioned output_dir \n",
    "ddf = dd.from_pandas(df, npartitions=10)\n",
    "# check partitions in ddf \n",
    "print(ddf.npartitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jovan\\anaconda3\\envs\\cs7643-a2\\Lib\\site-packages\\distributed\\client.py:3370: UserWarning: Sending large graph of size 2.80 GiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 19:15:15,669 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:54880 (pid=20988) exceeded 95% memory budget. Restarting...\n",
      "2025-02-21 19:15:15,748 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:54880' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('frompandas-06b75ce0b3c41958697107386cfac4c2', 1), ('frompandas-06b75ce0b3c41958697107386cfac4c2', 9)} (stimulus_id='handle-worker-cleanup-1740183315.7476473')\n",
      "2025-02-21 19:15:20,501 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-02-21 21:19:25,214 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 21:21:05,235 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 21:29:05,699 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:54913'.\n",
      "2025-02-21 21:29:09,215 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n",
      "2025-02-21 21:38:34,172 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:54912'.\n",
      "2025-02-21 21:38:38,161 - distributed.nanny - WARNING - Worker process still alive after 4.0 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "# Enable Progress Bar\n",
    "with ProgressBar():\n",
    "    # Process description sentiment\n",
    "    ddf['finbert_description'] = ddf.map_partitions(\n",
    "        lambda df: df['item_description'].apply(classify_sentiment), meta=(\"x\", \"object\")\n",
    "    )\n",
    "    ddf['finbert_description_label'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['label']), meta=(\"x\", \"str\")\n",
    "    )\n",
    "    ddf['finbert_description_score'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['score']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_description_positive'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['positive']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_description_neutral'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['neutral']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf['finbert_description_negative'] = ddf.map_partitions(\n",
    "        lambda df: df['finbert_description'].apply(lambda x: x['negative']), meta=(\"x\", \"float\")\n",
    "    )\n",
    "    ddf.to_csv(\"10k_articles_with_finbert_scores.csv\")\n",
    "\n",
    "# Convert back to Pandas\n",
    "# df_final = ddf.compute()\n",
    "\n",
    "# Save results\n",
    "# df_final.to_csv(\"articles_with_all_finbert_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.close()\n",
    "\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
