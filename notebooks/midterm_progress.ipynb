{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btada\\AppData\\Local\\Temp\\ipykernel_23356\\1377331886.py:38: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  market_data[\"price_change\"] = market_data.groupby(\"ticker\")[\"price\"].pct_change() * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notable Price Movement Events with Articles\n",
      "  trading_day_date ticker      price     volume  price_change  \\\n",
      "0       2024-11-21    ORI  38.220001   852850.0      2.001595   \n",
      "1       2024-12-10    ORI  35.980000  1158504.0     -3.097230   \n",
      "2       2024-12-17    ORI  36.430000  1479164.0     -2.853334   \n",
      "3       2024-12-18    ORI  35.630001  1411634.0     -2.195990   \n",
      "4       2022-01-03   NUVL  18.590000    37765.0     -2.363449   \n",
      "\n",
      "  mapped_trading_date                                      article_title  \n",
      "0          2024-11-21  With 73% ownership of the shares, Old Republic...  \n",
      "1                 NaT                                                NaN  \n",
      "2                 NaT                                                NaN  \n",
      "3                 NaT                                                NaN  \n",
      "4                 NaT                                                NaN  \n",
      "\n",
      "‚úÖ Aggregated Results ‚Äì Sample Price Movements:\n",
      "  trading_day_date ticker  price_change     volume\n",
      "0       2022-01-01   ACIW     48.290600   461476.0\n",
      "1       2022-01-01    AEL    -29.645699   216736.0\n",
      "2       2022-01-01    AEO     51.889622  3173105.0\n",
      "3       2022-01-01     AI     19.274807  3109928.0\n",
      "4       2022-01-01    ALB    104.469513   378976.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# Path to DuckDB database\n",
    "db_path = r'C:\\Users\\btada\\Documents\\financial_news.db'\n",
    "\n",
    "# Establish a single DuckDB connection\n",
    "conn = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "# Optimized SQL Queries (with JOIN to remove redundant filtering)\n",
    "market_data_query = \"\"\"\n",
    "SELECT \n",
    "    md.trading_day_date, \n",
    "    md.ticker, \n",
    "    md.price, \n",
    "    md.volume \n",
    "FROM Headlines.Market_Data_Daily_Processing md\n",
    "JOIN Headlines.Trading_Calendar tc \n",
    "ON md.trading_day_date = tc.trading_date;\n",
    "\"\"\"\n",
    "\n",
    "articles_trading_day_query = \"\"\"\n",
    "SELECT \n",
    "    mapped_trading_date, \n",
    "    ticker, \n",
    "    article_title \n",
    "FROM Headlines.Articles_Trading_Day;\n",
    "\"\"\"\n",
    "\n",
    "# Fetch Data\n",
    "market_data = conn.execute(market_data_query).fetchdf()\n",
    "articles_trading_day = conn.execute(articles_trading_day_query).fetchdf()\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "# Ensure price changes are computed correctly per stock\n",
    "market_data[\"price_change\"] = market_data.groupby(\"ticker\")[\"price\"].pct_change() * 100\n",
    "\n",
    "# Identify notable price movement events (>2% change)\n",
    "notable_events = market_data[abs(market_data[\"price_change\"]) > 2]\n",
    "\n",
    "# Merge with articles to find news coverage on those days\n",
    "notable_events_articles = notable_events.merge(\n",
    "    articles_trading_day, \n",
    "    left_on=[\"trading_day_date\", \"ticker\"], \n",
    "    right_on=[\"mapped_trading_date\", \"ticker\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nNotable Price Movement Events with Articles\")\n",
    "print(notable_events_articles.head())\n",
    "\n",
    "# Aggregate Price Movements for Modeling (Create aggregated_results)\n",
    "aggregated_results = notable_events_articles.groupby([\"trading_day_date\", \"ticker\"]).agg(\n",
    "    price_change=(\"price_change\", \"first\"),  # Since price change is the same across duplicate rows\n",
    "    volume=(\"volume\", \"first\"),  # Volume is the same across duplicate rows\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\n‚úÖ Aggregated Results ‚Äì Sample Price Movements:\")\n",
    "print(aggregated_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Engineering Completed ‚Äì Sample of Aggregated Classification Summary:\n",
      "  trading_day_date ticker  total_articles  earnings_articles  \\\n",
      "0       2022-01-01   ACIW               0                  0   \n",
      "1       2022-01-01    AEL               0                  0   \n",
      "2       2022-01-01    AEO               0                  0   \n",
      "3       2022-01-01     AI               0                  0   \n",
      "4       2022-01-01    ALB               0                  0   \n",
      "\n",
      "   high_risk_articles  general_articles  \n",
      "0                   0                 1  \n",
      "1                   0                 1  \n",
      "2                   0                 1  \n",
      "3                   0                 1  \n",
      "4                   0                 1  \n"
     ]
    }
   ],
   "source": [
    "# üõ† Feature Engineering\n",
    "\n",
    "# Count number of articles per trading day per ticker\n",
    "article_counts = notable_events_articles.groupby([\"trading_day_date\", \"ticker\"]).size().reset_index(name=\"article_count\")\n",
    "\n",
    "# Merge article counts with notable price movement events\n",
    "notable_events_articles_aggregated = notable_events_articles.merge(\n",
    "    article_counts, on=[\"trading_day_date\", \"ticker\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Identify earnings-related articles\n",
    "earnings_keywords = [\"earnings\", \"q1\", \"q2\", \"q3\", \"q4\", \"report\", \"guidance\", \"miss\", \"beat\"]\n",
    "notable_events_articles_aggregated[\"is_earnings_related\"] = notable_events_articles_aggregated[\"article_title\"].str.contains(\n",
    "    \"|\".join(earnings_keywords), case=False, na=False\n",
    ")\n",
    "\n",
    "# Load Loughran-McDonald Dictionary for High-Risk Words\n",
    "lm_dict_path = r\"C:\\Users\\btada\\Documents\\Loughran-McDonald_MasterDictionary_1993-2023.csv\"\n",
    "lm_dict = pd.read_csv(lm_dict_path)\n",
    "\n",
    "high_risk_words = lm_dict.query(\"Negative != 0 or Uncertainty != 0 or Litigious != 0\")[\"Word\"].str.lower().tolist()\n",
    "\n",
    "# Count high-risk words in article titles\n",
    "notable_events_articles_aggregated[\"high_risk_word_count\"] = notable_events_articles_aggregated[\"article_title\"].apply(\n",
    "    lambda x: sum(word in x.lower() for word in high_risk_words) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "# Classify articles\n",
    "def classify_article(row):\n",
    "    if row[\"is_earnings_related\"]:\n",
    "        return \"Earnings\"\n",
    "    elif row[\"high_risk_word_count\"] > 0:\n",
    "        return \"High-Risk\"\n",
    "    else:\n",
    "        return \"General\"\n",
    "\n",
    "notable_events_articles_aggregated[\"article_classification\"] = notable_events_articles_aggregated.apply(classify_article, axis=1)\n",
    "\n",
    "# Aggregate Classification Summary\n",
    "article_classification_summary = notable_events_articles_aggregated.groupby([\"trading_day_date\", \"ticker\"]).agg(\n",
    "    total_articles=(\"article_title\", \"count\"),\n",
    "    earnings_articles=(\"is_earnings_related\", \"sum\"),\n",
    "    high_risk_articles=(\"high_risk_word_count\", \"sum\"),\n",
    "    general_articles=(\"article_classification\", lambda x: (x == \"General\").sum())\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nFeature Engineering Completed ‚Äì Sample of Aggregated Classification Summary:\")\n",
    "print(article_classification_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk Score Computation Complete ‚Äì Sample Risk Scores:\n",
      "  trading_day_date ticker  total_articles  total_high_risk_articles  \\\n",
      "0       2022-01-01   ACIW               1                         0   \n",
      "1       2022-01-01    AEL               1                         0   \n",
      "2       2022-01-01    AEO               1                         0   \n",
      "3       2022-01-01     AI               1                         0   \n",
      "4       2022-01-01    ALB               1                         0   \n",
      "\n",
      "   total_earnings_articles  avg_news_risk_score  avg_price_impact_score  \\\n",
      "0                        0                  1.0                0.144540   \n",
      "1                        0                  1.0                0.088733   \n",
      "2                        0                  1.0                0.155312   \n",
      "3                        0                  1.0                0.057692   \n",
      "4                        0                  1.0                0.312690   \n",
      "\n",
      "   avg_final_risk_score  \n",
      "0              0.657816  \n",
      "1              0.635493  \n",
      "2              0.662125  \n",
      "3              0.623077  \n",
      "4              0.725076  \n"
     ]
    }
   ],
   "source": [
    "# üìä Risk Score Computation\n",
    "\n",
    "# News Risk Score: Weighted sum of high-risk, earnings, and general articles\n",
    "notable_events_articles_aggregated[\"news_risk_score\"] = (\n",
    "    notable_events_articles_aggregated[\"high_risk_word_count\"] * 3 +  # High-Risk Words ‚Üí 3x weight\n",
    "    notable_events_articles_aggregated[\"is_earnings_related\"].astype(int) * 2 +  # Earnings Articles ‚Üí 2x weight\n",
    "    notable_events_articles_aggregated[\"article_count\"] * 1  # General Articles ‚Üí 1x weight\n",
    ")\n",
    "\n",
    "# Normalize price change to scale price impact between 0 and 10\n",
    "max_price_change = notable_events_articles_aggregated[\"price_change\"].abs().max()\n",
    "notable_events_articles_aggregated[\"price_impact_score\"] = (\n",
    "    notable_events_articles_aggregated[\"price_change\"].abs() / max_price_change\n",
    ") * 10\n",
    "\n",
    "# Final Weighted Risk Score: News (60%) + Price Impact (40%)\n",
    "notable_events_articles_aggregated[\"final_risk_score\"] = (\n",
    "    notable_events_articles_aggregated[\"news_risk_score\"] * 0.6 +\n",
    "    notable_events_articles_aggregated[\"price_impact_score\"] * 0.4\n",
    ")\n",
    "\n",
    "# Aggregate Risk Scores per Stock per Trading Day (Ensure no double counting)\n",
    "risk_score_summary = notable_events_articles_aggregated.groupby([\"trading_day_date\", \"ticker\"]).agg(\n",
    "    total_articles=(\"article_count\", \"sum\"),\n",
    "    total_high_risk_articles=(\"high_risk_word_count\", \"sum\"),\n",
    "    total_earnings_articles=(\"is_earnings_related\", \"sum\"),\n",
    "    avg_news_risk_score=(\"news_risk_score\", \"mean\"),\n",
    "    avg_price_impact_score=(\"price_impact_score\", \"mean\"),\n",
    "    avg_final_risk_score=(\"final_risk_score\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nRisk Score Computation Complete ‚Äì Sample Risk Scores:\")\n",
    "print(risk_score_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Current Risk Score Computation ‚Äì Explanation**\n",
    "\n",
    "The **Risk Score** in this notebook is designed to measure **the potential market impact of news articles** on **a stock‚Äôs daily price movement**. It combines **textual analysis from news articles** with **actual price changes** to capture **the relationship between news sentiment and stock volatility**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏èComponents of the Risk Score**\n",
    "The risk score is calculated based on **two primary factors**:\n",
    "\n",
    "#### **1. News-Based Risk Indicators (Weighted Scoring System)**  \n",
    "Each news article is evaluated based on **its content** and **classified** into:\n",
    "| **Indicator**            | **Description**                                        | **Weight** |\n",
    "|--------------------------|--------------------------------------------------------|------------|\n",
    "| **High-Risk Words**       | Articles containing **negative, uncertainty, or legal terms** (e.g., \"lawsuit\", \"bankrupt\", \"uncertain\") from the **Loughran-McDonald Dictionary**. | **+3 per occurrence** |\n",
    "| **Earnings Mentions**     | Articles containing **earnings-related terms** (e.g., \"earnings\", \"report\", \"guidance\", \"miss\", \"beat\"). | **+2 per article** |\n",
    "| **General News Volume**   | **All other articles** that do not fall into the above categories. | **+1 per article** |\n",
    "\n",
    "This results in a **news risk score** per trading day:\n",
    "\\[\n",
    "\\text{News Risk Score} = 3 \\times (\\text{High-Risk Words}) + 2 \\times (\\text{Earnings Articles}) + 1 \\times (\\text{General Articles})\n",
    "\\]\n",
    "\n",
    "#### **2. Price Impact Normalization**\n",
    "Price movement is also factored in, as **large price swings** (up or down) often indicate **high market volatility**.  \n",
    "Price impact is **normalized** to a **0-10 scale** based on the **largest observed price change** across the dataset:\n",
    "\\[\n",
    "\\text{Price Impact Score} = \\left(\\frac{\\left|\\text{Price Change (\\%)}\\right|}{\\text{Max Price Change (\\%)}}\\right) \\times 10\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **2Final Risk Score Formula**\n",
    "The **final risk score** combines **news risk indicators** and **price impact** with **a weighted formula**:\n",
    "\\[\n",
    "\\text{Final Risk Score} = (0.6 \\times \\text{News Risk Score}) + (0.4 \\times \\text{Price Impact Score})\n",
    "\\]\n",
    "\n",
    "- **60% Weight ‚Üí News Risk (Content/Sentiment)**  \n",
    "- **40% Weight ‚Üí Price Impact (Market Reaction)**\n",
    "\n",
    "This **weighted approach** reflects the idea that **news sentiment often drives price changes**, but **large price swings** themselves may **indicate risk** regardless of news coverage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "| **Risk Component**         | **Purpose**                                                      | **Weighting in Final Score** |\n",
    "|----------------------------|-------------------------------------------------------------------|-------------------------------|\n",
    "| **News Risk Indicators**    | Evaluate **news content** using high-risk words, earnings, and volume. | **60%** |\n",
    "| **Price Impact Normalization** | Capture **market volatility** based on daily price movements.   | **40%** |\n",
    "\n",
    "The **Risk Score** aims to **quantify the relationship between market sentiment and price volatility**, providing a **composite indicator** that can be **used for modeling and prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Approach?**\n",
    "- **High-Risk Words & Earnings are prioritized** because **negative sentiment or financial disclosures** often **signal uncertainty**.\n",
    "- **Price Impact adds market confirmation**, ensuring that **large movements** are also flagged as **risky** even if **news coverage is light**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\btada/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison\n",
      "+----------------------+---------+----------+-----------+\n",
      "|                      |     MAE |      MSE |        R¬≤ |\n",
      "|----------------------+---------+----------+-----------|\n",
      "| RandomForest         | 4.9505  |  87.1631 | 0.673869  |\n",
      "| Ridge Regression     | 6.1935  | 266.135  | 0.0042229 |\n",
      "| XGBoost              | 5.72131 |  90.057  | 0.663041  |\n",
      "| Decision Tree        | 4.77659 | 133.238  | 0.501475  |\n",
      "| Neural Network (MLP) | 5.97711 |  90.5466 | 0.661209  |\n",
      "+----------------------+---------+----------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\btada\\Documents\\OMSAPracticum\\new_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Final Modeling DataFrame: Merge Risk Scores with Price Changes from aggregated_results\n",
    "risk_price_validation = risk_score_summary.merge(\n",
    "    aggregated_results[[\"trading_day_date\", \"ticker\", \"price_change\"]],\n",
    "    on=[\"trading_day_date\", \"ticker\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 1Ô∏è‚É£ Lagged Risk Score (Previous Day's Risk)\n",
    "risk_price_validation[\"lagged_risk_score\"] = risk_price_validation.groupby(\"ticker\")[\"avg_final_risk_score\"].shift(1)\n",
    "\n",
    "# 2Ô∏è‚É£ Sentiment-Weighted Risk Score\n",
    "# Merge back article titles to get sentiment\n",
    "risk_price_validation = risk_price_validation.merge(\n",
    "    notable_events_articles_aggregated[[\"trading_day_date\", \"ticker\", \"article_title\"]],\n",
    "    on=[\"trading_day_date\", \"ticker\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "risk_price_validation[\"sentiment_score\"] = risk_price_validation[\"article_title\"].fillna(\"\").apply(\n",
    "    lambda x: sia.polarity_scores(x)[\"compound\"]\n",
    ")\n",
    "\n",
    "risk_price_validation[\"adjusted_risk_score\"] = (\n",
    "    risk_price_validation[\"avg_final_risk_score\"] + (-risk_price_validation[\"sentiment_score\"] * 5)\n",
    ")\n",
    "\n",
    "# Optional: Clean up the DataFrame if you no longer need article titles\n",
    "risk_price_validation.drop(columns=[\"article_title\", \"sentiment_score\"], inplace=True)\n",
    "\n",
    "\n",
    "# Feature Selection & Data Preparation\n",
    "features = [\n",
    "    \"avg_final_risk_score\", \"adjusted_risk_score\", \"lagged_risk_score\", \n",
    "    \"total_articles\", \"total_high_risk_articles\", \"total_earnings_articles\"\n",
    "]\n",
    "target = \"price_change\"\n",
    "\n",
    "ml_data = risk_price_validation.dropna(subset=features + [target])\n",
    "\n",
    "X = ml_data[features]\n",
    "y = ml_data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42, objective=\"reg:squarederror\"),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    #\"Support Vector Regression (SVR)\": SVR(kernel=\"linear\"),\n",
    "    \"Neural Network (MLP)\": MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=200, warm_start=True, random_state=42),\n",
    "}\n",
    "\n",
    "# Train & Evaluate Models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\"MAE\": mae, \"MSE\": mse, \"R¬≤\": r2}\n",
    "\n",
    "# Display Model Performance\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "model_comparison = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison\")\n",
    "print(tabulate(model_comparison, headers=\"keys\", tablefmt=\"psql\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Approach, Results, and Next Steps Summary**\n",
    "\n",
    "---\n",
    "\n",
    "## **Approach Summary**\n",
    "This project aims to **predict daily stock price movements** by **analyzing news sentiment and price volatility**.  \n",
    "A **Risk Score** was constructed to **quantify the relationship between market sentiment and stock volatility** using the following approach:\n",
    "\n",
    "---\n",
    "\n",
    "### **Risk Score Computation**\n",
    "1. **News-Based Indicators (Weighted System)**:\n",
    "   - **High-Risk Words (Loughran-McDonald Dictionary)** ‚Üí **+3 per occurrence**.\n",
    "   - **Earnings-Related Articles** ‚Üí **+2 per article**.\n",
    "   - **General News Volume** ‚Üí **+1 per article**.\n",
    "\n",
    "2. **Price Impact Normalization**:\n",
    "   - **Price changes are scaled** to **0-10** based on the **largest observed daily movement**.\n",
    "\n",
    "3. **Final Risk Score**:\n",
    "   \\[\n",
    "   \\text{Final Risk Score} = (0.6 \\times \\text{News Risk Score}) + (0.4 \\times \\text{Price Impact Score})\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Engineering for Modeling**\n",
    "- **Lagged Risk Score** (previous day‚Äôs risk) ‚Üí captures potential **momentum effects**.\n",
    "- **Adjusted Risk Score (VADER Sentiment)** ‚Üí adjusts the risk score based on **article sentiment polarity**.\n",
    "- **Final features**:\n",
    "  - `avg_final_risk_score`  \n",
    "  - `adjusted_risk_score`  \n",
    "  - `lagged_risk_score`  \n",
    "  - `total_articles`  \n",
    "  - `total_high_risk_articles`  \n",
    "  - `total_earnings_articles`\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Training**\n",
    "Models Trained:\n",
    "| **Model**              | **Key Characteristics** |\n",
    "|------------------------|--------------------------|\n",
    "| **RandomForest**        | Non-linear, robust to noise. |\n",
    "| **Ridge Regression**    | Linear model with L2 regularization. |\n",
    "| **XGBoost**             | Gradient boosting model; often excels in structured data. |\n",
    "| **Decision Tree**        | Simple, interpretable, but prone to overfitting. |\n",
    "| **Neural Network (MLP)** | Captures complex patterns but requires tuning. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Results Summary**\n",
    "| **Model**              | **MAE** | **MSE** | **R¬≤** |\n",
    "|------------------------|---------|---------|--------|\n",
    "| **RandomForest**        | **4.95** | **87.16** | **0.674** |\n",
    "| **Ridge Regression**    | 6.19    | 266.13  | 0.004  |\n",
    "| **XGBoost**             | 5.72    | 90.06   | 0.663  |\n",
    "| **Decision Tree**        | 4.78    | 133.24  | 0.501  |\n",
    "| **Neural Network (MLP)** | 5.98    | 90.55   | 0.661  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "- **RandomForest** and **XGBoost** performed the best, achieving **R¬≤ ~ 0.67**, suggesting that **the Risk Score and sentiment features capture some meaningful relationship** with price changes.\n",
    "- **Linear models like Ridge Regression performed poorly**, indicating **the relationship between news and price movements is non-linear**.\n",
    "- **Neural Networks and Decision Trees performed moderately well** but **were outperformed by ensemble models like RandomForest**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps**\n",
    "### **Incorporate FinBERT Sentiment Scores**\n",
    "While **VADER** sentiment scores were useful, **FinBERT** is a **finance-specific sentiment model** trained on **financial news and filings**.  \n",
    "**Replacing or combining VADER with FinBERT** could **improve sentiment precision** and **further enhance the Adjusted Risk Score**.\n",
    "\n",
    "#### **Integrate FinBERT Sentiment (Future Update):**\n",
    "1. **Join `finbert_sentiment` Table** (from the ERD you shared) with the `notable_events_articles_aggregated` DataFrame.\n",
    "2. **Use the sentiment polarity/score from FinBERT** to replace **or adjust the VADER sentiment score**.\n",
    "3. **Update the `adjusted_risk_score` feature** to incorporate **FinBERT sentiment**.\n",
    "4. **Compare Model Performance** before/after.\n",
    "\n",
    "---\n",
    "\n",
    "### **Article-Level Modeling (Alternative Approach)**  \n",
    "Instead of **aggregating by day**, **each article becomes one observation**.  \n",
    "This approach allows us to ask:  \n",
    "**‚ÄúGiven this article title, what is the expected price change?‚Äù**\n",
    "\n",
    "#### **Steps to Implement:**\n",
    "1. **Treat Each Article as a Row**.\n",
    "2. **Features**:\n",
    "   - **High-Risk Word Count** (title & description).\n",
    "   - **FinBERT Sentiment Score**.\n",
    "   - **VADER Sentiment (optional for comparison)**.\n",
    "   - **Article Metadata** (e.g., publication time, source).\n",
    "   - **Earnings mention flag**.\n",
    "3. **Target Variable: Price Movement Window**:\n",
    "   - **Same day price change**.\n",
    "   - **T+1 or T+2 price change**.\n",
    "   - **Define a window** (e.g., **T to T+1 price change**).\n",
    "4. **Model Training**:\n",
    "   - Regression models to **predict price change** from **article features**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hybrid Approach (Article + Daily Signals)**\n",
    "This **keeps the daily aggregation but introduces article-level features** as **additional signals**:\n",
    "| **Granular Features**                        | **Example Integration**                               |\n",
    "|-----------------------------------------------|--------------------------------------------------------|\n",
    "| **Was there a High-Risk Article today?**      | Binary 0/1 feature.                                    |\n",
    "| **Sentiment of the most negative article?**   | Min sentiment score for the day.                       |\n",
    "| **How many articles had sentiment < -0.5?**   | Count of highly negative articles.                     |\n",
    "| **Article Volume for the day?**               | Already present (`total_articles`).                    |\n",
    "\n",
    "#### **Steps to Implement:**\n",
    "1. **Extract these granular features during Risk Score Computation.**\n",
    "2. **Add them to `risk_price_validation`** as **additional input features**.\n",
    "3. **Evaluate whether these ‚Äúmost negative‚Äù or ‚Äúhigh-risk‚Äù articles drive stronger market reactions**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Next Steps**\n",
    "| **Priority** | **Action**                                   | **Expected Benefit**                                         |\n",
    "|--------------|----------------------------------------------|--------------------------------------------------------------|\n",
    "| **High**     | **Integrate `finbert_sentiment` as a feature**| More precise **finance-specific sentiment analysis**.         |\n",
    "| **Medium**   | **Hybrid Approach (Add Article-Level Granular Features)** | Capture **maximum negative sentiment or high-risk counts**.  |\n",
    "| **Low**      | **Explore Full Article-Level Model**           | Granular prediction from **individual articles**, but requires **restructuring the data**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Potential Plan Moving Forward**\n",
    "| **Step**                     | **Description**                                 | **Priority** |\n",
    "|------------------------------|--------------------------------------------------|--------------|\n",
    "| **1. Integrate `finbert_sentiment`** | Replace or supplement VADER with FinBERT.        | **High**    |\n",
    "| **2. Add Granular Features** | Track **most negative sentiment & high-risk articles** per day. | **Medium**  |\n",
    "| **3. Explore Article-Level Model** | Build **article-level dataset** for price prediction. | **Low**     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
