{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\btada\\AppData\\Local\\Temp\\ipykernel_23356\\1377331886.py:38: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  market_data[\"price_change\"] = market_data.groupby(\"ticker\")[\"price\"].pct_change() * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notable Price Movement Events with Articles\n",
      "  trading_day_date ticker      price     volume  price_change  \\\n",
      "0       2024-11-21    ORI  38.220001   852850.0      2.001595   \n",
      "1       2024-12-10    ORI  35.980000  1158504.0     -3.097230   \n",
      "2       2024-12-17    ORI  36.430000  1479164.0     -2.853334   \n",
      "3       2024-12-18    ORI  35.630001  1411634.0     -2.195990   \n",
      "4       2022-01-03   NUVL  18.590000    37765.0     -2.363449   \n",
      "\n",
      "  mapped_trading_date                                      article_title  \n",
      "0          2024-11-21  With 73% ownership of the shares, Old Republic...  \n",
      "1                 NaT                                                NaN  \n",
      "2                 NaT                                                NaN  \n",
      "3                 NaT                                                NaN  \n",
      "4                 NaT                                                NaN  \n",
      "\n",
      "✅ Aggregated Results – Sample Price Movements:\n",
      "  trading_day_date ticker  price_change     volume\n",
      "0       2022-01-01   ACIW     48.290600   461476.0\n",
      "1       2022-01-01    AEL    -29.645699   216736.0\n",
      "2       2022-01-01    AEO     51.889622  3173105.0\n",
      "3       2022-01-01     AI     19.274807  3109928.0\n",
      "4       2022-01-01    ALB    104.469513   378976.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# Path to DuckDB database\n",
    "db_path = r'C:\\Users\\btada\\Documents\\financial_news.db'\n",
    "\n",
    "# Establish a single DuckDB connection\n",
    "conn = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "# Optimized SQL Queries (with JOIN to remove redundant filtering)\n",
    "market_data_query = \"\"\"\n",
    "SELECT \n",
    "    md.trading_day_date, \n",
    "    md.ticker, \n",
    "    md.price, \n",
    "    md.volume \n",
    "FROM Headlines.Market_Data_Daily_Processing md\n",
    "JOIN Headlines.Trading_Calendar tc \n",
    "ON md.trading_day_date = tc.trading_date;\n",
    "\"\"\"\n",
    "\n",
    "articles_trading_day_query = \"\"\"\n",
    "SELECT \n",
    "    mapped_trading_date, \n",
    "    ticker, \n",
    "    article_title \n",
    "FROM Headlines.Articles_Trading_Day;\n",
    "\"\"\"\n",
    "\n",
    "# Fetch Data\n",
    "market_data = conn.execute(market_data_query).fetchdf()\n",
    "articles_trading_day = conn.execute(articles_trading_day_query).fetchdf()\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "# Ensure price changes are computed correctly per stock\n",
    "market_data[\"price_change\"] = market_data.groupby(\"ticker\")[\"price\"].pct_change() * 100\n",
    "\n",
    "# Identify notable price movement events (>2% change)\n",
    "notable_events = market_data[abs(market_data[\"price_change\"]) > 2]\n",
    "\n",
    "# Merge with articles to find news coverage on those days\n",
    "notable_events_articles = notable_events.merge(\n",
    "    articles_trading_day, \n",
    "    left_on=[\"trading_day_date\", \"ticker\"], \n",
    "    right_on=[\"mapped_trading_date\", \"ticker\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nNotable Price Movement Events with Articles\")\n",
    "print(notable_events_articles.head())\n",
    "\n",
    "# Aggregate Price Movements for Modeling (Create aggregated_results)\n",
    "aggregated_results = notable_events_articles.groupby([\"trading_day_date\", \"ticker\"]).agg(\n",
    "    price_change=(\"price_change\", \"first\"),  # Since price change is the same across duplicate rows\n",
    "    volume=(\"volume\", \"first\"),  # Volume is the same across duplicate rows\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\n✅ Aggregated Results – Sample Price Movements:\")\n",
    "print(aggregated_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Engineering Completed – Sample of Aggregated Classification Summary:\n",
      "  trading_day_date ticker  total_articles  earnings_articles  \\\n",
      "0       2022-01-01   ACIW               0                  0   \n",
      "1       2022-01-01    AEL               0                  0   \n",
      "2       2022-01-01    AEO               0                  0   \n",
      "3       2022-01-01     AI               0                  0   \n",
      "4       2022-01-01    ALB               0                  0   \n",
      "\n",
      "   high_risk_articles  general_articles  \n",
      "0                   0                 1  \n",
      "1                   0                 1  \n",
      "2                   0                 1  \n",
      "3                   0                 1  \n",
      "4                   0                 1  \n"
     ]
    }
   ],
   "source": [
    "# 🛠 Feature Engineering\n",
    "\n",
    "# Count number of articles per trading day per ticker\n",
    "article_counts = notable_events_articles.groupby([\"trading_day_date\", \"ticker\"]).size().reset_index(name=\"article_count\")\n",
    "\n",
    "# Merge article counts with notable price movement events\n",
    "notable_events_articles_aggregated = notable_events_articles.merge(\n",
    "    article_counts, on=[\"trading_day_date\", \"ticker\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Identify earnings-related articles\n",
    "earnings_keywords = [\"earnings\", \"q1\", \"q2\", \"q3\", \"q4\", \"report\", \"guidance\", \"miss\", \"beat\"]\n",
    "notable_events_articles_aggregated[\"is_earnings_related\"] = notable_events_articles_aggregated[\"article_title\"].str.contains(\n",
    "    \"|\".join(earnings_keywords), case=False, na=False\n",
    ")\n",
    "\n",
    "# Load Loughran-McDonald Dictionary for High-Risk Words\n",
    "lm_dict_path = r\"C:\\Users\\btada\\Documents\\Loughran-McDonald_MasterDictionary_1993-2023.csv\"\n",
    "lm_dict = pd.read_csv(lm_dict_path)\n",
    "\n",
    "high_risk_words = lm_dict.query(\"Negative != 0 or Uncertainty != 0 or Litigious != 0\")[\"Word\"].str.lower().tolist()\n",
    "\n",
    "# Count high-risk words in article titles\n",
    "notable_events_articles_aggregated[\"high_risk_word_count\"] = notable_events_articles_aggregated[\"article_title\"].apply(\n",
    "    lambda x: sum(word in x.lower() for word in high_risk_words) if pd.notna(x) else 0\n",
    ")\n",
    "\n",
    "# Classify articles\n",
    "def classify_article(row):\n",
    "    if row[\"is_earnings_related\"]:\n",
    "        return \"Earnings\"\n",
    "    elif row[\"high_risk_word_count\"] > 0:\n",
    "        return \"High-Risk\"\n",
    "    else:\n",
    "        return \"General\"\n",
    "\n",
    "notable_events_articles_aggregated[\"article_classification\"] = notable_events_articles_aggregated.apply(classify_article, axis=1)\n",
    "\n",
    "# Aggregate Classification Summary\n",
    "article_classification_summary = notable_events_articles_aggregated.groupby([\"trading_day_date\", \"ticker\"]).agg(\n",
    "    total_articles=(\"article_title\", \"count\"),\n",
    "    earnings_articles=(\"is_earnings_related\", \"sum\"),\n",
    "    high_risk_articles=(\"high_risk_word_count\", \"sum\"),\n",
    "    general_articles=(\"article_classification\", lambda x: (x == \"General\").sum())\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nFeature Engineering Completed – Sample of Aggregated Classification Summary:\")\n",
    "print(article_classification_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk Score Computation Complete – Sample Risk Scores:\n",
      "  trading_day_date ticker  total_articles  total_high_risk_articles  \\\n",
      "0       2022-01-01   ACIW               1                         0   \n",
      "1       2022-01-01    AEL               1                         0   \n",
      "2       2022-01-01    AEO               1                         0   \n",
      "3       2022-01-01     AI               1                         0   \n",
      "4       2022-01-01    ALB               1                         0   \n",
      "\n",
      "   total_earnings_articles  avg_news_risk_score  avg_price_impact_score  \\\n",
      "0                        0                  1.0                0.144540   \n",
      "1                        0                  1.0                0.088733   \n",
      "2                        0                  1.0                0.155312   \n",
      "3                        0                  1.0                0.057692   \n",
      "4                        0                  1.0                0.312690   \n",
      "\n",
      "   avg_final_risk_score  \n",
      "0              0.657816  \n",
      "1              0.635493  \n",
      "2              0.662125  \n",
      "3              0.623077  \n",
      "4              0.725076  \n"
     ]
    }
   ],
   "source": [
    "# 📊 Risk Score Computation\n",
    "\n",
    "# News Risk Score: Weighted sum of high-risk, earnings, and general articles\n",
    "notable_events_articles_aggregated[\"news_risk_score\"] = (\n",
    "    notable_events_articles_aggregated[\"high_risk_word_count\"] * 3 +  # High-Risk Words → 3x weight\n",
    "    notable_events_articles_aggregated[\"is_earnings_related\"].astype(int) * 2 +  # Earnings Articles → 2x weight\n",
    "    notable_events_articles_aggregated[\"article_count\"] * 1  # General Articles → 1x weight\n",
    ")\n",
    "\n",
    "# Normalize price change to scale price impact between 0 and 10\n",
    "max_price_change = notable_events_articles_aggregated[\"price_change\"].abs().max()\n",
    "notable_events_articles_aggregated[\"price_impact_score\"] = (\n",
    "    notable_events_articles_aggregated[\"price_change\"].abs() / max_price_change\n",
    ") * 10\n",
    "\n",
    "# Final Weighted Risk Score: News (60%) + Price Impact (40%)\n",
    "notable_events_articles_aggregated[\"final_risk_score\"] = (\n",
    "    notable_events_articles_aggregated[\"news_risk_score\"] * 0.6 +\n",
    "    notable_events_articles_aggregated[\"price_impact_score\"] * 0.4\n",
    ")\n",
    "\n",
    "# Aggregate Risk Scores per Stock per Trading Day (Ensure no double counting)\n",
    "risk_score_summary = notable_events_articles_aggregated.groupby([\"trading_day_date\", \"ticker\"]).agg(\n",
    "    total_articles=(\"article_count\", \"sum\"),\n",
    "    total_high_risk_articles=(\"high_risk_word_count\", \"sum\"),\n",
    "    total_earnings_articles=(\"is_earnings_related\", \"sum\"),\n",
    "    avg_news_risk_score=(\"news_risk_score\", \"mean\"),\n",
    "    avg_price_impact_score=(\"price_impact_score\", \"mean\"),\n",
    "    avg_final_risk_score=(\"final_risk_score\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nRisk Score Computation Complete – Sample Risk Scores:\")\n",
    "print(risk_score_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Current Risk Score Computation – Explanation**\n",
    "\n",
    "The **Risk Score** in this notebook is designed to measure **the potential market impact of news articles** on **a stock’s daily price movement**. It combines **textual analysis from news articles** with **actual price changes** to capture **the relationship between news sentiment and stock volatility**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1️Components of the Risk Score**\n",
    "The risk score is calculated based on **two primary factors**:\n",
    "\n",
    "#### **1. News-Based Risk Indicators (Weighted Scoring System)**  \n",
    "Each news article is evaluated based on **its content** and **classified** into:\n",
    "| **Indicator**            | **Description**                                        | **Weight** |\n",
    "|--------------------------|--------------------------------------------------------|------------|\n",
    "| **High-Risk Words**       | Articles containing **negative, uncertainty, or legal terms** (e.g., \"lawsuit\", \"bankrupt\", \"uncertain\") from the **Loughran-McDonald Dictionary**. | **+3 per occurrence** |\n",
    "| **Earnings Mentions**     | Articles containing **earnings-related terms** (e.g., \"earnings\", \"report\", \"guidance\", \"miss\", \"beat\"). | **+2 per article** |\n",
    "| **General News Volume**   | **All other articles** that do not fall into the above categories. | **+1 per article** |\n",
    "\n",
    "This results in a **news risk score** per trading day:\n",
    "\\[\n",
    "\\text{News Risk Score} = 3 \\times (\\text{High-Risk Words}) + 2 \\times (\\text{Earnings Articles}) + 1 \\times (\\text{General Articles})\n",
    "\\]\n",
    "\n",
    "#### **2. Price Impact Normalization**\n",
    "Price movement is also factored in, as **large price swings** (up or down) often indicate **high market volatility**.  \n",
    "Price impact is **normalized** to a **0-10 scale** based on the **largest observed price change** across the dataset:\n",
    "\\[\n",
    "\\text{Price Impact Score} = \\left(\\frac{\\left|\\text{Price Change (\\%)}\\right|}{\\text{Max Price Change (\\%)}}\\right) \\times 10\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **2Final Risk Score Formula**\n",
    "The **final risk score** combines **news risk indicators** and **price impact** with **a weighted formula**:\n",
    "\\[\n",
    "\\text{Final Risk Score} = (0.6 \\times \\text{News Risk Score}) + (0.4 \\times \\text{Price Impact Score})\n",
    "\\]\n",
    "\n",
    "- **60% Weight → News Risk (Content/Sentiment)**  \n",
    "- **40% Weight → Price Impact (Market Reaction)**\n",
    "\n",
    "This **weighted approach** reflects the idea that **news sentiment often drives price changes**, but **large price swings** themselves may **indicate risk** regardless of news coverage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "| **Risk Component**         | **Purpose**                                                      | **Weighting in Final Score** |\n",
    "|----------------------------|-------------------------------------------------------------------|-------------------------------|\n",
    "| **News Risk Indicators**    | Evaluate **news content** using high-risk words, earnings, and volume. | **60%** |\n",
    "| **Price Impact Normalization** | Capture **market volatility** based on daily price movements.   | **40%** |\n",
    "\n",
    "The **Risk Score** aims to **quantify the relationship between market sentiment and price volatility**, providing a **composite indicator** that can be **used for modeling and prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Approach?**\n",
    "- **High-Risk Words & Earnings are prioritized** because **negative sentiment or financial disclosures** often **signal uncertainty**.\n",
    "- **Price Impact adds market confirmation**, ensuring that **large movements** are also flagged as **risky** even if **news coverage is light**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\btada/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison\n",
      "+----------------------+---------+----------+-----------+\n",
      "|                      |     MAE |      MSE |        R² |\n",
      "|----------------------+---------+----------+-----------|\n",
      "| RandomForest         | 4.9505  |  87.1631 | 0.673869  |\n",
      "| Ridge Regression     | 6.1935  | 266.135  | 0.0042229 |\n",
      "| XGBoost              | 5.72131 |  90.057  | 0.663041  |\n",
      "| Decision Tree        | 4.77659 | 133.238  | 0.501475  |\n",
      "| Neural Network (MLP) | 5.97711 |  90.5466 | 0.661209  |\n",
      "+----------------------+---------+----------+-----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\btada\\Documents\\OMSAPracticum\\new_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Final Modeling DataFrame: Merge Risk Scores with Price Changes from aggregated_results\n",
    "risk_price_validation = risk_score_summary.merge(\n",
    "    aggregated_results[[\"trading_day_date\", \"ticker\", \"price_change\"]],\n",
    "    on=[\"trading_day_date\", \"ticker\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 1️⃣ Lagged Risk Score (Previous Day's Risk)\n",
    "risk_price_validation[\"lagged_risk_score\"] = risk_price_validation.groupby(\"ticker\")[\"avg_final_risk_score\"].shift(1)\n",
    "\n",
    "# 2️⃣ Sentiment-Weighted Risk Score\n",
    "# Merge back article titles to get sentiment\n",
    "risk_price_validation = risk_price_validation.merge(\n",
    "    notable_events_articles_aggregated[[\"trading_day_date\", \"ticker\", \"article_title\"]],\n",
    "    on=[\"trading_day_date\", \"ticker\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "risk_price_validation[\"sentiment_score\"] = risk_price_validation[\"article_title\"].fillna(\"\").apply(\n",
    "    lambda x: sia.polarity_scores(x)[\"compound\"]\n",
    ")\n",
    "\n",
    "risk_price_validation[\"adjusted_risk_score\"] = (\n",
    "    risk_price_validation[\"avg_final_risk_score\"] + (-risk_price_validation[\"sentiment_score\"] * 5)\n",
    ")\n",
    "\n",
    "# Optional: Clean up the DataFrame if you no longer need article titles\n",
    "risk_price_validation.drop(columns=[\"article_title\", \"sentiment_score\"], inplace=True)\n",
    "\n",
    "\n",
    "# Feature Selection & Data Preparation\n",
    "features = [\n",
    "    \"avg_final_risk_score\", \"adjusted_risk_score\", \"lagged_risk_score\", \n",
    "    \"total_articles\", \"total_high_risk_articles\", \"total_earnings_articles\"\n",
    "]\n",
    "target = \"price_change\"\n",
    "\n",
    "ml_data = risk_price_validation.dropna(subset=features + [target])\n",
    "\n",
    "X = ml_data[features]\n",
    "y = ml_data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42, objective=\"reg:squarederror\"),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    #\"Support Vector Regression (SVR)\": SVR(kernel=\"linear\"),\n",
    "    \"Neural Network (MLP)\": MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=200, warm_start=True, random_state=42),\n",
    "}\n",
    "\n",
    "# Train & Evaluate Models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\"MAE\": mae, \"MSE\": mse, \"R²\": r2}\n",
    "\n",
    "# Display Model Performance\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "model_comparison = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison\")\n",
    "print(tabulate(model_comparison, headers=\"keys\", tablefmt=\"psql\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Approach, Results, and Next Steps Summary**\n",
    "\n",
    "---\n",
    "\n",
    "## **Approach Summary**\n",
    "This project aims to **predict daily stock price movements** by **analyzing news sentiment and price volatility**.  \n",
    "A **Risk Score** was constructed to **quantify the relationship between market sentiment and stock volatility** using the following approach:\n",
    "\n",
    "---\n",
    "\n",
    "### **Risk Score Computation**\n",
    "1. **News-Based Indicators (Weighted System)**:\n",
    "   - **High-Risk Words (Loughran-McDonald Dictionary)** → **+3 per occurrence**.\n",
    "   - **Earnings-Related Articles** → **+2 per article**.\n",
    "   - **General News Volume** → **+1 per article**.\n",
    "\n",
    "2. **Price Impact Normalization**:\n",
    "   - **Price changes are scaled** to **0-10** based on the **largest observed daily movement**.\n",
    "\n",
    "3. **Final Risk Score**:\n",
    "   \\[\n",
    "   \\text{Final Risk Score} = (0.6 \\times \\text{News Risk Score}) + (0.4 \\times \\text{Price Impact Score})\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Feature Engineering for Modeling**\n",
    "- **Lagged Risk Score** (previous day’s risk) → captures potential **momentum effects**.\n",
    "- **Adjusted Risk Score (VADER Sentiment)** → adjusts the risk score based on **article sentiment polarity**.\n",
    "- **Final features**:\n",
    "  - `avg_final_risk_score`  \n",
    "  - `adjusted_risk_score`  \n",
    "  - `lagged_risk_score`  \n",
    "  - `total_articles`  \n",
    "  - `total_high_risk_articles`  \n",
    "  - `total_earnings_articles`\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Training**\n",
    "Models Trained:\n",
    "| **Model**              | **Key Characteristics** |\n",
    "|------------------------|--------------------------|\n",
    "| **RandomForest**        | Non-linear, robust to noise. |\n",
    "| **Ridge Regression**    | Linear model with L2 regularization. |\n",
    "| **XGBoost**             | Gradient boosting model; often excels in structured data. |\n",
    "| **Decision Tree**        | Simple, interpretable, but prone to overfitting. |\n",
    "| **Neural Network (MLP)** | Captures complex patterns but requires tuning. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Results Summary**\n",
    "| **Model**              | **MAE** | **MSE** | **R²** |\n",
    "|------------------------|---------|---------|--------|\n",
    "| **RandomForest**        | **4.95** | **87.16** | **0.674** |\n",
    "| **Ridge Regression**    | 6.19    | 266.13  | 0.004  |\n",
    "| **XGBoost**             | 5.72    | 90.06   | 0.663  |\n",
    "| **Decision Tree**        | 4.78    | 133.24  | 0.501  |\n",
    "| **Neural Network (MLP)** | 5.98    | 90.55   | 0.661  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "- **RandomForest** and **XGBoost** performed the best, achieving **R² ~ 0.67**, suggesting that **the Risk Score and sentiment features capture some meaningful relationship** with price changes.\n",
    "- **Linear models like Ridge Regression performed poorly**, indicating **the relationship between news and price movements is non-linear**.\n",
    "- **Neural Networks and Decision Trees performed moderately well** but **were outperformed by ensemble models like RandomForest**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps**\n",
    "### **Incorporate FinBERT Sentiment Scores**\n",
    "While **VADER** sentiment scores were useful, **FinBERT** is a **finance-specific sentiment model** trained on **financial news and filings**.  \n",
    "**Replacing or combining VADER with FinBERT** could **improve sentiment precision** and **further enhance the Adjusted Risk Score**.\n",
    "\n",
    "#### **Integrate FinBERT Sentiment (Future Update):**\n",
    "1. **Join `finbert_sentiment` Table** (from the ERD you shared) with the `notable_events_articles_aggregated` DataFrame.\n",
    "2. **Use the sentiment polarity/score from FinBERT** to replace **or adjust the VADER sentiment score**.\n",
    "3. **Update the `adjusted_risk_score` feature** to incorporate **FinBERT sentiment**.\n",
    "4. **Compare Model Performance** before/after.\n",
    "\n",
    "---\n",
    "\n",
    "### **Article-Level Modeling (Alternative Approach)**  \n",
    "Instead of **aggregating by day**, **each article becomes one observation**.  \n",
    "This approach allows us to ask:  \n",
    "**“Given this article title, what is the expected price change?”**\n",
    "\n",
    "#### **Steps to Implement:**\n",
    "1. **Treat Each Article as a Row**.\n",
    "2. **Features**:\n",
    "   - **High-Risk Word Count** (title & description).\n",
    "   - **FinBERT Sentiment Score**.\n",
    "   - **VADER Sentiment (optional for comparison)**.\n",
    "   - **Article Metadata** (e.g., publication time, source).\n",
    "   - **Earnings mention flag**.\n",
    "3. **Target Variable: Price Movement Window**:\n",
    "   - **Same day price change**.\n",
    "   - **T+1 or T+2 price change**.\n",
    "   - **Define a window** (e.g., **T to T+1 price change**).\n",
    "4. **Model Training**:\n",
    "   - Regression models to **predict price change** from **article features**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hybrid Approach (Article + Daily Signals)**\n",
    "This **keeps the daily aggregation but introduces article-level features** as **additional signals**:\n",
    "| **Granular Features**                        | **Example Integration**                               |\n",
    "|-----------------------------------------------|--------------------------------------------------------|\n",
    "| **Was there a High-Risk Article today?**      | Binary 0/1 feature.                                    |\n",
    "| **Sentiment of the most negative article?**   | Min sentiment score for the day.                       |\n",
    "| **How many articles had sentiment < -0.5?**   | Count of highly negative articles.                     |\n",
    "| **Article Volume for the day?**               | Already present (`total_articles`).                    |\n",
    "\n",
    "#### **Steps to Implement:**\n",
    "1. **Extract these granular features during Risk Score Computation.**\n",
    "2. **Add them to `risk_price_validation`** as **additional input features**.\n",
    "3. **Evaluate whether these “most negative” or “high-risk” articles drive stronger market reactions**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Next Steps**\n",
    "| **Priority** | **Action**                                   | **Expected Benefit**                                         |\n",
    "|--------------|----------------------------------------------|--------------------------------------------------------------|\n",
    "| **High**     | **Integrate `finbert_sentiment` as a feature**| More precise **finance-specific sentiment analysis**.         |\n",
    "| **Medium**   | **Hybrid Approach (Add Article-Level Granular Features)** | Capture **maximum negative sentiment or high-risk counts**.  |\n",
    "| **Low**      | **Explore Full Article-Level Model**           | Granular prediction from **individual articles**, but requires **restructuring the data**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Potential Plan Moving Forward**\n",
    "| **Step**                     | **Description**                                 | **Priority** |\n",
    "|------------------------------|--------------------------------------------------|--------------|\n",
    "| **1. Integrate `finbert_sentiment`** | Replace or supplement VADER with FinBERT.        | **High**    |\n",
    "| **2. Add Granular Features** | Track **most negative sentiment & high-risk articles** per day. | **Medium**  |\n",
    "| **3. Explore Article-Level Model** | Build **article-level dataset** for price prediction. | **Low**     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
